import{_ as e,e as i,g as s,o as t}from"./app-CbGUE4hV.js";const n="/assets/image-20230906094634937-CRx8xThB.png",l="/assets/image-20230906094624349-BOnO2qlG.png",r="/assets/image-20230906095433786-DFWfrCjo.png",h="/assets/webui_0813_1-CiF0QWGe.png",p="/assets/image-20230906100313064--osWr8cU.png",o="/assets/image-20230906095433786-DFWfrCjo.png",c="/assets/image-20230906101805804-DOVxlawV.png",d={};function g(k,a){return t(),i("div",null,a[0]||(a[0]=[s('<h1 id="langchain-chatchat" tabindex="-1"><a class="header-anchor" href="#langchain-chatchat"><span>Langchain-Chatchat</span></a></h1><p>项目文档目录</p><ul><li>FAQ.md</li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/INSTALL.md" target="_blank" rel="noopener noreferrer">Install.md</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/Issue-with-Installing-Packages-Using-pip-in-Anaconda.md" target="_blank" rel="noopener noreferrer">在Anaconda中使用pip安装包的问题.md</a></li><li>向量库环境 docker.md</li><li>启动API服务.md</li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/%E5%9C%A8Anaconda%E4%B8%AD%E4%BD%BF%E7%94%A8pip%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%A0%E6%95%88%E9%97%AE%E9%A2%98.md" target="_blank" rel="noopener noreferrer">在Anaconda中使用pip安装包无效问题.md</a></li></ul><p>参考：https://github.com/chatchat-space/Langchain-Chatchat</p><p>README 目录</p><ul><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E4%BB%8B%E7%BB%8D" target="_blank" rel="noopener noreferrer">介绍</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E5%8F%98%E6%9B%B4%E6%97%A5%E5%BF%97" target="_blank" rel="noopener noreferrer">变更日志</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E6%A8%A1%E5%9E%8B%E6%94%AF%E6%8C%81" target="_blank" rel="noopener noreferrer">模型支持</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#Docker-%E9%83%A8%E7%BD%B2" target="_blank" rel="noopener noreferrer">Docker 部署</a></li><li>开发部署 <ul><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E8%BD%AF%E4%BB%B6%E9%9C%80%E6%B1%82" target="_blank" rel="noopener noreferrer">软件需求</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#1.-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87" target="_blank" rel="noopener noreferrer">1. 开发环境准备</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#2.-%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E8%87%B3%E6%9C%AC%E5%9C%B0" target="_blank" rel="noopener noreferrer">2. 下载模型至本地</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#3.-%E8%AE%BE%E7%BD%AE%E9%85%8D%E7%BD%AE%E9%A1%B9" target="_blank" rel="noopener noreferrer">3. 设置配置项</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#4.-%E7%9F%A5%E8%AF%86%E5%BA%93%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%BF%81%E7%A7%BB" target="_blank" rel="noopener noreferrer">4. 知识库初始化与迁移</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#6.-%E4%B8%80%E9%94%AE%E5%90%AF%E5%8A%A8" target="_blank" rel="noopener noreferrer">5. 一键启动API服务或WebUI服务</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.-%E5%90%AF%E5%8A%A8-API-%E6%9C%8D%E5%8A%A1%E6%88%96-Web-UI" target="_blank" rel="noopener noreferrer">6. 分步启动 API 服务或 Web UI</a></li></ul></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98" target="_blank" rel="noopener noreferrer">常见问题</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E8%B7%AF%E7%BA%BF%E5%9B%BE" target="_blank" rel="noopener noreferrer">路线图</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E9%A1%B9%E7%9B%AE%E4%BA%A4%E6%B5%81%E7%BE%A4" target="_blank" rel="noopener noreferrer">项目交流群</a></li></ul><h2 id="介绍" tabindex="-1"><a class="header-anchor" href="#介绍"><span>介绍</span></a></h2><p>🤖️ 一种利用 <a href="https://github.com/hwchase17/langchain" target="_blank" rel="noopener noreferrer">langchain</a> 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。</p><p>💡 受 <a href="https://github.com/GanymedeNil" target="_blank" rel="noopener noreferrer">GanymedeNil</a> 的项目 <a href="https://github.com/GanymedeNil/document.ai" target="_blank" rel="noopener noreferrer">document.ai</a> 和 <a href="https://github.com/AlexZhangji" target="_blank" rel="noopener noreferrer">AlexZhangji</a> 创建的 <a href="https://github.com/THUDM/ChatGLM-6B/pull/216" target="_blank" rel="noopener noreferrer">ChatGLM-6B Pull Request</a> 启发，建立了全流程可使用开源模型实现的本地知识库问答应用。本项目的最新版本中通过使用 <a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noopener noreferrer">FastChat</a> 接入 Vicuna, Alpaca, LLaMA, Koala, RWKV 等模型，依托于 <a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer">langchain</a> 框架支持通过基于 <a href="https://github.com/tiangolo/fastapi" target="_blank" rel="noopener noreferrer">FastAPI</a> 提供的 API 调用服务，或使用基于 <a href="https://github.com/streamlit/streamlit" target="_blank" rel="noopener noreferrer">Streamlit</a> 的 WebUI 进行操作。</p><p>✅ 依托于本项目支持的开源 <strong>LLM</strong> 与 <strong>Embedding</strong> 模型，本项目可实现全部使用<strong>开源</strong>模型<strong>离线私有部署</strong>。与此同时，本项目也支持 OpenAI GPT API 的调用，并将在后续持续扩充对各类模型及模型 API 的接入。<br> （术语：LLM 是大型语言模型，Embedding 是向量库映射）</p><p>⛓️ 本项目实现原理如下图所示，过程包括加载文件 -&gt; 读取文本 -&gt; 文本分割 -&gt; 文本向量化 -&gt; 问句向量化 -&gt; 在文本向量中匹配出与问句向量最相似的 <code>top k</code>个 -&gt; 匹配出的文本作为上下文和问题一起添加到 <code>prompt</code>中 -&gt; 提交给 <code>LLM</code>生成回答。</p><p>📺 <a href="https://www.bilibili.com/video/BV13M4y1e7cN/?share_source=copy_web&amp;vd_source=e6c5aafe684f30fbe41925d61ca6d514" target="_blank" rel="noopener noreferrer">原理介绍视频</a></p><p><img src="'+n+'" alt="image-20230906094634937" loading="lazy"></p><p>从文档处理角度来看，实现流程如下：</p><p><img src="'+l+`" alt="image-20230906094624349" loading="lazy"></p><p>🚩 本项目未涉及微调、训练过程，但可利用微调或训练对本项目效果进行优化。</p><p>🌐 <a href="https://www.codewithgpu.com/i/imClumsyPanda/langchain-ChatGLM/Langchain-Chatchat" target="_blank" rel="noopener noreferrer">AutoDL 镜像</a> 中 <code>v7</code> 版本所使用代码已更新至本项目 <code>v0.2.3</code> 版本。</p><p>🐳 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0" target="_blank" rel="noopener noreferrer">Docker 镜像</a></p><p>💻 一行命令运行 Docker：</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">docker</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -d</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --gpus</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> all</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -p</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 80:8501</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="模型支持" tabindex="-1"><a class="header-anchor" href="#模型支持"><span>模型支持</span></a></h3><p>本项目中默认使用的 LLM 模型为 <a href="https://huggingface.co/THUDM/chatglm2-6b" target="_blank" rel="noopener noreferrer">THUDM/chatglm2-6b</a>，默认使用的 Embedding 模型为 <a href="https://huggingface.co/moka-ai/m3e-base" target="_blank" rel="noopener noreferrer">moka-ai/m3e-base</a> 为例。</p><h4 id="llm-模型支持" tabindex="-1"><a class="header-anchor" href="#llm-模型支持"><span>LLM 模型支持</span></a></h4><p>本项目最新版本中基于 <a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noopener noreferrer">FastChat</a> 进行本地 LLM 模型接入，支持模型如下：</p><ul><li><a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" target="_blank" rel="noopener noreferrer">meta-llama/Llama-2-7b-chat-hf</a></li><li>Vicuna, Alpaca, LLaMA, Koala</li><li><a href="https://huggingface.co/BlinkDL/rwkv-4-raven" target="_blank" rel="noopener noreferrer">BlinkDL/RWKV-4-Raven</a></li><li><a href="https://huggingface.co/camel-ai/CAMEL-13B-Combined-Data" target="_blank" rel="noopener noreferrer">camel-ai/CAMEL-13B-Combined-Data</a></li><li><a href="https://huggingface.co/databricks/dolly-v2-12b" target="_blank" rel="noopener noreferrer">databricks/dolly-v2-12b</a></li><li><a href="https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b" target="_blank" rel="noopener noreferrer">FreedomIntelligence/phoenix-inst-chat-7b</a></li><li><a href="https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b" target="_blank" rel="noopener noreferrer">h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b</a></li><li><a href="https://huggingface.co/lcw99/polyglot-ko-12.8b-chang-instruct-chat" target="_blank" rel="noopener noreferrer">lcw99/polyglot-ko-12.8b-chang-instruct-chat</a></li><li><a href="https://huggingface.co/lmsys/fastchat-t5" target="_blank" rel="noopener noreferrer">lmsys/fastchat-t5-3b-v1.0</a></li><li><a href="https://huggingface.co/mosaicml/mpt-7b-chat" target="_blank" rel="noopener noreferrer">mosaicml/mpt-7b-chat</a></li><li><a href="https://huggingface.co/Neutralzz/BiLLa-7B-SFT" target="_blank" rel="noopener noreferrer">Neutralzz/BiLLa-7B-SFT</a></li><li><a href="https://huggingface.co/nomic-ai/gpt4all-13b-snoozy" target="_blank" rel="noopener noreferrer">nomic-ai/gpt4all-13b-snoozy</a></li><li><a href="https://huggingface.co/NousResearch/Nous-Hermes-13b" target="_blank" rel="noopener noreferrer">NousResearch/Nous-Hermes-13b</a></li><li><a href="https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg" target="_blank" rel="noopener noreferrer">openaccess-ai-collective/manticore-13b-chat-pyg</a></li><li><a href="https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5" target="_blank" rel="noopener noreferrer">OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5</a></li><li><a href="https://huggingface.co/project-baize/baize-v2-7b" target="_blank" rel="noopener noreferrer">project-baize/baize-v2-7b</a></li><li><a href="https://huggingface.co/Salesforce/codet5p-6b" target="_blank" rel="noopener noreferrer">Salesforce/codet5p-6b</a></li><li><a href="https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b" target="_blank" rel="noopener noreferrer">StabilityAI/stablelm-tuned-alpha-7b</a></li><li><a href="https://huggingface.co/THUDM/chatglm-6b" target="_blank" rel="noopener noreferrer">THUDM/chatglm-6b</a></li><li><a href="https://huggingface.co/THUDM/chatglm2-6b" target="_blank" rel="noopener noreferrer">THUDM/chatglm2-6b</a></li><li><a href="https://huggingface.co/tiiuae/falcon-40b" target="_blank" rel="noopener noreferrer">tiiuae/falcon-40b</a></li><li><a href="https://huggingface.co/timdettmers/guanaco-33b-merged" target="_blank" rel="noopener noreferrer">timdettmers/guanaco-33b-merged</a></li><li><a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat" target="_blank" rel="noopener noreferrer">togethercomputer/RedPajama-INCITE-7B-Chat</a></li><li><a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.0" target="_blank" rel="noopener noreferrer">WizardLM/WizardLM-13B-V1.0</a></li><li><a href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0" target="_blank" rel="noopener noreferrer">WizardLM/WizardCoder-15B-V1.0</a></li><li><a href="https://huggingface.co/baichuan-inc/baichuan-7B" target="_blank" rel="noopener noreferrer">baichuan-inc/baichuan-7B</a></li><li><a href="https://huggingface.co/internlm/internlm-chat-7b" target="_blank" rel="noopener noreferrer">internlm/internlm-chat-7b</a></li><li><a href="https://huggingface.co/Qwen/Qwen-7B-Chat" target="_blank" rel="noopener noreferrer">Qwen/Qwen-7B-Chat</a></li><li><a href="https://huggingface.co/HuggingFaceH4/starchat-beta" target="_blank" rel="noopener noreferrer">HuggingFaceH4/starchat-beta</a></li><li>任何 <a href="https://huggingface.co/EleutherAI" target="_blank" rel="noopener noreferrer">EleutherAI</a> 的 pythia 模型，如 <a href="https://huggingface.co/EleutherAI/pythia-6.9b" target="_blank" rel="noopener noreferrer">pythia-6.9b</a></li><li>在以上模型基础上训练的任何 <a href="https://github.com/huggingface/peft" target="_blank" rel="noopener noreferrer">Peft</a> 适配器。为了激活，模型路径中必须有 <code>peft</code> 。注意：如果加载多个peft模型，你可以通过在任何模型工作器中设置环境变量 <code>PEFT_SHARE_BASE_WEIGHTS=true</code> 来使它们共享基础模型的权重。</li></ul><p>以上模型支持列表可能随 <a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noopener noreferrer">FastChat</a> 更新而持续更新，可参考 <a href="https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md" target="_blank" rel="noopener noreferrer">FastChat 已支持模型列表</a>。</p><p>除本地模型外，本项目也支持直接接入 OpenAI API，具体设置可参考 <code>configs/model_configs.py.example</code> 中的 <code>llm_model_dict</code> 的 <code>openai-chatgpt-3.5</code> 配置信息。</p><h4 id="embedding-模型支持" tabindex="-1"><a class="header-anchor" href="#embedding-模型支持"><span>Embedding 模型支持</span></a></h4><p>本项目支持调用 <a href="https://huggingface.co/models?pipeline_tag=sentence-similarity" target="_blank" rel="noopener noreferrer">HuggingFace</a> 中的 Embedding 模型，已支持的 Embedding 模型如下：</p><ul><li><a href="https://huggingface.co/moka-ai/m3e-small" target="_blank" rel="noopener noreferrer">moka-ai/m3e-small</a></li><li><a href="https://huggingface.co/moka-ai/m3e-base" target="_blank" rel="noopener noreferrer">moka-ai/m3e-base</a></li><li><a href="https://huggingface.co/moka-ai/m3e-large" target="_blank" rel="noopener noreferrer">moka-ai/m3e-large</a></li><li><a href="https://huggingface.co/BAAI/bge-small-zh" target="_blank" rel="noopener noreferrer">BAAI/bge-small-zh</a></li><li><a href="https://huggingface.co/BAAI/bge-base-zh" target="_blank" rel="noopener noreferrer">BAAI/bge-base-zh</a></li><li><a href="https://huggingface.co/BAAI/bge-large-zh" target="_blank" rel="noopener noreferrer">BAAI/bge-large-zh</a></li><li><a href="https://huggingface.co/BAAI/bge-large-zh-noinstruct" target="_blank" rel="noopener noreferrer">BAAI/bge-large-zh-noinstruct</a></li><li><a href="https://huggingface.co/shibing624/text2vec-base-chinese-sentence" target="_blank" rel="noopener noreferrer">shibing624/text2vec-base-chinese-sentence</a></li><li><a href="https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase" target="_blank" rel="noopener noreferrer">shibing624/text2vec-base-chinese-paraphrase</a></li><li><a href="https://huggingface.co/shibing624/text2vec-base-multilingual" target="_blank" rel="noopener noreferrer">shibing624/text2vec-base-multilingual</a></li><li><a href="https://huggingface.co/shibing624/text2vec-base-chinese" target="_blank" rel="noopener noreferrer">shibing624/text2vec-base-chinese</a></li><li><a href="https://huggingface.co/shibing624/text2vec-bge-large-chinese" target="_blank" rel="noopener noreferrer">shibing624/text2vec-bge-large-chinese</a></li><li><a href="https://huggingface.co/GanymedeNil/text2vec-large-chinese" target="_blank" rel="noopener noreferrer">GanymedeNil/text2vec-large-chinese</a></li><li><a href="https://huggingface.co/nghuyong/ernie-3.0-nano-zh" target="_blank" rel="noopener noreferrer">nghuyong/ernie-3.0-nano-zh</a></li><li><a href="https://huggingface.co/nghuyong/ernie-3.0-base-zh" target="_blank" rel="noopener noreferrer">nghuyong/ernie-3.0-base-zh</a></li><li><a href="https://platform.openai.com/docs/guides/embeddings" target="_blank" rel="noopener noreferrer">OpenAI/text-embedding-ada-002</a></li></ul><h2 id="docker-部署" tabindex="-1"><a class="header-anchor" href="#docker-部署"><span>Docker 部署</span></a></h2><p>🐳 Docker 镜像地址: <code>registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0)</code></p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">docker</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -d</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --gpus</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> all</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -p</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 80:8501</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>该版本镜像大小 <code>33.9GB</code>，使用 <code>v0.2.0</code>，以 <code>nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04</code> 为基础镜像</li><li>该版本内置一个 <code>embedding</code> 模型：<code>m3e-large</code>，内置 <code>chatglm2-6b-32k</code></li><li>该版本目标为方便一键部署使用，请确保您已经在Linux发行版上安装了NVIDIA驱动程序</li><li>请注意，您不需要在主机系统上安装CUDA工具包，但需要安装 <code>NVIDIA Driver</code> 以及 <code>NVIDIA Container Toolkit</code>，请参考<a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" target="_blank" rel="noopener noreferrer">安装指南</a></li><li>首次拉取和启动均需要一定时间，首次启动时请参照下图使用 <code>docker logs -f &lt;container id&gt;</code> 查看日志</li><li>如遇到启动过程卡在 <code>Waiting..</code> 步骤，建议使用 <code>docker exec -it &lt;container id&gt; bash</code> 进入 <code>/logs/</code> 目录查看对应阶段日志</li></ul><h2 id="开发部署" tabindex="-1"><a class="header-anchor" href="#开发部署"><span>开发部署</span></a></h2><h3 id="环境准备" tabindex="-1"><a class="header-anchor" href="#环境准备"><span>环境准备</span></a></h3><p>开发环境准备</p><p>本项目已在 Python 3.8.1 - 3.10，CUDA 11.7 环境下完成测试。已在 Windows、ARM 架构的 macOS、Linux 系统中完成测试。</p><p>参见 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/INSTALL.md" target="_blank" rel="noopener noreferrer">开发环境准备</a>。</p><p><strong>请注意：</strong> <code>0.2.0</code> 及更新版本的依赖包与 <code>0.1.x</code> 版本依赖包可能发生冲突，强烈建议新建环境后重新安装依赖包。</p><h4 id="环境检查" tabindex="-1"><a class="header-anchor" href="#环境检查"><span>环境检查</span></a></h4><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 首先，确信你的机器安装了 Python 3.8 - 3.10 版本</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --version</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">Python</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 3.8.13</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 如果低于这个版本，可使用conda安装环境</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> conda</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> create</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -p</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> /your_path/env_name</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3.8</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 激活环境</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> source</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> activate</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> /your_path/env_name</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 或，conda安装，不指定路径, 注意以下，都将/your_path/env_name替换为env_name</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> conda</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> create</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> env_name</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3.8</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> conda</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> activate</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> env_name</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # Activate the environment</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 更新py库</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pip3</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> install</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --upgrade</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pip</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 关闭环境</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> source</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> deactivate</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> /your_path/env_name</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 删除环境</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> conda</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> env</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> remove</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -p</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  /your_path/env_name</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="项目依赖" tabindex="-1"><a class="header-anchor" href="#项目依赖"><span>项目依赖</span></a></h4><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 拉取仓库</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> git</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> clone</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> https://github.com/chatchat-space/Langchain-Chatchat.git</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 进入目录</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> cd</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Langchain-Chatchat</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 安装依赖 (三选一，为方便用户 API 与 webui 分离运行，可单独根据运行需求安装依赖包)</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pip</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> install</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -r</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> requirements.txt</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # 安装全部依赖</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># $ pip install -r requirements_api.txt # 如果只需运行 API</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># $ pip install -r requirements_webui.txt # 如果只需运行 WebUI</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 默认依赖包括基本运行环境（FAISS向量库）。如果要使用 milvus/pg_vector 等向量库，请将 requirements.txt 中相应依赖取消注释再安装。</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>注：使用 <code>langchain.document_loaders.UnstructuredFileLoader</code> 进行 <code>.docx</code> 等格式非结构化文件接入时，可能需要依据文档进行其他依赖包的安装，请参考 <a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html" target="_blank" rel="noopener noreferrer">langchain 文档</a>。</p><h3 id="模型下载-llm-embedding" tabindex="-1"><a class="header-anchor" href="#模型下载-llm-embedding"><span>模型下载 (LLM &amp; Embedding)</span></a></h3><p>下载模型至本地</p><p>如需在本地或离线环境下运行本项目，需要首先将项目所需的模型下载至本地，通常开源 LLM 与 Embedding 模型可以从 <a href="https://huggingface.co/models" target="_blank" rel="noopener noreferrer">HuggingFace</a> 下载。</p><p>以本项目中默认使用的 LLM 模型 <a href="https://huggingface.co/THUDM/chatglm2-6b" target="_blank" rel="noopener noreferrer">THUDM/chatglm2-6b</a> 与 Embedding 模型 <a href="https://huggingface.co/moka-ai/m3e-base" target="_blank" rel="noopener noreferrer">moka-ai/m3e-base</a> 为例：</p><p>下载模型需要先<a href="https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage" target="_blank" rel="noopener noreferrer">安装Git LFS</a>，然后运行</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> git</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> clone</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> https://huggingface.co/THUDM/chatglm2-6b</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> git</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> clone</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> https://huggingface.co/moka-ai/m3e-base</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="配置选项" tabindex="-1"><a class="header-anchor" href="#配置选项"><span>配置选项</span></a></h3><p>设置配置项</p><p>复制模型相关参数配置模板文件 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/configs/model_config.py.example" target="_blank" rel="noopener noreferrer">configs/model_config.py.example</a> 存储至项目路径下 <code>./configs</code> 路径下，并重命名为 <code>model_config.py</code>。</p><p>复制服务相关参数配置模板文件 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/configs/server_config.py.example" target="_blank" rel="noopener noreferrer">configs/server_config.py.example</a> 存储至项目路径下 <code>./configs</code> 路径下，并重命名为 <code>server_config.py</code>。</p><p>在开始执行 Web UI 或命令行交互前，请先检查 <code>configs/model_config.py</code> 和 <code>configs/server_config.py</code> 中的各项模型参数设计是否符合需求：</p><ul><li><p>请确认已下载至本地的 LLM 模型本地存储路径写在 <code>llm_model_dict</code> 对应模型的 <code>local_model_path</code> 属性中，如:</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm_model_dict</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;chatglm2-6b&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: {</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;local_model_path&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;/Users/xxx/Downloads/chatglm2-6b&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;api_base_url&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;http://localhost:8888/v1&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># &quot;name&quot;修改为 FastChat 服务中的&quot;api_base_url&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;api_key&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;EMPTY&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    },</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>请确认已下载至本地的 Embedding 模型本地存储路径写在 <code>embedding_model_dict</code> 对应模型位置，如：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">embedding_model_dict </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> {</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;m3e-base&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;/Users/xxx/Downloads/m3e-base&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><p>如果你选择使用OpenAI的Embedding模型，请将模型的 <code>key</code>写入 <code>embedding_model_dict</code>中。使用该模型，你需要能够访问OpenAI官的API，或设置代理。</p><h3 id="知识库初始化与迁移" tabindex="-1"><a class="header-anchor" href="#知识库初始化与迁移"><span>知识库初始化与迁移</span></a></h3><p>当前项目的知识库信息存储在数据库中，在正式运行项目之前请先初始化数据库（我们强烈建议您在执行操作前备份您的知识文件）。</p><ul><li><p>如果您是从 <code>0.1.x</code> 版本升级过来的用户，针对已建立的知识库，请确认知识库的向量库类型、Embedding 模型与 <code>configs/model_config.py</code> 中默认设置一致，如无变化只需以下命令将现有知识库信息添加到数据库即可：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> init_database.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></li><li><p>如果您是第一次运行本项目，知识库尚未建立，或者配置文件中的知识库类型、嵌入模型发生变化，或者之前的向量库没有开启 <code>normalize_L2</code>，需要以下命令初始化或重建知识库：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> init_database.py</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --recreate-vs</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></li></ul><h3 id="一键启动api-服务或-web-ui" tabindex="-1"><a class="header-anchor" href="#一键启动api-服务或-web-ui"><span>一键启动API 服务或 Web UI</span></a></h3><h4 id="启动命令" tabindex="-1"><a class="header-anchor" href="#启动命令"><span>启动命令</span></a></h4><p>一键启动脚本 startup.py,一键启动所有 Fastchat 服务、API 服务、WebUI 服务，示例代码：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>$ python startup.py -a</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>并可使用 <code>Ctrl + C</code> 直接关闭所有运行服务。如果一次结束不了，可以多按几次。</p><p>可选参数包括 <code>-a (或--all-webui)</code>, <code>--all-api</code>, <code>--llm-api</code>, <code>-c (或--controller)</code>, <code>--openai-api</code>, <code>-m (或--model-worker)</code>, <code>--api</code>, <code>--webui</code>，其中：</p><ul><li><code>--all-webui</code> 为一键启动 WebUI 所有依赖服务；</li><li><code>--all-api</code> 为一键启动 API 所有依赖服务；</li><li><code>--llm-api</code> 为一键启动 Fastchat 所有依赖的 LLM 服务；</li><li><code>--openai-api</code> 为仅启动 FastChat 的 controller 和 openai-api-server 服务；</li><li>其他为单独服务启动选项。</li></ul><h4 id="启动非默认模型" tabindex="-1"><a class="header-anchor" href="#启动非默认模型"><span>启动非默认模型</span></a></h4><p>若想指定非默认模型，需要用 <code>--model-name</code> 选项，示例：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>$ python startup.py --all-webui --model-name Qwen-7B-Chat</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>更多信息可通过 <code>python startup.py -h</code>查看。</p><h4 id="多卡加载" tabindex="-1"><a class="header-anchor" href="#多卡加载"><span>多卡加载</span></a></h4><p>项目支持多卡加载，需在 startup.py 中的 create_model_worker_app 函数中，修改如下三个参数:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>gpus=None, </span></span>
<span class="line"><span>num_gpus=1, </span></span>
<span class="line"><span>max_gpu_memory=&quot;20GiB&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，<code>gpus</code> 控制使用的显卡的ID，例如 &quot;0,1&quot;;</p><p><code>num_gpus</code> 控制使用的卡数;</p><p><code>max_gpu_memory</code> 控制每个卡使用的显存容量。</p><p>注1：server_config.py的FSCHAT_MODEL_WORKERS字典中也增加了相关配置，如有需要也可通过修改FSCHAT_MODEL_WORKERS字典中对应参数实现多卡加载。</p><p>注2：少数情况下，gpus参数会不生效，此时需要通过设置环境变量CUDA_VISIBLE_DEVICES来指定torch可见的gpu,示例代码：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>CUDA_VISIBLE_DEVICES=0,1 python startup.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等" tabindex="-1"><a class="header-anchor" href="#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等"><span>PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)</span></a></h4><p>本项目基于 FastChat 加载 LLM 服务，故需以 FastChat 加载 PEFT 路径，即保证路径名称里必须有 peft 这个词，配置文件的名字为 adapter_config.json，peft 路径下包含.bin 格式的 PEFT 权重，peft路径在startup.py中create_model_worker_app函数的args.model_names中指定，并开启环境变量PEFT_SHARE_BASE_WEIGHTS=true参数。</p><p>注：如果上述方式启动失败，则需要以标准的fastchat服务启动方式分步启动，分步启动步骤参考第六节，PEFT加载详细步骤参考<a href="https://github.com/chatchat-space/Langchain-Chatchat/issues/1130#issuecomment-1685291822" target="_blank" rel="noopener noreferrer">加载lora微调后模型失效</a>，</p><h4 id="注意事项" tabindex="-1"><a class="header-anchor" href="#注意事项"><span><strong>注意事项</strong></span></a></h4><p><strong>1. startup 脚本用多进程方式启动各模块的服务，可能会导致打印顺序问题，请等待全部服务发起后再调用，并根据默认或指定端口调用服务（默认 LLM API 服务端口：<code>127.0.0.1:8888</code>,默认 API 服务端口：<code>127.0.0.1:7861</code>,默认 WebUI 服务端口：<code>本机IP：8501</code>)</strong></p><p><strong>2.服务启动时间示设备不同而不同，约 3-10 分钟，如长时间没有启动请前往 <code>./logs</code>目录下监控日志，定位问题。</strong></p><p><strong>3. 在Linux上使用ctrl+C退出可能会由于linux的多进程机制导致multiprocessing遗留孤儿进程，可通过shutdown_all.sh进行退出</strong></p><h4 id="启动界面示例" tabindex="-1"><a class="header-anchor" href="#启动界面示例"><span>启动界面示例</span></a></h4><ol><li>FastAPI docs 界面</li></ol><p><img src="`+r+'" alt="image-20230906095433786" loading="lazy"></p><ol start="2"><li>webui启动界面示例：</li></ol><ul><li>Web UI 对话界面： <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_0.png" target="_blank" rel="noopener noreferrer"><img src="https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png" alt="img" loading="lazy"></a></li><li>Web UI 知识库管理页面： <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_1.png" target="_blank" rel="noopener noreferrer"><img src="'+h+`" alt="img" loading="lazy"></a></li></ul><h3 id="分步启动-api-服务或-web-ui-一键启动忽略本节" tabindex="-1"><a class="header-anchor" href="#分步启动-api-服务或-web-ui-一键启动忽略本节"><span>分步启动 API 服务或 Web UI（一键启动忽略本节）</span></a></h3><p>注意：如使用了一键启动方式，可忽略本节。</p><h4 id="启动-llm-服务-三种方式" tabindex="-1"><a class="header-anchor" href="#启动-llm-服务-三种方式"><span>启动 LLM 服务 (三种方式)</span></a></h4><p>如需使用开源模型进行本地部署，需首先启动 LLM 服务，启动方式分为三种：</p><ul><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1.1-%E5%9F%BA%E4%BA%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B%E8%84%9A%E6%9C%AC-llm_api.py-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1" target="_blank" rel="noopener noreferrer">基于多进程脚本 llm_api.py 启动 LLM 服务</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1.2-%E5%9F%BA%E4%BA%8E%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%84%9A%E6%9C%AC-llm_api_stale.py-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1" target="_blank" rel="noopener noreferrer">基于命令行脚本 llm_api_stale.py 启动 LLM 服务</a></li><li><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1.3-PEFT-%E5%8A%A0%E8%BD%BD" target="_blank" rel="noopener noreferrer">PEFT 加载</a></li></ul><p>三种方式只需选择一个即可，具体操作方式详见 5.1.1 - 5.1.3。</p><p>如果启动在线的API服务（如 OPENAI 的 API 接口），则无需启动 LLM 服务，即 5.1 小节的任何命令均无需启动。</p><h5 id="基于多进程脚本-llm-api-py-启动-llm-服务" tabindex="-1"><a class="header-anchor" href="#基于多进程脚本-llm-api-py-启动-llm-服务"><span>基于多进程脚本 llm_api.py 启动 LLM 服务</span></a></h5><p>在项目根目录下，执行 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/llm_api.py" target="_blank" rel="noopener noreferrer">server/llm_api.py</a> 脚本启动 <strong>LLM 模型</strong>服务：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>$ python server/llm_api.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>项目支持多卡加载，需在 llm_api.py 中的 create_model_worker_app 函数中，修改如下三个参数:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>gpus=None, </span></span>
<span class="line"><span>num_gpus=1, </span></span>
<span class="line"><span>max_gpu_memory=&quot;20GiB&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，<code>gpus</code> 控制使用的显卡的ID，如果 &quot;0,1&quot;;</p><p><code>num_gpus</code> 控制使用的卡数;</p><p><code>max_gpu_memory</code> 控制每个卡使用的显存容量。</p><h5 id="基于命令行脚本-llm-api-stale-py-启动-llm-服务" tabindex="-1"><a class="header-anchor" href="#基于命令行脚本-llm-api-stale-py-启动-llm-服务"><span>基于命令行脚本 llm_api_stale.py 启动 LLM 服务</span></a></h5><p>⚠️ <strong>注意:</strong></p><p><strong>1.llm_api_stale.py脚本原生仅适用于linux。mac设备需要安装对应的linux命令，win平台请使用wsl;</strong></p><p><strong>2.加载非默认模型需要用命令行参数--model-path-address指定模型，不会读取model_config.py配置;</strong></p><p>在项目根目录下，执行 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/llm_api_stale.py" target="_blank" rel="noopener noreferrer">server/llm_api_stale.py</a> 脚本启动 <strong>LLM 模型</strong>服务：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> server/llm_api_stale.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>该方式支持启动多个worker，示例启动方式：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> server/llm_api_stale.py</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --model-path-address</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> model1@host1@port1</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> model2@host2@port2</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如果出现server端口占用情况，需手动指定server端口,并同步修改model_config.py下对应模型的base_api_url为指定端口:</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> server/llm_api_stale.py</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --server-port</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 8887</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如果要启动多卡加载，示例命令如下：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> server/llm_api_stale.py</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --gpus</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 0,1</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --num-gpus</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --max-gpu-memory</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 10GiB</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>注：以如上方式启动LLM服务会以nohup命令在后台运行 FastChat 服务，如需停止服务，可以运行如下命令：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> server/llm_api_shutdown.py</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --serve</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> all</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>亦可单独停止一个 FastChat 服务模块，可选 [<code>all</code>, <code>controller</code>, <code>model_worker</code>, <code>openai_api_server</code>]</p><h5 id="peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1" tabindex="-1"><a class="header-anchor" href="#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1"><span>PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)</span></a></h5><p>本项目基于 FastChat 加载 LLM 服务，故需以 FastChat 加载 PEFT 路径，即保证路径名称里必须有 peft 这个词，配置文件的名字为 adapter_config.json，peft 路径下包含 model.bin 格式的 PEFT 权重。 详细步骤参考<a href="https://github.com/chatchat-space/Langchain-Chatchat/issues/1130#issuecomment-1685291822" target="_blank" rel="noopener noreferrer">加载lora微调后模型失效</a></p><p><img src="`+p+'" alt="image-20230906100313064" loading="lazy"></p><h4 id="启动-api-服务" tabindex="-1"><a class="header-anchor" href="#启动-api-服务"><span>启动 API 服务</span></a></h4><p>本地部署情况下，按照 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1" target="_blank" rel="noopener noreferrer">5.1 节</a><strong>启动 LLM 服务后</strong>，再执行 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/api.py" target="_blank" rel="noopener noreferrer">server/api.py</a> 脚本启动 <strong>API</strong> 服务；</p><p>在线调用API服务的情况下，直接执执行 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/api.py" target="_blank" rel="noopener noreferrer">server/api.py</a> 脚本启动 <strong>API</strong> 服务；</p><p>调用命令示例：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> server/api.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>启动 API 服务后，可访问 <code>localhost:7861</code> 或 <code>{API 所在服务器 IP}:7861</code> FastAPI 自动生成的 docs 进行接口查看与测试。</p><ul><li>FastAPI docs 界面<br><img src="'+o+`" alt="image-20230906100403529" loading="lazy"></li></ul><h4 id="启动-web-ui-服务" tabindex="-1"><a class="header-anchor" href="#启动-web-ui-服务"><span>启动 Web UI 服务</span></a></h4><p>按照 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.2-%E5%90%AF%E5%8A%A8-API-%E6%9C%8D%E5%8A%A1" target="_blank" rel="noopener noreferrer">5.2 节</a><strong>启动 API 服务后</strong>，执行 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/webui.py" target="_blank" rel="noopener noreferrer">webui.py</a> 启动 <strong>Web UI</strong> 服务（默认使用端口 <code>8501</code>）</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> streamlit</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> webui.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>使用 Langchain-Chatchat 主题色启动 <strong>Web UI</strong> 服务（默认使用端口 <code>8501</code>）</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> streamlit</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> webui.py</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --theme.base</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;light&quot;</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --theme.primaryColor</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;#165dff&quot;</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --theme.secondaryBackgroundColor</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;#f5f5f5&quot;</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --theme.textColor</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;#000000&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>或使用以下命令指定启动 <strong>Web UI</strong> 服务并指定端口号</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">$</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> streamlit</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> webui.py</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --server.port</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 666</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li><p>Web UI 对话界面：</p><p><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_0.png" target="_blank" rel="noopener noreferrer"><img src="https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png" alt="img" loading="lazy"></a></p></li><li><p>Web UI 知识库管理页面：</p><p><a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_1.png" target="_blank" rel="noopener noreferrer"><img src="https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_1.png" alt="img" loading="lazy"></a></p></li></ul><h2 id="常见问题" tabindex="-1"><a class="header-anchor" href="#常见问题"><span>常见问题</span></a></h2><p>参见 <a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/FAQ.md" target="_blank" rel="noopener noreferrer">常见问题</a>。</p><h2 id="路线图" tabindex="-1"><a class="header-anchor" href="#路线图"><span>路线图</span></a></h2><ul><li>Langchain 应用</li><li>本地数据接入<br> - 接入非结构化文档 <ul><li>.md</li><li>.txt</li><li>.docx</li></ul></li></ul><pre><code>-  结构化数据接入
  -  .csv
  -  .xlsx
-  分词及召回
  -  接入不同类型 TextSplitter
  -  优化依据中文标点符号设计的 ChineseTextSplitter
  -  重新实现上下文拼接召回
-  本地网页接入
-  SQL 接入
-  知识图谱/图数据库接入
</code></pre><ul><li>搜索引擎接入<br> - Bing 搜索<br> - DuckDuckGo 搜索</li><li>Agent 实现</li><li>LLM 模型接入</li><li>支持通过调用 <a href="https://github.com/lm-sys/fastchat" target="_blank" rel="noopener noreferrer">FastChat</a> api 调用 llm</li><li>支持 ChatGLM API 等 LLM API 的接入</li><li>Embedding 模型接入</li><li>支持调用 HuggingFace 中各开源 Emebdding 模型</li><li>支持 OpenAI Embedding API 等 Embedding API 的接入</li><li>基于 FastAPI 的 API 方式调用</li><li>Web UI</li><li>基于 Streamlit 的 Web UI</li></ul><h2 id="项目交流群" tabindex="-1"><a class="header-anchor" href="#项目交流群"><span>项目交流群</span></a></h2><img src="`+c+'" alt="image-20230906101805804" style="zoom:25%;">',149)]))}const m=e(d,[["render",g],["__file","index.html.vue"]]),u=JSON.parse('{"path":"/MdNote_Public/01.%20DesignAndDevelop/Develop/04.%20Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/","title":"Langchain-Chatchat","lang":"zh-CN","frontmatter":{"description":"Langchain-Chatchat 项目文档目录 FAQ.md Install.md 在Anaconda中使用pip安装包的问题.md 向量库环境 docker.md 启动API服务.md 在Anaconda中使用pip安装包无效问题.md 参考：https://github.com/chatchat-space/Langchain-Chatchat...","head":[["meta",{"property":"og:url","content":"https://LincZero.github.io/MdNote_Public/01.%20DesignAndDevelop/Develop/04.%20Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/"}],["meta",{"property":"og:site_name","content":"Linc 的小站"}],["meta",{"property":"og:title","content":"Langchain-Chatchat"}],["meta",{"property":"og:description","content":"Langchain-Chatchat 项目文档目录 FAQ.md Install.md 在Anaconda中使用pip安装包的问题.md 向量库环境 docker.md 启动API服务.md 在Anaconda中使用pip安装包无效问题.md 参考：https://github.com/chatchat-space/Langchain-Chatchat..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Langchain-Chatchat\\",\\"image\\":[\\"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png\\",\\"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png\\",\\"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_1.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LincZero\\",\\"url\\":\\"https://github.com/LincZero/\\"}]}"]]},"git":{},"readingTime":{"minutes":15.14,"words":4543},"filePathRelative":"MdNote_Public/01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/README.md","excerpt":"\\n<p>项目文档目录</p>\\n<ul>\\n<li>FAQ.md</li>\\n<li><a href=\\"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/INSTALL.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Install.md</a></li>\\n<li><a href=\\"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/Issue-with-Installing-Packages-Using-pip-in-Anaconda.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">在Anaconda中使用pip安装包的问题.md</a></li>\\n<li>向量库环境 docker.md</li>\\n<li>启动API服务.md</li>\\n<li><a href=\\"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/%E5%9C%A8Anaconda%E4%B8%AD%E4%BD%BF%E7%94%A8pip%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%A0%E6%95%88%E9%97%AE%E9%A2%98.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">在Anaconda中使用pip安装包无效问题.md</a></li>\\n</ul>","autoDesc":true,"bioChainData":{"outlink":[],"backlink":[],"localMap":{"nodes":[{"id":"MdNote_Public/01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/README.md","value":{"title":"Chat/","path":"MdNote_Public/01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/README.md","outlink":[],"backlink":[]}}],"links":[]}}}');export{m as comp,u as data};
