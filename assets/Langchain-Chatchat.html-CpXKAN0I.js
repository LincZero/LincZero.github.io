import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as i,o,c as r,a as e,b as a,d as s,e as t}from"./app-_2Vv0rF6.js";const c="/assets/image-20230906094634937-CRx8xThB.png",p="/assets/image-20230906094624349-BOnO2qlG.png",h="/assets/image-20230906095433786-DFWfrCjo.png",d="/assets/webui_0813_1-CiF0QWGe.png",u="/assets/image-20230906100313064--osWr8cU.png",g="/assets/image-20230906095433786-DFWfrCjo.png",m="/assets/image-20230906101805804-DOVxlawV.png",b={},_=e("h1",{id:"langchain-chatchat",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#langchain-chatchat"},[e("span",null,"Langchain-Chatchat")])],-1),v=e("h1",{id:"目录",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#目录"},[e("span",null,"目录")])],-1),k=e("p",null,"项目文档目录",-1),f=e("li",null,"FAQ.md",-1),E={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/INSTALL.md",target:"_blank",rel:"noopener noreferrer"},A={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/Issue-with-Installing-Packages-Using-pip-in-Anaconda.md",target:"_blank",rel:"noopener noreferrer"},y=e("li",null,"向量库环境 docker.md",-1),L=e("li",null,"启动API服务.md",-1),x={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/%E5%9C%A8Anaconda%E4%B8%AD%E4%BD%BF%E7%94%A8pip%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%A0%E6%95%88%E9%97%AE%E9%A2%98.md",target:"_blank",rel:"noopener noreferrer"},B=e("h1",{id:"项目readme",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#项目readme"},[e("span",null,"项目README")])],-1),C=e("p",null,"参考：https://github.com/chatchat-space/Langchain-Chatchat",-1),I=e("p",null,"README 目录",-1),D={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E4%BB%8B%E7%BB%8D",target:"_blank",rel:"noopener noreferrer"},M={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E5%8F%98%E6%9B%B4%E6%97%A5%E5%BF%97",target:"_blank",rel:"noopener noreferrer"},w={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E6%A8%A1%E5%9E%8B%E6%94%AF%E6%8C%81",target:"_blank",rel:"noopener noreferrer"},P={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#Docker-%E9%83%A8%E7%BD%B2",target:"_blank",rel:"noopener noreferrer"},F={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E8%BD%AF%E4%BB%B6%E9%9C%80%E6%B1%82",target:"_blank",rel:"noopener noreferrer"},T={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#1.-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87",target:"_blank",rel:"noopener noreferrer"},z={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#2.-%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E8%87%B3%E6%9C%AC%E5%9C%B0",target:"_blank",rel:"noopener noreferrer"},U={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#3.-%E8%AE%BE%E7%BD%AE%E9%85%8D%E7%BD%AE%E9%A1%B9",target:"_blank",rel:"noopener noreferrer"},q={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#4.-%E7%9F%A5%E8%AF%86%E5%BA%93%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%BF%81%E7%A7%BB",target:"_blank",rel:"noopener noreferrer"},N={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#6.-%E4%B8%80%E9%94%AE%E5%90%AF%E5%8A%A8",target:"_blank",rel:"noopener noreferrer"},W={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.-%E5%90%AF%E5%8A%A8-API-%E6%9C%8D%E5%8A%A1%E6%88%96-Web-UI",target:"_blank",rel:"noopener noreferrer"},G={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98",target:"_blank",rel:"noopener noreferrer"},R={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E8%B7%AF%E7%BA%BF%E5%9B%BE",target:"_blank",rel:"noopener noreferrer"},S={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E9%A1%B9%E7%9B%AE%E4%BA%A4%E6%B5%81%E7%BE%A4",target:"_blank",rel:"noopener noreferrer"},$=e("h2",{id:"介绍",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#介绍"},[e("span",null,"介绍")])],-1),V={href:"https://github.com/hwchase17/langchain",target:"_blank",rel:"noopener noreferrer"},H={href:"https://github.com/GanymedeNil",target:"_blank",rel:"noopener noreferrer"},j={href:"https://github.com/GanymedeNil/document.ai",target:"_blank",rel:"noopener noreferrer"},O={href:"https://github.com/AlexZhangji",target:"_blank",rel:"noopener noreferrer"},Q={href:"https://github.com/THUDM/ChatGLM-6B/pull/216",target:"_blank",rel:"noopener noreferrer"},K={href:"https://github.com/lm-sys/FastChat",target:"_blank",rel:"noopener noreferrer"},Z={href:"https://github.com/langchain-ai/langchain",target:"_blank",rel:"noopener noreferrer"},J={href:"https://github.com/tiangolo/fastapi",target:"_blank",rel:"noopener noreferrer"},Y={href:"https://github.com/streamlit/streamlit",target:"_blank",rel:"noopener noreferrer"},X=t("<p>✅ 依托于本项目支持的开源 <strong>LLM</strong> 与 <strong>Embedding</strong> 模型，本项目可实现全部使用<strong>开源</strong>模型<strong>离线私有部署</strong>。与此同时，本项目也支持 OpenAI GPT API 的调用，并将在后续持续扩充对各类模型及模型 API 的接入。 （术语：LLM 是大型语言模型，Embedding 是向量库映射）</p><p>⛓️ 本项目实现原理如下图所示，过程包括加载文件 -&gt; 读取文本 -&gt; 文本分割 -&gt; 文本向量化 -&gt; 问句向量化 -&gt; 在文本向量中匹配出与问句向量最相似的 <code>top k</code>个 -&gt; 匹配出的文本作为上下文和问题一起添加到 <code>prompt</code>中 -&gt; 提交给 <code>LLM</code>生成回答。</p>",2),ee={href:"https://www.bilibili.com/video/BV13M4y1e7cN/?share_source=copy_web&vd_source=e6c5aafe684f30fbe41925d61ca6d514",target:"_blank",rel:"noopener noreferrer"},ae=e("p",null,[e("img",{src:c,alt:"image-20230906094634937",loading:"lazy"})],-1),ne=e("p",null,"从文档处理角度来看，实现流程如下：",-1),se=e("p",null,[e("img",{src:p,alt:"image-20230906094624349",loading:"lazy"})],-1),te=e("p",null,"🚩 本项目未涉及微调、训练过程，但可利用微调或训练对本项目效果进行优化。",-1),le={href:"https://www.codewithgpu.com/i/imClumsyPanda/langchain-ChatGLM/Langchain-Chatchat",target:"_blank",rel:"noopener noreferrer"},ie=e("code",null,"v7",-1),oe=e("code",null,"v0.2.3",-1),re={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0",target:"_blank",rel:"noopener noreferrer"},ce=t(`<p>💻 一行命令运行 Docker：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-p</span> <span class="token number">80</span>:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="模型支持" tabindex="-1"><a class="header-anchor" href="#模型支持"><span>模型支持</span></a></h3>`,3),pe={href:"https://huggingface.co/THUDM/chatglm2-6b",target:"_blank",rel:"noopener noreferrer"},he={href:"https://huggingface.co/moka-ai/m3e-base",target:"_blank",rel:"noopener noreferrer"},de=e("h4",{id:"llm-模型支持",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-模型支持"},[e("span",null,"LLM 模型支持")])],-1),ue={href:"https://github.com/lm-sys/FastChat",target:"_blank",rel:"noopener noreferrer"},ge={href:"https://huggingface.co/meta-llama/Llama-2-7b-chat-hf",target:"_blank",rel:"noopener noreferrer"},me=e("li",null,"Vicuna, Alpaca, LLaMA, Koala",-1),be={href:"https://huggingface.co/BlinkDL/rwkv-4-raven",target:"_blank",rel:"noopener noreferrer"},_e={href:"https://huggingface.co/camel-ai/CAMEL-13B-Combined-Data",target:"_blank",rel:"noopener noreferrer"},ve={href:"https://huggingface.co/databricks/dolly-v2-12b",target:"_blank",rel:"noopener noreferrer"},ke={href:"https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b",target:"_blank",rel:"noopener noreferrer"},fe={href:"https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b",target:"_blank",rel:"noopener noreferrer"},Ee={href:"https://huggingface.co/lcw99/polyglot-ko-12.8b-chang-instruct-chat",target:"_blank",rel:"noopener noreferrer"},Ae={href:"https://huggingface.co/lmsys/fastchat-t5",target:"_blank",rel:"noopener noreferrer"},ye={href:"https://huggingface.co/mosaicml/mpt-7b-chat",target:"_blank",rel:"noopener noreferrer"},Le={href:"https://huggingface.co/Neutralzz/BiLLa-7B-SFT",target:"_blank",rel:"noopener noreferrer"},xe={href:"https://huggingface.co/nomic-ai/gpt4all-13b-snoozy",target:"_blank",rel:"noopener noreferrer"},Be={href:"https://huggingface.co/NousResearch/Nous-Hermes-13b",target:"_blank",rel:"noopener noreferrer"},Ce={href:"https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg",target:"_blank",rel:"noopener noreferrer"},Ie={href:"https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5",target:"_blank",rel:"noopener noreferrer"},De={href:"https://huggingface.co/project-baize/baize-v2-7b",target:"_blank",rel:"noopener noreferrer"},Me={href:"https://huggingface.co/Salesforce/codet5p-6b",target:"_blank",rel:"noopener noreferrer"},we={href:"https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b",target:"_blank",rel:"noopener noreferrer"},Pe={href:"https://huggingface.co/THUDM/chatglm-6b",target:"_blank",rel:"noopener noreferrer"},Fe={href:"https://huggingface.co/THUDM/chatglm2-6b",target:"_blank",rel:"noopener noreferrer"},Te={href:"https://huggingface.co/tiiuae/falcon-40b",target:"_blank",rel:"noopener noreferrer"},ze={href:"https://huggingface.co/timdettmers/guanaco-33b-merged",target:"_blank",rel:"noopener noreferrer"},Ue={href:"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat",target:"_blank",rel:"noopener noreferrer"},qe={href:"https://huggingface.co/WizardLM/WizardLM-13B-V1.0",target:"_blank",rel:"noopener noreferrer"},Ne={href:"https://huggingface.co/WizardLM/WizardCoder-15B-V1.0",target:"_blank",rel:"noopener noreferrer"},We={href:"https://huggingface.co/baichuan-inc/baichuan-7B",target:"_blank",rel:"noopener noreferrer"},Ge={href:"https://huggingface.co/internlm/internlm-chat-7b",target:"_blank",rel:"noopener noreferrer"},Re={href:"https://huggingface.co/Qwen/Qwen-7B-Chat",target:"_blank",rel:"noopener noreferrer"},Se={href:"https://huggingface.co/HuggingFaceH4/starchat-beta",target:"_blank",rel:"noopener noreferrer"},$e={href:"https://huggingface.co/EleutherAI",target:"_blank",rel:"noopener noreferrer"},Ve={href:"https://huggingface.co/EleutherAI/pythia-6.9b",target:"_blank",rel:"noopener noreferrer"},He={href:"https://github.com/huggingface/peft",target:"_blank",rel:"noopener noreferrer"},je=e("code",null,"peft",-1),Oe=e("code",null,"PEFT_SHARE_BASE_WEIGHTS=true",-1),Qe={href:"https://github.com/lm-sys/FastChat",target:"_blank",rel:"noopener noreferrer"},Ke={href:"https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md",target:"_blank",rel:"noopener noreferrer"},Ze=e("p",null,[a("除本地模型外，本项目也支持直接接入 OpenAI API，具体设置可参考 "),e("code",null,"configs/model_configs.py.example"),a(" 中的 "),e("code",null,"llm_model_dict"),a(" 的 "),e("code",null,"openai-chatgpt-3.5"),a(" 配置信息。")],-1),Je=e("h4",{id:"embedding-模型支持",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#embedding-模型支持"},[e("span",null,"Embedding 模型支持")])],-1),Ye={href:"https://huggingface.co/models?pipeline_tag=sentence-similarity",target:"_blank",rel:"noopener noreferrer"},Xe={href:"https://huggingface.co/moka-ai/m3e-small",target:"_blank",rel:"noopener noreferrer"},ea={href:"https://huggingface.co/moka-ai/m3e-base",target:"_blank",rel:"noopener noreferrer"},aa={href:"https://huggingface.co/moka-ai/m3e-large",target:"_blank",rel:"noopener noreferrer"},na={href:"https://huggingface.co/BAAI/bge-small-zh",target:"_blank",rel:"noopener noreferrer"},sa={href:"https://huggingface.co/BAAI/bge-base-zh",target:"_blank",rel:"noopener noreferrer"},ta={href:"https://huggingface.co/BAAI/bge-large-zh",target:"_blank",rel:"noopener noreferrer"},la={href:"https://huggingface.co/BAAI/bge-large-zh-noinstruct",target:"_blank",rel:"noopener noreferrer"},ia={href:"https://huggingface.co/shibing624/text2vec-base-chinese-sentence",target:"_blank",rel:"noopener noreferrer"},oa={href:"https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase",target:"_blank",rel:"noopener noreferrer"},ra={href:"https://huggingface.co/shibing624/text2vec-base-multilingual",target:"_blank",rel:"noopener noreferrer"},ca={href:"https://huggingface.co/shibing624/text2vec-base-chinese",target:"_blank",rel:"noopener noreferrer"},pa={href:"https://huggingface.co/shibing624/text2vec-bge-large-chinese",target:"_blank",rel:"noopener noreferrer"},ha={href:"https://huggingface.co/GanymedeNil/text2vec-large-chinese",target:"_blank",rel:"noopener noreferrer"},da={href:"https://huggingface.co/nghuyong/ernie-3.0-nano-zh",target:"_blank",rel:"noopener noreferrer"},ua={href:"https://huggingface.co/nghuyong/ernie-3.0-base-zh",target:"_blank",rel:"noopener noreferrer"},ga={href:"https://platform.openai.com/docs/guides/embeddings",target:"_blank",rel:"noopener noreferrer"},ma=t(`<h2 id="docker-部署" tabindex="-1"><a class="header-anchor" href="#docker-部署"><span>Docker 部署</span></a></h2><p>🐳 Docker 镜像地址: <code>registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0)</code></p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-p</span> <span class="token number">80</span>:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>`,3),ba=t("<li>该版本镜像大小 <code>33.9GB</code>，使用 <code>v0.2.0</code>，以 <code>nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04</code> 为基础镜像</li><li>该版本内置一个 <code>embedding</code> 模型：<code>m3e-large</code>，内置 <code>chatglm2-6b-32k</code></li><li>该版本目标为方便一键部署使用，请确保您已经在Linux发行版上安装了NVIDIA驱动程序</li>",3),_a=e("code",null,"NVIDIA Driver",-1),va=e("code",null,"NVIDIA Container Toolkit",-1),ka={href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html",target:"_blank",rel:"noopener noreferrer"},fa=e("li",null,[a("首次拉取和启动均需要一定时间，首次启动时请参照下图使用 "),e("code",null,"docker logs -f <container id>"),a(" 查看日志")],-1),Ea=e("li",null,[a("如遇到启动过程卡在 "),e("code",null,"Waiting.."),a(" 步骤，建议使用 "),e("code",null,"docker exec -it <container id> bash"),a(" 进入 "),e("code",null,"/logs/"),a(" 目录查看对应阶段日志")],-1),Aa=e("h2",{id:"开发部署",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#开发部署"},[e("span",null,"开发部署")])],-1),ya=e("h3",{id:"环境准备",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#环境准备"},[e("span",null,"环境准备")])],-1),La=e("p",null,"开发环境准备",-1),xa=e("p",null,"本项目已在 Python 3.8.1 - 3.10，CUDA 11.7 环境下完成测试。已在 Windows、ARM 架构的 macOS、Linux 系统中完成测试。",-1),Ba={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/INSTALL.md",target:"_blank",rel:"noopener noreferrer"},Ca=t(`<p><strong>请注意：</strong> <code>0.2.0</code> 及更新版本的依赖包与 <code>0.1.x</code> 版本依赖包可能发生冲突，强烈建议新建环境后重新安装依赖包。</p><h4 id="环境检查" tabindex="-1"><a class="header-anchor" href="#环境检查"><span>环境检查</span></a></h4><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># 首先，确信你的机器安装了 Python 3.8 - 3.10 版本</span>
$ python <span class="token parameter variable">--version</span>
Python <span class="token number">3.8</span>.13

<span class="token comment"># 如果低于这个版本，可使用conda安装环境</span>
$ conda create <span class="token parameter variable">-p</span> /your_path/env_name <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span>

<span class="token comment"># 激活环境</span>
$ <span class="token builtin class-name">source</span> activate /your_path/env_name

<span class="token comment"># 或，conda安装，不指定路径, 注意以下，都将/your_path/env_name替换为env_name</span>
$ conda create <span class="token parameter variable">-n</span> env_name <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span>
$ conda activate env_name <span class="token comment"># Activate the environment</span>

<span class="token comment"># 更新py库</span>
$ pip3 <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pip

<span class="token comment"># 关闭环境</span>
$ <span class="token builtin class-name">source</span> deactivate /your_path/env_name

<span class="token comment"># 删除环境</span>
$ conda <span class="token function">env</span> remove <span class="token parameter variable">-p</span>  /your_path/env_name
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="项目依赖" tabindex="-1"><a class="header-anchor" href="#项目依赖"><span>项目依赖</span></a></h4><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># 拉取仓库</span>
$ <span class="token function">git</span> clone https://github.com/chatchat-space/Langchain-Chatchat.git

<span class="token comment"># 进入目录</span>
$ <span class="token builtin class-name">cd</span> Langchain-Chatchat

<span class="token comment"># 安装依赖 (三选一，为方便用户 API 与 webui 分离运行，可单独根据运行需求安装依赖包)</span>
$ pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt <span class="token comment"># 安装全部依赖</span>
<span class="token comment"># $ pip install -r requirements_api.txt # 如果只需运行 API</span>
<span class="token comment"># $ pip install -r requirements_webui.txt # 如果只需运行 WebUI</span>

<span class="token comment"># 默认依赖包括基本运行环境（FAISS向量库）。如果要使用 milvus/pg_vector 等向量库，请将 requirements.txt 中相应依赖取消注释再安装。</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,5),Ia=e("code",null,"langchain.document_loaders.UnstructuredFileLoader",-1),Da=e("code",null,".docx",-1),Ma={href:"https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html",target:"_blank",rel:"noopener noreferrer"},wa=e("h3",{id:"模型下载-llm-embedding",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#模型下载-llm-embedding"},[e("span",null,"模型下载 (LLM & Embedding)")])],-1),Pa=e("p",null,"下载模型至本地",-1),Fa={href:"https://huggingface.co/models",target:"_blank",rel:"noopener noreferrer"},Ta={href:"https://huggingface.co/THUDM/chatglm2-6b",target:"_blank",rel:"noopener noreferrer"},za={href:"https://huggingface.co/moka-ai/m3e-base",target:"_blank",rel:"noopener noreferrer"},Ua={href:"https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage",target:"_blank",rel:"noopener noreferrer"},qa=t(`<div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ <span class="token function">git</span> clone https://huggingface.co/THUDM/chatglm2-6b
$ <span class="token function">git</span> clone https://huggingface.co/moka-ai/m3e-base
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="配置选项" tabindex="-1"><a class="header-anchor" href="#配置选项"><span>配置选项</span></a></h3><p>设置配置项</p>`,3),Na={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/configs/model_config.py.example",target:"_blank",rel:"noopener noreferrer"},Wa=e("code",null,"./configs",-1),Ga=e("code",null,"model_config.py",-1),Ra={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/configs/server_config.py.example",target:"_blank",rel:"noopener noreferrer"},Sa=e("code",null,"./configs",-1),$a=e("code",null,"server_config.py",-1),Va=t(`<p>在开始执行 Web UI 或命令行交互前，请先检查 <code>configs/model_config.py</code> 和 <code>configs/server_config.py</code> 中的各项模型参数设计是否符合需求：</p><ul><li><p>请确认已下载至本地的 LLM 模型本地存储路径写在 <code>llm_model_dict</code> 对应模型的 <code>local_model_path</code> 属性中，如:</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>llm_model_dict<span class="token operator">=</span><span class="token punctuation">{</span>
    <span class="token string">&quot;chatglm2-6b&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;local_model_path&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;/Users/xxx/Downloads/chatglm2-6b&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;api_base_url&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;http://localhost:8888/v1&quot;</span><span class="token punctuation">,</span>  <span class="token comment"># &quot;name&quot;修改为 FastChat 服务中的&quot;api_base_url&quot;</span>
        <span class="token string">&quot;api_key&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;EMPTY&quot;</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>请确认已下载至本地的 Embedding 模型本地存储路径写在 <code>embedding_model_dict</code> 对应模型位置，如：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>embedding_model_dict <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;m3e-base&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;/Users/xxx/Downloads/m3e-base&quot;</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><p>如果你选择使用OpenAI的Embedding模型，请将模型的 <code>key</code>写入 <code>embedding_model_dict</code>中。使用该模型，你需要能够访问OpenAI官的API，或设置代理。</p><h3 id="知识库初始化与迁移" tabindex="-1"><a class="header-anchor" href="#知识库初始化与迁移"><span>知识库初始化与迁移</span></a></h3><p>当前项目的知识库信息存储在数据库中，在正式运行项目之前请先初始化数据库（我们强烈建议您在执行操作前备份您的知识文件）。</p><ul><li><p>如果您是从 <code>0.1.x</code> 版本升级过来的用户，针对已建立的知识库，请确认知识库的向量库类型、Embedding 模型与 <code>configs/model_config.py</code> 中默认设置一致，如无变化只需以下命令将现有知识库信息添加到数据库即可：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python init_database.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li><li><p>如果您是第一次运行本项目，知识库尚未建立，或者配置文件中的知识库类型、嵌入模型发生变化，或者之前的向量库没有开启 <code>normalize_L2</code>，需要以下命令初始化或重建知识库：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python init_database.py --recreate-vs
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li></ul><h3 id="一键启动api-服务或-web-ui" tabindex="-1"><a class="header-anchor" href="#一键启动api-服务或-web-ui"><span>一键启动API 服务或 Web UI</span></a></h3><h4 id="启动命令" tabindex="-1"><a class="header-anchor" href="#启动命令"><span>启动命令</span></a></h4><p>一键启动脚本 startup.py,一键启动所有 Fastchat 服务、API 服务、WebUI 服务，示例代码：</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>$ python startup.py -a
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>并可使用 <code>Ctrl + C</code> 直接关闭所有运行服务。如果一次结束不了，可以多按几次。</p><p>可选参数包括 <code>-a (或--all-webui)</code>, <code>--all-api</code>, <code>--llm-api</code>, <code>-c (或--controller)</code>, <code>--openai-api</code>, <code>-m (或--model-worker)</code>, <code>--api</code>, <code>--webui</code>，其中：</p><ul><li><code>--all-webui</code> 为一键启动 WebUI 所有依赖服务；</li><li><code>--all-api</code> 为一键启动 API 所有依赖服务；</li><li><code>--llm-api</code> 为一键启动 Fastchat 所有依赖的 LLM 服务；</li><li><code>--openai-api</code> 为仅启动 FastChat 的 controller 和 openai-api-server 服务；</li><li>其他为单独服务启动选项。</li></ul><h4 id="启动非默认模型" tabindex="-1"><a class="header-anchor" href="#启动非默认模型"><span>启动非默认模型</span></a></h4><p>若想指定非默认模型，需要用 <code>--model-name</code> 选项，示例：</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>$ python startup.py --all-webui --model-name Qwen-7B-Chat
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>更多信息可通过 <code>python startup.py -h</code>查看。</p><h4 id="多卡加载" tabindex="-1"><a class="header-anchor" href="#多卡加载"><span>多卡加载</span></a></h4><p>项目支持多卡加载，需在 startup.py 中的 create_model_worker_app 函数中，修改如下三个参数:</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>gpus=None, 
num_gpus=1, 
max_gpu_memory=&quot;20GiB&quot;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，<code>gpus</code> 控制使用的显卡的ID，例如 &quot;0,1&quot;;</p><p><code>num_gpus</code> 控制使用的卡数;</p><p><code>max_gpu_memory</code> 控制每个卡使用的显存容量。</p><p>注1：server_config.py的FSCHAT_MODEL_WORKERS字典中也增加了相关配置，如有需要也可通过修改FSCHAT_MODEL_WORKERS字典中对应参数实现多卡加载。</p><p>注2：少数情况下，gpus参数会不生效，此时需要通过设置环境变量CUDA_VISIBLE_DEVICES来指定torch可见的gpu,示例代码：</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>CUDA_VISIBLE_DEVICES=0,1 python startup.py 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h4 id="peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等" tabindex="-1"><a class="header-anchor" href="#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等"><span>PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)</span></a></h4><p>本项目基于 FastChat 加载 LLM 服务，故需以 FastChat 加载 PEFT 路径，即保证路径名称里必须有 peft 这个词，配置文件的名字为 adapter_config.json，peft 路径下包含.bin 格式的 PEFT 权重，peft路径在startup.py中create_model_worker_app函数的args.model_names中指定，并开启环境变量PEFT_SHARE_BASE_WEIGHTS=true参数。</p>`,28),Ha={href:"https://github.com/chatchat-space/Langchain-Chatchat/issues/1130#issuecomment-1685291822",target:"_blank",rel:"noopener noreferrer"},ja=t('<h4 id="注意事项" tabindex="-1"><a class="header-anchor" href="#注意事项"><span><strong>注意事项</strong></span></a></h4><p><strong>1. startup 脚本用多进程方式启动各模块的服务，可能会导致打印顺序问题，请等待全部服务发起后再调用，并根据默认或指定端口调用服务（默认 LLM API 服务端口：<code>127.0.0.1:8888</code>,默认 API 服务端口：<code>127.0.0.1:7861</code>,默认 WebUI 服务端口：<code>本机IP：8501</code>)</strong></p><p><strong>2.服务启动时间示设备不同而不同，约 3-10 分钟，如长时间没有启动请前往 <code>./logs</code>目录下监控日志，定位问题。</strong></p><p><strong>3. 在Linux上使用ctrl+C退出可能会由于linux的多进程机制导致multiprocessing遗留孤儿进程，可通过shutdown_all.sh进行退出</strong></p><h4 id="启动界面示例" tabindex="-1"><a class="header-anchor" href="#启动界面示例"><span>启动界面示例</span></a></h4><ol><li>FastAPI docs 界面</li></ol><p><img src="'+h+'" alt="image-20230906095433786" loading="lazy"></p><ol start="2"><li>webui启动界面示例：</li></ol>',8),Oa={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_0.png",target:"_blank",rel:"noopener noreferrer"},Qa=e("img",{src:"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png",alt:"img",loading:"lazy"},null,-1),Ka={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_1.png",target:"_blank",rel:"noopener noreferrer"},Za=e("img",{src:d,alt:"img",loading:"lazy"},null,-1),Ja=e("h3",{id:"分步启动-api-服务或-web-ui-一键启动忽略本节",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#分步启动-api-服务或-web-ui-一键启动忽略本节"},[e("span",null,"分步启动 API 服务或 Web UI（一键启动忽略本节）")])],-1),Ya=e("p",null,"注意：如使用了一键启动方式，可忽略本节。",-1),Xa=e("h4",{id:"启动-llm-服务-三种方式",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#启动-llm-服务-三种方式"},[e("span",null,"启动 LLM 服务 (三种方式)")])],-1),en=e("p",null,"如需使用开源模型进行本地部署，需首先启动 LLM 服务，启动方式分为三种：",-1),an={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1.1-%E5%9F%BA%E4%BA%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B%E8%84%9A%E6%9C%AC-llm_api.py-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1",target:"_blank",rel:"noopener noreferrer"},nn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1.2-%E5%9F%BA%E4%BA%8E%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%84%9A%E6%9C%AC-llm_api_stale.py-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1",target:"_blank",rel:"noopener noreferrer"},sn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1.3-PEFT-%E5%8A%A0%E8%BD%BD",target:"_blank",rel:"noopener noreferrer"},tn=e("p",null,"三种方式只需选择一个即可，具体操作方式详见 5.1.1 - 5.1.3。",-1),ln=e("p",null,"如果启动在线的API服务（如 OPENAI 的 API 接口），则无需启动 LLM 服务，即 5.1 小节的任何命令均无需启动。",-1),on=e("h5",{id:"基于多进程脚本-llm-api-py-启动-llm-服务",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#基于多进程脚本-llm-api-py-启动-llm-服务"},[e("span",null,"基于多进程脚本 llm_api.py 启动 LLM 服务")])],-1),rn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/llm_api.py",target:"_blank",rel:"noopener noreferrer"},cn=e("strong",null,"LLM 模型",-1),pn=t(`<div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>$ python server/llm_api.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>项目支持多卡加载，需在 llm_api.py 中的 create_model_worker_app 函数中，修改如下三个参数:</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>gpus=None, 
num_gpus=1, 
max_gpu_memory=&quot;20GiB&quot;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，<code>gpus</code> 控制使用的显卡的ID，如果 &quot;0,1&quot;;</p><p><code>num_gpus</code> 控制使用的卡数;</p><p><code>max_gpu_memory</code> 控制每个卡使用的显存容量。</p><h5 id="基于命令行脚本-llm-api-stale-py-启动-llm-服务" tabindex="-1"><a class="header-anchor" href="#基于命令行脚本-llm-api-stale-py-启动-llm-服务"><span>基于命令行脚本 llm_api_stale.py 启动 LLM 服务</span></a></h5><p>⚠️ <strong>注意:</strong></p><p><strong>1.llm_api_stale.py脚本原生仅适用于linux。mac设备需要安装对应的linux命令，win平台请使用wsl;</strong></p><p><strong>2.加载非默认模型需要用命令行参数--model-path-address指定模型，不会读取model_config.py配置;</strong></p>`,10),hn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/llm_api_stale.py",target:"_blank",rel:"noopener noreferrer"},dn=e("strong",null,"LLM 模型",-1),un=t(`<div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_stale.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>该方式支持启动多个worker，示例启动方式：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_stale.py --model-path-address model1@host1@port1 model2@host2@port2
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>如果出现server端口占用情况，需手动指定server端口,并同步修改model_config.py下对应模型的base_api_url为指定端口:</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_stale.py --server-port <span class="token number">8887</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>如果要启动多卡加载，示例命令如下：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_stale.py <span class="token parameter variable">--gpus</span> <span class="token number">0,1</span> --num-gpus <span class="token number">2</span> --max-gpu-memory 10GiB
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>注：以如上方式启动LLM服务会以nohup命令在后台运行 FastChat 服务，如需停止服务，可以运行如下命令：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_shutdown.py <span class="token parameter variable">--serve</span> all 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>亦可单独停止一个 FastChat 服务模块，可选 [<code>all</code>, <code>controller</code>, <code>model_worker</code>, <code>openai_api_server</code>]</p><h5 id="peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1" tabindex="-1"><a class="header-anchor" href="#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1"><span>PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)</span></a></h5>`,11),gn={href:"https://github.com/chatchat-space/Langchain-Chatchat/issues/1130#issuecomment-1685291822",target:"_blank",rel:"noopener noreferrer"},mn=e("p",null,[e("img",{src:u,alt:"image-20230906100313064",loading:"lazy"})],-1),bn=e("h4",{id:"启动-api-服务",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#启动-api-服务"},[e("span",null,"启动 API 服务")])],-1),_n={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1",target:"_blank",rel:"noopener noreferrer"},vn=e("strong",null,"启动 LLM 服务后",-1),kn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/api.py",target:"_blank",rel:"noopener noreferrer"},fn=e("strong",null,"API",-1),En={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/api.py",target:"_blank",rel:"noopener noreferrer"},An=e("strong",null,"API",-1),yn=t(`<p>调用命令示例：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/api.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>启动 API 服务后，可访问 <code>localhost:7861</code> 或 <code>{API 所在服务器 IP}:7861</code> FastAPI 自动生成的 docs 进行接口查看与测试。</p><ul><li>FastAPI docs 界面 <img src="`+g+'" alt="image-20230906100403529" loading="lazy"></li></ul><h4 id="启动-web-ui-服务" tabindex="-1"><a class="header-anchor" href="#启动-web-ui-服务"><span>启动 Web UI 服务</span></a></h4>',5),Ln={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.2-%E5%90%AF%E5%8A%A8-API-%E6%9C%8D%E5%8A%A1",target:"_blank",rel:"noopener noreferrer"},xn=e("strong",null,"启动 API 服务后",-1),Bn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/webui.py",target:"_blank",rel:"noopener noreferrer"},Cn=e("strong",null,"Web UI",-1),In=e("code",null,"8501",-1),Dn=t(`<div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ streamlit run webui.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>使用 Langchain-Chatchat 主题色启动 <strong>Web UI</strong> 服务（默认使用端口 <code>8501</code>）</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ streamlit run webui.py <span class="token parameter variable">--theme.base</span> <span class="token string">&quot;light&quot;</span> <span class="token parameter variable">--theme.primaryColor</span> <span class="token string">&quot;#165dff&quot;</span> <span class="token parameter variable">--theme.secondaryBackgroundColor</span> <span class="token string">&quot;#f5f5f5&quot;</span> <span class="token parameter variable">--theme.textColor</span> <span class="token string">&quot;#000000&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>或使用以下命令指定启动 <strong>Web UI</strong> 服务并指定端口号</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ streamlit run webui.py <span class="token parameter variable">--server.port</span> <span class="token number">666</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>`,5),Mn=e("p",null,"Web UI 对话界面：",-1),wn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_0.png",target:"_blank",rel:"noopener noreferrer"},Pn=e("img",{src:"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png",alt:"img",loading:"lazy"},null,-1),Fn=e("p",null,"Web UI 知识库管理页面：",-1),Tn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_1.png",target:"_blank",rel:"noopener noreferrer"},zn=e("img",{src:"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_1.png",alt:"img",loading:"lazy"},null,-1),Un=e("h2",{id:"常见问题",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#常见问题"},[e("span",null,"常见问题")])],-1),qn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/FAQ.md",target:"_blank",rel:"noopener noreferrer"},Nn=e("h2",{id:"路线图",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#路线图"},[e("span",null,"路线图")])],-1),Wn=e("ul",null,[e("li",null,"Langchain 应用"),e("li",null,[a("本地数据接入 - 接入非结构化文档 "),e("ul",null,[e("li",null,".md"),e("li",null,".txt"),e("li",null,".docx")])])],-1),Gn=e("pre",null,[e("code",null,`-  结构化数据接入
  -  .csv
  -  .xlsx
-  分词及召回
  -  接入不同类型 TextSplitter
  -  优化依据中文标点符号设计的 ChineseTextSplitter
  -  重新实现上下文拼接召回
-  本地网页接入
-  SQL 接入
-  知识图谱/图数据库接入
`)],-1),Rn=e("li",null,"搜索引擎接入 - Bing 搜索 - DuckDuckGo 搜索",-1),Sn=e("li",null,"Agent 实现",-1),$n=e("li",null,"LLM 模型接入",-1),Vn={href:"https://github.com/lm-sys/fastchat",target:"_blank",rel:"noopener noreferrer"},Hn=e("li",null,"支持 ChatGLM API 等 LLM API 的接入",-1),jn=e("li",null,"Embedding 模型接入",-1),On=e("li",null,"支持调用 HuggingFace 中各开源 Emebdding 模型",-1),Qn=e("li",null,"支持 OpenAI Embedding API 等 Embedding API 的接入",-1),Kn=e("li",null,"基于 FastAPI 的 API 方式调用",-1),Zn=e("li",null,"Web UI",-1),Jn=e("li",null,"基于 Streamlit 的 Web UI",-1),Yn=t('<h2 id="项目交流群" tabindex="-1"><a class="header-anchor" href="#项目交流群"><span>项目交流群</span></a></h2><img src="'+m+'" alt="image-20230906101805804" style="zoom:25%;"><h1 id="个人环境备注" tabindex="-1"><a class="header-anchor" href="#个人环境备注"><span>个人环境备注</span></a></h1><h2 id="llm模型替换问题" tabindex="-1"><a class="header-anchor" href="#llm模型替换问题"><span>LLM模型替换问题</span></a></h2><h3 id="换用更大参数量的模型" tabindex="-1"><a class="header-anchor" href="#换用更大参数量的模型"><span>换用更大参数量的模型</span></a></h3>',5),Xn=e("p",null,[a("他这里默认是6b，可以给7G的显卡用。但公司电脑是24G显存的3090Ti，可以换用更强的本地模型。 这里大概算一下，b就是billion 十亿。 "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mn",null,"7"),e("mi",null,"G"),e("mo",null,"="),e("mn",null,"7"),e("mo",null,"∗"),e("mn",null,"1"),e("msup",null,[e("mn",null,"0"),e("mrow",null,[e("mn",null,"3"),e("mo",null,"∗"),e("mn",null,"3")])]),e("mo",null,"="),e("mn",null,"7"),e("mi",null,"B")]),e("annotation",{encoding:"application/x-tex"},"7G=7*10^{3*3}=7B")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6833em"}}),e("span",{class:"mord"},"7"),e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),e("span",{class:"mrel"},"="),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6444em"}}),e("span",{class:"mord"},"7"),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),e("span",{class:"mbin"},"∗"),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8141em"}}),e("span",{class:"mord"},"1"),e("span",{class:"mord"},[e("span",{class:"mord"},"0"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"3"),e("span",{class:"mbin mtight"},"∗"),e("span",{class:"mord mtight"},"3")])])])])])])])]),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),e("span",{class:"mrel"},"="),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6833em"}}),e("span",{class:"mord"},"7"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.05017em"}},"B")])])]),a("，由于其他损耗就大概是7G显卡的样子。 但这也不一定，像 https://zhuanlan.zhihu.com/p/618690572 这里说这个130B缩放特性可以量化到100B级别，然后可以由 4x24G 来运行")],-1),es=e("p",null,"参考：",-1),as=e("ul",null,[e("li",null,"GPT-3 175B"),e("li",null,"OPT-175B"),e("li",null,"BLOOM-176B")],-1),ns=e("h3",{id:"同参数量的模型、量化or高参数量",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#同参数量的模型、量化or高参数量"},[e("span",null,"同参数量的模型、量化or高参数量")])],-1),ss=e("p",null,"另外，参数量相同的模型也会有差异。如新的优化方案导致性能不同，或者训练集不同，调参差异，导致的侧重点不同。以及双语的支持等，都是要考虑的重点。",-1),ts={href:"https://blog.csdn.net/nlpstarter/article/details/131129240",target:"_blank",rel:"noopener noreferrer"},ls=t('<p>模型比较</p><ul><li>Alpaca-Plus-13B：120G预训练，400万指令精调。 <ul><li>特点：高训练数据，Q8量化</li></ul></li><li>Alpaca-33B：20G预训练，400万指令精调 <ul><li>特点：模型量级大，Q4量化</li></ul></li></ul><p>测试</p><ul><li>温室效应问题：33B的回答比较简练，内容长度上不占优</li><li>数学问题：骑7个猴。<strong>33B的完胜</strong>Plus-13B，可能模型量级对于这种数值计算和推理类的有较大优势吧</li><li>如何制作宫保鸡丁？：差不多</li><li>写一封信：Plus-13B占优一些，内容详实。可能33B吃了训练数据少的亏，写的内容不是特别生动</li><li>代码方面：<strong>33B显著胜出</strong></li><li>角色扮演：差不多，13回复长，略优</li></ul><p>总结</p><blockquote><p>Plus-13B相比之前的7B/13B已经有显著性能提升了，尤其是在生成类的任务上内容更加详实。33B的优缺点比较明显，优点是代码能力和数值计算方面确实比之前高出一截，但是在文本生成类的任务上效果略低于plus-13B。不过33B是基础版，这么比可能有点不讲武德，哈哈。这样其实就比较期待后续plus-33b的效果了，生成类任务的效果应该会有一个提升。</p></blockquote><h3 id="羊驼系模型" tabindex="-1"><a class="header-anchor" href="#羊驼系模型"><span>羊驼系模型</span></a></h3><p>参考：</p>',8),is={href:"https://zhuanlan.zhihu.com/p/106262896",target:"_blank",rel:"noopener noreferrer"},os={href:"https://www.cnblogs.com/jiangxinyang/p/17310398.html",target:"_blank",rel:"noopener noreferrer"},rs=e("li",null,"https://blog.csdn.net/v_JULY_v/article/details/129709105",-1),cs={href:"https://github.com/lm-sys/FastChat",target:"_blank",rel:"noopener noreferrer"},ps=t(`<p>一些模型图标的区分：https://kaiyuan.blog.csdn.net/article/details/131467606</p><h2 id="bug1-无法找到显卡驱动" tabindex="-1"><a class="header-anchor" href="#bug1-无法找到显卡驱动"><span>bug1 无法找到显卡驱动</span></a></h2><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token punctuation">(</span>base<span class="token punctuation">)</span> root@msy-System-Product-Name:~<span class="token comment"># docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0</span>
4bcc140c4c5b24e21da7ba6bf863d372f899ddd20e141469959b08ac45b5cf5a
docker: Error response from daemon: could not <span class="token keyword">select</span> device driver <span class="token string">&quot;&quot;</span> with capabilities: <span class="token punctuation">[</span><span class="token punctuation">[</span>gpu<span class="token punctuation">]</span><span class="token punctuation">]</span>.
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>解决该bug</p><hr><p>这个错误通常发生在 Docker 无法在你的机器上找到正确的驱动程序来使用 GPU 的情况下。以下是一些可以解决这个问题的步骤：</p><ol><li><p><strong>检查Docker版本是否支持 --gpus 选项</strong></p><p>Docker从19.03版本开始支持原生使用GPU。用命令 <code>docker --version</code> 检查Docker版本，如果你使用的是更旧的版本，考虑更新Docker至最新版本。</p></li><li><p><strong>检查是否安装了 NVIDIA 容器工具箱</strong></p><p>如果你在docker中使用GPU，你需要安装nvidia-docker2，并在你的docker配置中添加NVIDIA运行时。如果没有安装，按照以下步骤进行安装</p></li></ol><p>a. <strong>移除旧版本</strong></p><p>旧版本的 Docker 由名为 <code>nvidia-docker2</code> 的包支持。新的 Docker 版本由 NVIDIA 容器工具箱支持。如果你已经安装了旧的 <code>nvidia-docker2</code> 包，请移除它：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">sudo</span> <span class="token function">apt-get</span> purge <span class="token parameter variable">-y</span> nvidia-docker
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>b. <strong>安装 NVIDIA 容器工具箱</strong> - 添加包存储库：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">curl</span> <span class="token parameter variable">-s</span> <span class="token parameter variable">-L</span> https://nvidia.github.io/nvidia-docker/gpgkey <span class="token operator">|</span> <span class="token function">sudo</span> apt-key <span class="token function">add</span> -
<span class="token assign-left variable">distribution</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token builtin class-name">.</span> /etc/os-release<span class="token punctuation">;</span><span class="token builtin class-name">echo</span> $ID$VERSION_ID<span class="token variable">)</span></span>
<span class="token function">curl</span> <span class="token parameter variable">-s</span> <span class="token parameter variable">-L</span> https://nvidia.github.io/nvidia-docker/<span class="token variable">$distribution</span>/nvidia-docker.list <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">tee</span> /etc/apt/sources.list.d/nvidia-docker.list
<span class="token function">sudo</span> <span class="token function">apt-get</span> update
<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> nvidia-docker2
<span class="token function">sudo</span> systemctl restart <span class="token function">docker</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>c. <strong>测试 NVIDIA Docker 安装</strong> - 通过运行一个示例来测试 NVIDIA Docker 的安装是否成功：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">--rm</span> <span class="token parameter variable">--gpus</span> all nvidia/cuda:11.0-base nvidia-smi
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ol><li>如果仍然存在相同的错误，那么你应该检查Docker是否可以访问GPU。你可以通过运行命令 <code>nvidia-smi</code> 来检查。如果你可以看到你的GPU列表，Docker也应该能够使用它们。</li><li>确保您的机器上正确安装了 NVIDIA 驱动程序。您可以使用命令 <code>nvidia-smi</code> 来验证。</li></ol><p>如果问题仍然存在，请提供更多信息，以便我能更深入地帮助你。</p><h2 id="bug2-回答为空" tabindex="-1"><a class="header-anchor" href="#bug2-回答为空"><span>bug2 回答为空</span></a></h2><p>群里另一个用 docker 方式的，也遇到了相同的问题。决定放弃docker方案，转手动部署。但我不想重新下这些模型包了，就在原docker的基础上重新部署吧</p><p>原</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-p</span> <span class="token number">80</span>:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>现</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># 可选，删掉旧的docker</span>
<span class="token function">docker</span> stop xxx
dcker <span class="token function">rm</span> xxx

<span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">--name</span><span class="token operator">=</span>chatchat2 <span class="token parameter variable">--restart</span><span class="token operator">=</span>always <span class="token parameter variable">-p</span> <span class="token number">80</span>:8501 f224e85162da

<span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> chatchat2 /bin/bash
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="bug3-git问题" tabindex="-1"><a class="header-anchor" href="#bug3-git问题"><span>bug3 git问题</span></a></h2><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token punctuation">(</span>base<span class="token punctuation">)</span> root@msy-System-Product-Name:~/chatchat<span class="token comment"># git clone https://huggingface.co/moka-ai/m3e-base</span>
正克隆到 <span class="token string">&#39;m3e-base&#39;</span><span class="token punctuation">..</span>.
remote: Enumerating objects: <span class="token number">108</span>, done.
remote: Counting objects: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">16</span>/16<span class="token punctuation">)</span>, done.
remote: Compressing objects: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">15</span>/15<span class="token punctuation">)</span>, done.
remote: Total <span class="token number">108</span> <span class="token punctuation">(</span>delta <span class="token number">6</span><span class="token punctuation">)</span>, reused <span class="token number">0</span> <span class="token punctuation">(</span>delta <span class="token number">0</span><span class="token punctuation">)</span>, pack-reused <span class="token number">92</span>
接收对象中: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">108</span>/108<span class="token punctuation">)</span>, <span class="token number">194.12</span> KiB <span class="token operator">|</span> <span class="token number">2.55</span> MiB/s, 完成.
处理 delta 中: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">57</span>/57<span class="token punctuation">)</span>, 完成.
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>git clone卡在这一步</p><p>尝试用安装 Git LFS 解决该问题。</p><p>在 Ubuntu 上安装 Git LFS 可以使用以下命令：</p><p>旧：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># 安装必要的软件包</span>
$ <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">git</span>

<span class="token comment"># 下载并安装git-lfs</span>
$ <span class="token function">curl</span> <span class="token parameter variable">-s</span> https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">bash</span>
$ <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> git-lfs

<span class="token comment"># 初始化git-lfs</span>
$ <span class="token function">git</span> lfs <span class="token function">install</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>新：</p><p>见官网链接</p><p>但看起来没成功</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token punctuation">(</span>base<span class="token punctuation">)</span> root@msy-System-Product-Name:~/chatchat<span class="token comment"># git lfs install</span>
Error: Failed to call <span class="token function">git</span> rev-parse --git-dir: <span class="token builtin class-name">exit</span> status <span class="token number">128</span>
Git LFS initialized.
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>后来直接复制黏贴了，不git了。其实就是10G的git时间太慢了而已。耐心等也可以，我选择先用有线下载再用wifi传过去</p><h2 id="bug4-找不到存在的路径" tabindex="-1"><a class="header-anchor" href="#bug4-找不到存在的路径"><span>bug4 找不到存在的路径</span></a></h2><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>FileNotFoundError: <span class="token punctuation">[</span>Errno <span class="token number">2</span><span class="token punctuation">]</span> No usable temporary directory found <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">&#39;/tmp&#39;</span>, <span class="token string">&#39;/var/tmp&#39;</span>, <span class="token string">&#39;/usr/tmp&#39;</span>, <span class="token string">&#39;/root/chatchat/Langchain-Chatchat&#39;</span><span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>但路径都是真实存在的</p><p>后来发现应该是我硬盘空间满了的原因</p><h2 id="bug5-一直卡在-wait-controller-running" tabindex="-1"><a class="header-anchor" href="#bug5-一直卡在-wait-controller-running"><span>bug5 一直卡在 wait controller running</span></a></h2><p>分步运行中，用 <code> python server/llm_api_stale.py</code> 则一直卡在wait controller running</p><p>换用回一键启动的命令，群友说那个分步启用的可能官方要不维护了</p><h2 id="bug6-爆显存" tabindex="-1"><a class="header-anchor" href="#bug6-爆显存"><span>bug6 爆显存</span></a></h2><p>重启电脑解决</p>`,43);function hs(ds,us){const n=i("ExternalLinkIcon");return o(),r("div",null,[_,v,k,e("ul",null,[f,e("li",null,[e("a",E,[a("Install.md"),s(n)])]),e("li",null,[e("a",A,[a("在Anaconda中使用pip安装包的问题.md"),s(n)])]),y,L,e("li",null,[e("a",x,[a("在Anaconda中使用pip安装包无效问题.md"),s(n)])])]),B,C,I,e("ul",null,[e("li",null,[e("a",D,[a("介绍"),s(n)])]),e("li",null,[e("a",M,[a("变更日志"),s(n)])]),e("li",null,[e("a",w,[a("模型支持"),s(n)])]),e("li",null,[e("a",P,[a("Docker 部署"),s(n)])]),e("li",null,[a("开发部署 "),e("ul",null,[e("li",null,[e("a",F,[a("软件需求"),s(n)])]),e("li",null,[e("a",T,[a("1. 开发环境准备"),s(n)])]),e("li",null,[e("a",z,[a("2. 下载模型至本地"),s(n)])]),e("li",null,[e("a",U,[a("3. 设置配置项"),s(n)])]),e("li",null,[e("a",q,[a("4. 知识库初始化与迁移"),s(n)])]),e("li",null,[e("a",N,[a("5. 一键启动API服务或WebUI服务"),s(n)])]),e("li",null,[e("a",W,[a("6. 分步启动 API 服务或 Web UI"),s(n)])])])]),e("li",null,[e("a",G,[a("常见问题"),s(n)])]),e("li",null,[e("a",R,[a("路线图"),s(n)])]),e("li",null,[e("a",S,[a("项目交流群"),s(n)])])]),$,e("p",null,[a("🤖️ 一种利用 "),e("a",V,[a("langchain"),s(n)]),a(" 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。")]),e("p",null,[a("💡 受 "),e("a",H,[a("GanymedeNil"),s(n)]),a(" 的项目 "),e("a",j,[a("document.ai"),s(n)]),a(" 和 "),e("a",O,[a("AlexZhangji"),s(n)]),a(" 创建的 "),e("a",Q,[a("ChatGLM-6B Pull Request"),s(n)]),a(" 启发，建立了全流程可使用开源模型实现的本地知识库问答应用。本项目的最新版本中通过使用 "),e("a",K,[a("FastChat"),s(n)]),a(" 接入 Vicuna, Alpaca, LLaMA, Koala, RWKV 等模型，依托于 "),e("a",Z,[a("langchain"),s(n)]),a(" 框架支持通过基于 "),e("a",J,[a("FastAPI"),s(n)]),a(" 提供的 API 调用服务，或使用基于 "),e("a",Y,[a("Streamlit"),s(n)]),a(" 的 WebUI 进行操作。")]),X,e("p",null,[a("📺 "),e("a",ee,[a("原理介绍视频"),s(n)])]),ae,ne,se,te,e("p",null,[a("🌐 "),e("a",le,[a("AutoDL 镜像"),s(n)]),a(" 中 "),ie,a(" 版本所使用代码已更新至本项目 "),oe,a(" 版本。")]),e("p",null,[a("🐳 "),e("a",re,[a("Docker 镜像"),s(n)])]),ce,e("p",null,[a("本项目中默认使用的 LLM 模型为 "),e("a",pe,[a("THUDM/chatglm2-6b"),s(n)]),a("，默认使用的 Embedding 模型为 "),e("a",he,[a("moka-ai/m3e-base"),s(n)]),a(" 为例。")]),de,e("p",null,[a("本项目最新版本中基于 "),e("a",ue,[a("FastChat"),s(n)]),a(" 进行本地 LLM 模型接入，支持模型如下：")]),e("ul",null,[e("li",null,[e("a",ge,[a("meta-llama/Llama-2-7b-chat-hf"),s(n)])]),me,e("li",null,[e("a",be,[a("BlinkDL/RWKV-4-Raven"),s(n)])]),e("li",null,[e("a",_e,[a("camel-ai/CAMEL-13B-Combined-Data"),s(n)])]),e("li",null,[e("a",ve,[a("databricks/dolly-v2-12b"),s(n)])]),e("li",null,[e("a",ke,[a("FreedomIntelligence/phoenix-inst-chat-7b"),s(n)])]),e("li",null,[e("a",fe,[a("h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b"),s(n)])]),e("li",null,[e("a",Ee,[a("lcw99/polyglot-ko-12.8b-chang-instruct-chat"),s(n)])]),e("li",null,[e("a",Ae,[a("lmsys/fastchat-t5-3b-v1.0"),s(n)])]),e("li",null,[e("a",ye,[a("mosaicml/mpt-7b-chat"),s(n)])]),e("li",null,[e("a",Le,[a("Neutralzz/BiLLa-7B-SFT"),s(n)])]),e("li",null,[e("a",xe,[a("nomic-ai/gpt4all-13b-snoozy"),s(n)])]),e("li",null,[e("a",Be,[a("NousResearch/Nous-Hermes-13b"),s(n)])]),e("li",null,[e("a",Ce,[a("openaccess-ai-collective/manticore-13b-chat-pyg"),s(n)])]),e("li",null,[e("a",Ie,[a("OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"),s(n)])]),e("li",null,[e("a",De,[a("project-baize/baize-v2-7b"),s(n)])]),e("li",null,[e("a",Me,[a("Salesforce/codet5p-6b"),s(n)])]),e("li",null,[e("a",we,[a("StabilityAI/stablelm-tuned-alpha-7b"),s(n)])]),e("li",null,[e("a",Pe,[a("THUDM/chatglm-6b"),s(n)])]),e("li",null,[e("a",Fe,[a("THUDM/chatglm2-6b"),s(n)])]),e("li",null,[e("a",Te,[a("tiiuae/falcon-40b"),s(n)])]),e("li",null,[e("a",ze,[a("timdettmers/guanaco-33b-merged"),s(n)])]),e("li",null,[e("a",Ue,[a("togethercomputer/RedPajama-INCITE-7B-Chat"),s(n)])]),e("li",null,[e("a",qe,[a("WizardLM/WizardLM-13B-V1.0"),s(n)])]),e("li",null,[e("a",Ne,[a("WizardLM/WizardCoder-15B-V1.0"),s(n)])]),e("li",null,[e("a",We,[a("baichuan-inc/baichuan-7B"),s(n)])]),e("li",null,[e("a",Ge,[a("internlm/internlm-chat-7b"),s(n)])]),e("li",null,[e("a",Re,[a("Qwen/Qwen-7B-Chat"),s(n)])]),e("li",null,[e("a",Se,[a("HuggingFaceH4/starchat-beta"),s(n)])]),e("li",null,[a("任何 "),e("a",$e,[a("EleutherAI"),s(n)]),a(" 的 pythia 模型，如 "),e("a",Ve,[a("pythia-6.9b"),s(n)])]),e("li",null,[a("在以上模型基础上训练的任何 "),e("a",He,[a("Peft"),s(n)]),a(" 适配器。为了激活，模型路径中必须有 "),je,a(" 。注意：如果加载多个peft模型，你可以通过在任何模型工作器中设置环境变量 "),Oe,a(" 来使它们共享基础模型的权重。")])]),e("p",null,[a("以上模型支持列表可能随 "),e("a",Qe,[a("FastChat"),s(n)]),a(" 更新而持续更新，可参考 "),e("a",Ke,[a("FastChat 已支持模型列表"),s(n)]),a("。")]),Ze,Je,e("p",null,[a("本项目支持调用 "),e("a",Ye,[a("HuggingFace"),s(n)]),a(" 中的 Embedding 模型，已支持的 Embedding 模型如下：")]),e("ul",null,[e("li",null,[e("a",Xe,[a("moka-ai/m3e-small"),s(n)])]),e("li",null,[e("a",ea,[a("moka-ai/m3e-base"),s(n)])]),e("li",null,[e("a",aa,[a("moka-ai/m3e-large"),s(n)])]),e("li",null,[e("a",na,[a("BAAI/bge-small-zh"),s(n)])]),e("li",null,[e("a",sa,[a("BAAI/bge-base-zh"),s(n)])]),e("li",null,[e("a",ta,[a("BAAI/bge-large-zh"),s(n)])]),e("li",null,[e("a",la,[a("BAAI/bge-large-zh-noinstruct"),s(n)])]),e("li",null,[e("a",ia,[a("shibing624/text2vec-base-chinese-sentence"),s(n)])]),e("li",null,[e("a",oa,[a("shibing624/text2vec-base-chinese-paraphrase"),s(n)])]),e("li",null,[e("a",ra,[a("shibing624/text2vec-base-multilingual"),s(n)])]),e("li",null,[e("a",ca,[a("shibing624/text2vec-base-chinese"),s(n)])]),e("li",null,[e("a",pa,[a("shibing624/text2vec-bge-large-chinese"),s(n)])]),e("li",null,[e("a",ha,[a("GanymedeNil/text2vec-large-chinese"),s(n)])]),e("li",null,[e("a",da,[a("nghuyong/ernie-3.0-nano-zh"),s(n)])]),e("li",null,[e("a",ua,[a("nghuyong/ernie-3.0-base-zh"),s(n)])]),e("li",null,[e("a",ga,[a("OpenAI/text-embedding-ada-002"),s(n)])])]),ma,e("ul",null,[ba,e("li",null,[a("请注意，您不需要在主机系统上安装CUDA工具包，但需要安装 "),_a,a(" 以及 "),va,a("，请参考"),e("a",ka,[a("安装指南"),s(n)])]),fa,Ea]),Aa,ya,La,xa,e("p",null,[a("参见 "),e("a",Ba,[a("开发环境准备"),s(n)]),a("。")]),Ca,e("p",null,[a("注：使用 "),Ia,a(" 进行 "),Da,a(" 等格式非结构化文件接入时，可能需要依据文档进行其他依赖包的安装，请参考 "),e("a",Ma,[a("langchain 文档"),s(n)]),a("。")]),wa,Pa,e("p",null,[a("如需在本地或离线环境下运行本项目，需要首先将项目所需的模型下载至本地，通常开源 LLM 与 Embedding 模型可以从 "),e("a",Fa,[a("HuggingFace"),s(n)]),a(" 下载。")]),e("p",null,[a("以本项目中默认使用的 LLM 模型 "),e("a",Ta,[a("THUDM/chatglm2-6b"),s(n)]),a(" 与 Embedding 模型 "),e("a",za,[a("moka-ai/m3e-base"),s(n)]),a(" 为例：")]),e("p",null,[a("下载模型需要先"),e("a",Ua,[a("安装Git LFS"),s(n)]),a("，然后运行")]),qa,e("p",null,[a("复制模型相关参数配置模板文件 "),e("a",Na,[a("configs/model_config.py.example"),s(n)]),a(" 存储至项目路径下 "),Wa,a(" 路径下，并重命名为 "),Ga,a("。")]),e("p",null,[a("复制服务相关参数配置模板文件 "),e("a",Ra,[a("configs/server_config.py.example"),s(n)]),a(" 存储至项目路径下 "),Sa,a(" 路径下，并重命名为 "),$a,a("。")]),Va,e("p",null,[a("注：如果上述方式启动失败，则需要以标准的fastchat服务启动方式分步启动，分步启动步骤参考第六节，PEFT加载详细步骤参考"),e("a",Ha,[a("加载lora微调后模型失效"),s(n)]),a("，")]),ja,e("ul",null,[e("li",null,[a("Web UI 对话界面： "),e("a",Oa,[Qa,s(n)])]),e("li",null,[a("Web UI 知识库管理页面： "),e("a",Ka,[Za,s(n)])])]),Ja,Ya,Xa,en,e("ul",null,[e("li",null,[e("a",an,[a("基于多进程脚本 llm_api.py 启动 LLM 服务"),s(n)])]),e("li",null,[e("a",nn,[a("基于命令行脚本 llm_api_stale.py 启动 LLM 服务"),s(n)])]),e("li",null,[e("a",sn,[a("PEFT 加载"),s(n)])])]),tn,ln,on,e("p",null,[a("在项目根目录下，执行 "),e("a",rn,[a("server/llm_api.py"),s(n)]),a(" 脚本启动 "),cn,a("服务：")]),pn,e("p",null,[a("在项目根目录下，执行 "),e("a",hn,[a("server/llm_api_stale.py"),s(n)]),a(" 脚本启动 "),dn,a("服务：")]),un,e("p",null,[a("本项目基于 FastChat 加载 LLM 服务，故需以 FastChat 加载 PEFT 路径，即保证路径名称里必须有 peft 这个词，配置文件的名字为 adapter_config.json，peft 路径下包含 model.bin 格式的 PEFT 权重。 详细步骤参考"),e("a",gn,[a("加载lora微调后模型失效"),s(n)])]),mn,bn,e("p",null,[a("本地部署情况下，按照 "),e("a",_n,[a("5.1 节"),s(n)]),vn,a("，再执行 "),e("a",kn,[a("server/api.py"),s(n)]),a(" 脚本启动 "),fn,a(" 服务；")]),e("p",null,[a("在线调用API服务的情况下，直接执执行 "),e("a",En,[a("server/api.py"),s(n)]),a(" 脚本启动 "),An,a(" 服务；")]),yn,e("p",null,[a("按照 "),e("a",Ln,[a("5.2 节"),s(n)]),xn,a("，执行 "),e("a",Bn,[a("webui.py"),s(n)]),a(" 启动 "),Cn,a(" 服务（默认使用端口 "),In,a("）")]),Dn,e("ul",null,[e("li",null,[Mn,e("p",null,[e("a",wn,[Pn,s(n)])])]),e("li",null,[Fn,e("p",null,[e("a",Tn,[zn,s(n)])])])]),Un,e("p",null,[a("参见 "),e("a",qn,[a("常见问题"),s(n)]),a("。")]),Nn,Wn,Gn,e("ul",null,[Rn,Sn,$n,e("li",null,[a("支持通过调用 "),e("a",Vn,[a("FastChat"),s(n)]),a(" api 调用 llm")]),Hn,jn,On,Qn,Kn,Zn,Jn]),Yn,Xn,es,as,ns,ss,e("p",null,[a("参考："),e("a",ts,[a("中文羊驼大模型Alpaca-Plus-13B、Alpaca-33B效果大比拼"),s(n)])]),ls,e("ul",null,[e("li",null,[e("a",is,[a("Guanaco, Llama, Vicuña, Alpaca该怎么区别"),s(n)])]),e("li",null,[e("a",os,[a("大模型入门（一）—— LLaMa/Alpaca/Vicuna"),s(n)])]),rs]),e("p",null,[a("Vicuna是在LLaMa-13B的基础上使用监督数据微调得到的模型，数据集来自于ShareGPT.com 产生的用户对话数据，共70K条。使用Pytorch FSDP在8张A100上训练了一天。相较于Alpaca，Vicuna在训练中将序列长度由512扩展到了2048，并且通过梯度检测和flash attention来解决内存问题；调整训练损失考虑多轮对话，并仅根据模型的输出进行微调。通过GPT4来打分评测，Vicuna可以达到ChatGPT 90%的效果。并且还提供了可调用的分布式聊天服务"),e("a",cs,[a("FastChat"),s(n)]),a("。")]),ps])}const bs=l(b,[["render",hs],["__file","Langchain-Chatchat.html.vue"]]),_s=JSON.parse('{"path":"/MdNote_Public/01.%20%E8%AE%BE%E8%AE%A1%E5%BC%80%E5%8F%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%94%9F%E4%BA%A7/Develop/04.%20Project/Type/Artificial_Intelligence/%E5%AD%97%E5%85%B8%E5%9E%8B/10.%20%E5%B7%A5%E5%85%B7/Langchain-Chatchat.html","title":"Langchain-Chatchat","lang":"zh-CN","frontmatter":{"description":"Langchain-Chatchat 目录 项目文档目录 FAQ.md Install.md 在Anaconda中使用pip安装包的问题.md 向量库环境 docker.md 启动API服务.md 在Anaconda中使用pip安装包无效问题.md 项目README 参考：https://github.com/chatchat-space/Langch...","head":[["meta",{"property":"og:url","content":"http://192.168.0.101:8080/MdNote_Public/01.%20%E8%AE%BE%E8%AE%A1%E5%BC%80%E5%8F%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%94%9F%E4%BA%A7/Develop/04.%20Project/Type/Artificial_Intelligence/%E5%AD%97%E5%85%B8%E5%9E%8B/10.%20%E5%B7%A5%E5%85%B7/Langchain-Chatchat.html"}],["meta",{"property":"og:site_name","content":"Linc 的小站"}],["meta",{"property":"og:title","content":"Langchain-Chatchat"}],["meta",{"property":"og:description","content":"Langchain-Chatchat 目录 项目文档目录 FAQ.md Install.md 在Anaconda中使用pip安装包的问题.md 向量库环境 docker.md 启动API服务.md 在Anaconda中使用pip安装包无效问题.md 项目README 参考：https://github.com/chatchat-space/Langch..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"LincZero"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Langchain-Chatchat\\",\\"image\\":[\\"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png\\",\\"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png\\",\\"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_1.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LincZero\\",\\"url\\":\\"https://github.com/LincZero/\\"}]}"]]},"headers":[{"level":1,"title":"Langchain-Chatchat","slug":"langchain-chatchat","link":"#langchain-chatchat","children":[]},{"level":1,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":1,"title":"项目README","slug":"项目readme","link":"#项目readme","children":[{"level":2,"title":"介绍","slug":"介绍","link":"#介绍","children":[{"level":3,"title":"模型支持","slug":"模型支持","link":"#模型支持","children":[{"level":4,"title":"LLM 模型支持","slug":"llm-模型支持","link":"#llm-模型支持","children":[]},{"level":4,"title":"Embedding 模型支持","slug":"embedding-模型支持","link":"#embedding-模型支持","children":[]}]}]},{"level":2,"title":"Docker 部署","slug":"docker-部署","link":"#docker-部署","children":[]},{"level":2,"title":"开发部署","slug":"开发部署","link":"#开发部署","children":[{"level":3,"title":"环境准备","slug":"环境准备","link":"#环境准备","children":[{"level":4,"title":"环境检查","slug":"环境检查","link":"#环境检查","children":[]},{"level":4,"title":"项目依赖","slug":"项目依赖","link":"#项目依赖","children":[]}]},{"level":3,"title":"模型下载 (LLM & Embedding)","slug":"模型下载-llm-embedding","link":"#模型下载-llm-embedding","children":[]},{"level":3,"title":"配置选项","slug":"配置选项","link":"#配置选项","children":[]},{"level":3,"title":"知识库初始化与迁移","slug":"知识库初始化与迁移","link":"#知识库初始化与迁移","children":[]},{"level":3,"title":"一键启动API 服务或 Web UI","slug":"一键启动api-服务或-web-ui","link":"#一键启动api-服务或-web-ui","children":[{"level":4,"title":"启动命令","slug":"启动命令","link":"#启动命令","children":[]},{"level":4,"title":"启动非默认模型","slug":"启动非默认模型","link":"#启动非默认模型","children":[]},{"level":4,"title":"多卡加载","slug":"多卡加载","link":"#多卡加载","children":[]},{"level":4,"title":"PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)","slug":"peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等","link":"#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等","children":[]},{"level":4,"title":"注意事项","slug":"注意事项","link":"#注意事项","children":[]},{"level":4,"title":"启动界面示例","slug":"启动界面示例","link":"#启动界面示例","children":[]}]},{"level":3,"title":"分步启动 API 服务或 Web UI（一键启动忽略本节）","slug":"分步启动-api-服务或-web-ui-一键启动忽略本节","link":"#分步启动-api-服务或-web-ui-一键启动忽略本节","children":[{"level":4,"title":"启动 LLM 服务 (三种方式)","slug":"启动-llm-服务-三种方式","link":"#启动-llm-服务-三种方式","children":[{"level":5,"title":"基于多进程脚本 llm_api.py 启动 LLM 服务","slug":"基于多进程脚本-llm-api-py-启动-llm-服务","link":"#基于多进程脚本-llm-api-py-启动-llm-服务","children":[]},{"level":5,"title":"基于命令行脚本 llm_api_stale.py 启动 LLM 服务","slug":"基于命令行脚本-llm-api-stale-py-启动-llm-服务","link":"#基于命令行脚本-llm-api-stale-py-启动-llm-服务","children":[]},{"level":5,"title":"PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)","slug":"peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1","link":"#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1","children":[]}]},{"level":4,"title":"启动 API 服务","slug":"启动-api-服务","link":"#启动-api-服务","children":[]},{"level":4,"title":"启动 Web UI 服务","slug":"启动-web-ui-服务","link":"#启动-web-ui-服务","children":[]}]}]},{"level":2,"title":"常见问题","slug":"常见问题","link":"#常见问题","children":[]},{"level":2,"title":"路线图","slug":"路线图","link":"#路线图","children":[]},{"level":2,"title":"项目交流群","slug":"项目交流群","link":"#项目交流群","children":[]}]},{"level":1,"title":"个人环境备注","slug":"个人环境备注","link":"#个人环境备注","children":[{"level":2,"title":"LLM模型替换问题","slug":"llm模型替换问题","link":"#llm模型替换问题","children":[{"level":3,"title":"换用更大参数量的模型","slug":"换用更大参数量的模型","link":"#换用更大参数量的模型","children":[]},{"level":3,"title":"同参数量的模型、量化or高参数量","slug":"同参数量的模型、量化or高参数量","link":"#同参数量的模型、量化or高参数量","children":[]},{"level":3,"title":"羊驼系模型","slug":"羊驼系模型","link":"#羊驼系模型","children":[]}]},{"level":2,"title":"bug1 无法找到显卡驱动","slug":"bug1-无法找到显卡驱动","link":"#bug1-无法找到显卡驱动","children":[]},{"level":2,"title":"bug2 回答为空","slug":"bug2-回答为空","link":"#bug2-回答为空","children":[]},{"level":2,"title":"bug3 git问题","slug":"bug3-git问题","link":"#bug3-git问题","children":[]},{"level":2,"title":"bug4 找不到存在的路径","slug":"bug4-找不到存在的路径","link":"#bug4-找不到存在的路径","children":[]},{"level":2,"title":"bug5 一直卡在 wait controller running","slug":"bug5-一直卡在-wait-controller-running","link":"#bug5-一直卡在-wait-controller-running","children":[]},{"level":2,"title":"bug6 爆显存","slug":"bug6-爆显存","link":"#bug6-爆显存","children":[]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":20.85,"words":6255},"filePathRelative":"MdNote_Public/01. 设计开发与数据生产/Develop/04. Project/Type/Artificial_Intelligence/字典型/10. 工具/Langchain-Chatchat.md","excerpt":"\\n<h1>目录</h1>\\n<p>项目文档目录</p>\\n<ul>\\n<li>FAQ.md</li>\\n<li><a href=\\"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/INSTALL.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Install.md</a></li>\\n<li><a href=\\"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/Issue-with-Installing-Packages-Using-pip-in-Anaconda.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">在Anaconda中使用pip安装包的问题.md</a></li>\\n<li>向量库环境 docker.md</li>\\n<li>启动API服务.md</li>\\n<li><a href=\\"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/%E5%9C%A8Anaconda%E4%B8%AD%E4%BD%BF%E7%94%A8pip%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%A0%E6%95%88%E9%97%AE%E9%A2%98.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">在Anaconda中使用pip安装包无效问题.md</a></li>\\n</ul>","autoDesc":true}');export{bs as comp,_s as data};
