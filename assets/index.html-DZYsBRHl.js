import{_ as i,a as s}from"./sunset-D_mTrkyk.js";import{_ as r,c as a,a as t,d as o,b as l,o as p,r as m}from"./app-CnEjATvv.js";const d="/assets/unclip_example-ylfwQZgN.png",u="/assets/unclip_example_multiple-BrjIAybS.png",g="/assets/unclip_2pass-BSnfbF23.png",f={},h={href:"https://github.com/comfyanonymous/ComfyUI",target:"_blank",rel:"noopener noreferrer"},c={href:"https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip/tree/main",target:"_blank",rel:"noopener noreferrer"},y={href:"https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/tree/main",target:"_blank",rel:"noopener noreferrer"},x={href:"https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/tree/main",target:"_blank",rel:"noopener noreferrer"},w={href:"https://github.com/comfyanonymous/ComfyUI",target:"_blank",rel:"noopener noreferrer"};function I(k,e){const n=m("ExternalLinkIcon");return p(),a("div",null,[e[12]||(e[12]=t("h1",{id:"unclip-model-examples",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#unclip-model-examples"},[t("span",null,"unCLIP Model Examples")])],-1)),e[13]||(e[13]=t("p",null,"unCLIP models are versions of SD models that are specially tuned to receive image concepts as input in addition to your text prompt. Images are encoded using the CLIPVision these models come with and then the concepts extracted by it are passed to the main model when sampling.",-1)),e[14]||(e[14]=t("p",null,"It basically lets you use images in your prompt.",-1)),t("p",null,[e[1]||(e[1]=o("Here is how you use it in ComfyUI (you can drag this into ")),t("a",h,[e[0]||(e[0]=o("ComfyUI")),l(n)]),e[2]||(e[2]=o(" to get the workflow):"))]),e[15]||(e[15]=t("p",null,[t("img",{src:d,alt:"Example",loading:"lazy"})],-1)),e[16]||(e[16]=t("p",null,"noise_augmentation controls how closely the model will try to follow the image concept. The lower the value the more it will follow the concept.",-1)),e[17]||(e[17]=t("p",null,"strength is how strongly it will influence the image.",-1)),e[18]||(e[18]=t("p",null,"Multiple images can be used like this:",-1)),e[19]||(e[19]=t("p",null,[t("img",{src:u,alt:"Example",loading:"lazy"})],-1)),e[20]||(e[20]=t("p",null,"You'll notice how it doesn't blend the images together in the traditional sense but actually picks some concepts from both and makes a coherent image.",-1)),e[21]||(e[21]=t("p",null,"Input images:",-1)),e[22]||(e[22]=t("p",null,[t("img",{src:i,width:"256"}),t("span",null,"        "),t("img",{src:s,width:"256"})],-1)),t("p",null,[e[4]||(e[4]=o("You can find the official unCLIP checkpoints ")),t("a",c,[e[3]||(e[3]=o("here")),l(n)])]),t("p",null,[e[7]||(e[7]=o("You can find some unCLIP checkpoints I made from some existing 768-v checkpoints with some clever merging ")),t("a",y,[e[5]||(e[5]=o("here (based on WD1.5 beta 2)")),l(n)]),e[8]||(e[8]=o(" and ")),t("a",x,[e[6]||(e[6]=o("here (based on illuminati Diffusion)")),l(n)])]),e[23]||(e[23]=t("h3",{id:"more-advanced-workflows",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#more-advanced-workflows"},[t("span",null,"More advanced Workflows")])],-1)),t("p",null,[e[10]||(e[10]=o("A good way of using unCLIP checkpoints is to use them for the first pass of a 2 pass workflow and then switch to a 1.x model for the second pass. This is how the following image was generated. (you can load it into ")),t("a",w,[e[9]||(e[9]=o("ComfyUI")),l(n)]),e[11]||(e[11]=o(" to get the workflow):"))]),e[24]||(e[24]=t("p",null,[t("img",{src:g,alt:"Example",loading:"lazy"})],-1))])}const v=r(f,[["render",I],["__file","index.html.vue"]]),L=JSON.parse('{"path":"/MdNote_Other/ComfyUI_examples/unclip/","title":"unCLIP Model Examples","lang":"zh-CN","frontmatter":{"description":"unCLIP Model Examples unCLIP models are versions of SD models that are specially tuned to receive image concepts as input in addition to your text prompt. Images are encoded usi...","head":[["meta",{"property":"og:url","content":"https://LincZero.github.io/MdNote_Other/ComfyUI_examples/unclip/"}],["meta",{"property":"og:site_name","content":"Linc 的小站"}],["meta",{"property":"og:title","content":"unCLIP Model Examples"}],["meta",{"property":"og:description","content":"unCLIP Model Examples unCLIP models are versions of SD models that are specially tuned to receive image concepts as input in addition to your text prompt. Images are encoded usi..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"LincZero"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"unCLIP Model Examples\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LincZero\\",\\"url\\":\\"https://github.com/LincZero/\\"}]}"]]},"headers":[{"level":1,"title":"unCLIP Model Examples","slug":"unclip-model-examples","link":"#unclip-model-examples","children":[{"level":3,"title":"More advanced Workflows","slug":"more-advanced-workflows","link":"#more-advanced-workflows","children":[]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.94,"words":281},"filePathRelative":"MdNote_Other/ComfyUI_examples/unclip/README.md","excerpt":"\\n<p>unCLIP models are versions of SD models that are specially tuned to receive image concepts as input in addition to your text prompt. Images are encoded using the CLIPVision these models come with and then the concepts extracted by it are passed to the main model when sampling.</p>\\n<p>It basically lets you use images in your prompt.</p>","autoDesc":true,"bioChainData":{"outlink":[],"backlink":[],"localMap":{"nodes":[{"id":"MdNote_Other/ComfyUI_examples/unclip/README.md","value":{"title":"ComfyUI_examples/","path":"MdNote_Other/ComfyUI_examples/unclip/README.md","outlink":[],"backlink":[]}}],"links":[]}}}');export{v as comp,L as data};
