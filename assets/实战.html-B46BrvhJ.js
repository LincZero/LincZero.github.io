import{_ as i,c as p,e as l,a,d as n,b as t,o as c,r as o}from"./app-BlNik9Xl.js";const r={},d={href:"https://blog.csdn.net/nlpstarter/article/details/131129240",target:"_blank",rel:"noopener noreferrer"},u={href:"https://zhuanlan.zhihu.com/p/106262896",target:"_blank",rel:"noopener noreferrer"},m={href:"https://www.cnblogs.com/jiangxinyang/p/17310398.html",target:"_blank",rel:"noopener noreferrer"},h={href:"https://github.com/lm-sys/FastChat",target:"_blank",rel:"noopener noreferrer"};function g(b,s){const e=o("ExternalLinkIcon");return c(),p("div",null,[s[8]||(s[8]=l('<h1 id="实战、个人环境备注" tabindex="-1"><a class="header-anchor" href="#实战、个人环境备注"><span>实战、个人环境备注</span></a></h1><h2 id="llm模型替换问题" tabindex="-1"><a class="header-anchor" href="#llm模型替换问题"><span>LLM模型替换问题</span></a></h2><h3 id="换用更大参数量的模型" tabindex="-1"><a class="header-anchor" href="#换用更大参数量的模型"><span>换用更大参数量的模型</span></a></h3><p>他这里默认是6b，可以给7G的显卡用。但公司电脑是24G显存的3090Ti，可以换用更强的本地模型。 这里大概算一下，b就是billion 十亿。 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><mi>G</mi><mo>=</mo><mn>7</mn><mo>∗</mo><msup><mn>10</mn><mrow><mn>3</mn><mo>∗</mo><mn>3</mn></mrow></msup><mo>=</mo><mn>7</mn><mi>B</mi></mrow><annotation encoding="application/x-tex">7G=7*10^{3*3}=7B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">7</span><span class="mord mathnormal">G</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">∗</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">7</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>，由于其他损耗就大概是7G显卡的样子。 但这也不一定，像 https://zhuanlan.zhihu.com/p/618690572 这里说这个130B缩放特性可以量化到100B级别，然后可以由 4x24G 来运行</p><p>参考：</p><ul><li>GPT-3 175B</li><li>OPT-175B</li><li>BLOOM-176B</li></ul><h3 id="同参数量的模型、量化or高参数量" tabindex="-1"><a class="header-anchor" href="#同参数量的模型、量化or高参数量"><span>同参数量的模型、量化or高参数量</span></a></h3><p>另外，参数量相同的模型也会有差异。如新的优化方案导致性能不同，或者训练集不同，调参差异，导致的侧重点不同。以及双语的支持等，都是要考虑的重点。</p>',8)),a("p",null,[s[1]||(s[1]=n("参考：")),a("a",d,[s[0]||(s[0]=n("中文羊驼大模型Alpaca-Plus-13B、Alpaca-33B效果大比拼")),t(e)])]),s[9]||(s[9]=l('<p>模型比较</p><ul><li>Alpaca-Plus-13B：120G预训练，400万指令精调。 <ul><li>特点：高训练数据，Q8量化</li></ul></li><li>Alpaca-33B：20G预训练，400万指令精调 <ul><li>特点：模型量级大，Q4量化</li></ul></li></ul><p>测试</p><ul><li>温室效应问题：33B的回答比较简练，内容长度上不占优</li><li>数学问题：骑7个猴。<strong>33B的完胜</strong>Plus-13B，可能模型量级对于这种数值计算和推理类的有较大优势吧</li><li>如何制作宫保鸡丁？：差不多</li><li>写一封信：Plus-13B占优一些，内容详实。可能33B吃了训练数据少的亏，写的内容不是特别生动</li><li>代码方面：<strong>33B显著胜出</strong></li><li>角色扮演：差不多，13回复长，略优</li></ul><p>总结</p><blockquote><p>Plus-13B相比之前的7B/13B已经有显著性能提升了，尤其是在生成类的任务上内容更加详实。33B的优缺点比较明显，优点是代码能力和数值计算方面确实比之前高出一截，但是在文本生成类的任务上效果略低于plus-13B。不过33B是基础版，这么比可能有点不讲武德，哈哈。这样其实就比较期待后续plus-33b的效果了，生成类任务的效果应该会有一个提升。</p></blockquote><h3 id="羊驼系模型" tabindex="-1"><a class="header-anchor" href="#羊驼系模型"><span>羊驼系模型</span></a></h3><p>参考：</p>',8)),a("ul",null,[a("li",null,[a("a",u,[s[2]||(s[2]=n("Guanaco, Llama, Vicuña, Alpaca该怎么区别")),t(e)])]),a("li",null,[a("a",m,[s[3]||(s[3]=n("大模型入门（一）—— LLaMa/Alpaca/Vicuna")),t(e)])]),s[4]||(s[4]=a("li",null,"https://blog.csdn.net/v_JULY_v/article/details/129709105",-1))]),a("p",null,[s[6]||(s[6]=n("Vicuna是在LLaMa-13B的基础上使用监督数据微调得到的模型，数据集来自于ShareGPT.com 产生的用户对话数据，共70K条。使用Pytorch FSDP在8张A100上训练了一天。相较于Alpaca，Vicuna在训练中将序列长度由512扩展到了2048，并且通过梯度检测和flash attention来解决内存问题；调整训练损失考虑多轮对话，并仅根据模型的输出进行微调。通过GPT4来打分评测，Vicuna可以达到ChatGPT 90%的效果。并且还提供了可调用的分布式聊天服务")),a("a",h,[s[5]||(s[5]=n("FastChat")),t(e)]),s[7]||(s[7]=n("。"))]),s[10]||(s[10]=l(`<p>一些模型图标的区分：https://kaiyuan.blog.csdn.net/article/details/131467606</p><h2 id="bug1-无法找到显卡驱动" tabindex="-1"><a class="header-anchor" href="#bug1-无法找到显卡驱动"><span>bug1 无法找到显卡驱动</span></a></h2><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token punctuation">(</span>base<span class="token punctuation">)</span> root@msy-System-Product-Name:~<span class="token comment"># docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0</span>
4bcc140c4c5b24e21da7ba6bf863d372f899ddd20e141469959b08ac45b5cf5a
docker: Error response from daemon: could not <span class="token keyword">select</span> device driver <span class="token string">&quot;&quot;</span> with capabilities: <span class="token punctuation">[</span><span class="token punctuation">[</span>gpu<span class="token punctuation">]</span><span class="token punctuation">]</span>.
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>解决该bug</p><hr><p>这个错误通常发生在 Docker 无法在你的机器上找到正确的驱动程序来使用 GPU 的情况下。以下是一些可以解决这个问题的步骤：</p><ol><li><p><strong>检查Docker版本是否支持 --gpus 选项</strong></p><p>Docker从19.03版本开始支持原生使用GPU。用命令 <code>docker --version</code> 检查Docker版本，如果你使用的是更旧的版本，考虑更新Docker至最新版本。</p></li><li><p><strong>检查是否安装了 NVIDIA 容器工具箱</strong></p><p>如果你在docker中使用GPU，你需要安装nvidia-docker2，并在你的docker配置中添加NVIDIA运行时。如果没有安装，按照以下步骤进行安装</p></li></ol><p>a. <strong>移除旧版本</strong></p><p>旧版本的 Docker 由名为 <code>nvidia-docker2</code> 的包支持。新的 Docker 版本由 NVIDIA 容器工具箱支持。如果你已经安装了旧的 <code>nvidia-docker2</code> 包，请移除它：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">sudo</span> <span class="token function">apt-get</span> purge <span class="token parameter variable">-y</span> nvidia-docker
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>b. <strong>安装 NVIDIA 容器工具箱</strong> - 添加包存储库：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">curl</span> <span class="token parameter variable">-s</span> <span class="token parameter variable">-L</span> https://nvidia.github.io/nvidia-docker/gpgkey <span class="token operator">|</span> <span class="token function">sudo</span> apt-key <span class="token function">add</span> -
<span class="token assign-left variable">distribution</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token builtin class-name">.</span> /etc/os-release<span class="token punctuation">;</span><span class="token builtin class-name">echo</span> $ID$VERSION_ID<span class="token variable">)</span></span>
<span class="token function">curl</span> <span class="token parameter variable">-s</span> <span class="token parameter variable">-L</span> https://nvidia.github.io/nvidia-docker/<span class="token variable">$distribution</span>/nvidia-docker.list <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">tee</span> /etc/apt/sources.list.d/nvidia-docker.list
<span class="token function">sudo</span> <span class="token function">apt-get</span> update
<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> nvidia-docker2
<span class="token function">sudo</span> systemctl restart <span class="token function">docker</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>c. <strong>测试 NVIDIA Docker 安装</strong> - 通过运行一个示例来测试 NVIDIA Docker 的安装是否成功：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">--rm</span> <span class="token parameter variable">--gpus</span> all nvidia/cuda:11.0-base nvidia-smi
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ol><li>如果仍然存在相同的错误，那么你应该检查Docker是否可以访问GPU。你可以通过运行命令 <code>nvidia-smi</code> 来检查。如果你可以看到你的GPU列表，Docker也应该能够使用它们。</li><li>确保您的机器上正确安装了 NVIDIA 驱动程序。您可以使用命令 <code>nvidia-smi</code> 来验证。</li></ol><p>如果问题仍然存在，请提供更多信息，以便我能更深入地帮助你。</p><h2 id="bug2-回答为空" tabindex="-1"><a class="header-anchor" href="#bug2-回答为空"><span>bug2 回答为空</span></a></h2><p>群里另一个用 docker 方式的，也遇到了相同的问题。决定放弃docker方案，转手动部署。但我不想重新下这些模型包了，就在原docker的基础上重新部署吧</p><p>原</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-p</span> <span class="token number">80</span>:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>现</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># 可选，删掉旧的docker</span>
<span class="token function">docker</span> stop xxx
dcker <span class="token function">rm</span> xxx

<span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">--name</span><span class="token operator">=</span>chatchat2 <span class="token parameter variable">--restart</span><span class="token operator">=</span>always <span class="token parameter variable">-p</span> <span class="token number">80</span>:8501 f224e85162da

<span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> chatchat2 /bin/bash
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="bug3-git问题" tabindex="-1"><a class="header-anchor" href="#bug3-git问题"><span>bug3 git问题</span></a></h2><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token punctuation">(</span>base<span class="token punctuation">)</span> root@msy-System-Product-Name:~/chatchat<span class="token comment"># git clone https://huggingface.co/moka-ai/m3e-base</span>
正克隆到 <span class="token string">&#39;m3e-base&#39;</span><span class="token punctuation">..</span>.
remote: Enumerating objects: <span class="token number">108</span>, done.
remote: Counting objects: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">16</span>/16<span class="token punctuation">)</span>, done.
remote: Compressing objects: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">15</span>/15<span class="token punctuation">)</span>, done.
remote: Total <span class="token number">108</span> <span class="token punctuation">(</span>delta <span class="token number">6</span><span class="token punctuation">)</span>, reused <span class="token number">0</span> <span class="token punctuation">(</span>delta <span class="token number">0</span><span class="token punctuation">)</span>, pack-reused <span class="token number">92</span>
接收对象中: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">108</span>/108<span class="token punctuation">)</span>, <span class="token number">194.12</span> KiB <span class="token operator">|</span> <span class="token number">2.55</span> MiB/s, 完成.
处理 delta 中: <span class="token number">100</span>% <span class="token punctuation">(</span><span class="token number">57</span>/57<span class="token punctuation">)</span>, 完成.
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>git clone卡在这一步</p><p>尝试用安装 Git LFS 解决该问题。</p><p>在 Ubuntu 上安装 Git LFS 可以使用以下命令：</p><p>旧：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># 安装必要的软件包</span>
$ <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">git</span>

<span class="token comment"># 下载并安装git-lfs</span>
$ <span class="token function">curl</span> <span class="token parameter variable">-s</span> https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">bash</span>
$ <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> git-lfs

<span class="token comment"># 初始化git-lfs</span>
$ <span class="token function">git</span> lfs <span class="token function">install</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>新：</p><p>见官网链接</p><p>但看起来没成功</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token punctuation">(</span>base<span class="token punctuation">)</span> root@msy-System-Product-Name:~/chatchat<span class="token comment"># git lfs install</span>
Error: Failed to call <span class="token function">git</span> rev-parse --git-dir: <span class="token builtin class-name">exit</span> status <span class="token number">128</span>
Git LFS initialized.
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>后来直接复制黏贴了，不git了。其实就是10G的git时间太慢了而已。耐心等也可以，我选择先用有线下载再用wifi传过去</p><h2 id="bug4-找不到存在的路径" tabindex="-1"><a class="header-anchor" href="#bug4-找不到存在的路径"><span>bug4 找不到存在的路径</span></a></h2><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>FileNotFoundError: <span class="token punctuation">[</span>Errno <span class="token number">2</span><span class="token punctuation">]</span> No usable temporary directory found <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">&#39;/tmp&#39;</span>, <span class="token string">&#39;/var/tmp&#39;</span>, <span class="token string">&#39;/usr/tmp&#39;</span>, <span class="token string">&#39;/root/chatchat/Langchain-Chatchat&#39;</span><span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>但路径都是真实存在的</p><p>后来发现应该是我硬盘空间满了的原因</p><h2 id="bug5-一直卡在-wait-controller-running" tabindex="-1"><a class="header-anchor" href="#bug5-一直卡在-wait-controller-running"><span>bug5 一直卡在 wait controller running</span></a></h2><p>分步运行中，用 <code> python server/llm_api_stale.py</code> 则一直卡在wait controller running</p><p>换用回一键启动的命令，群友说那个分步启用的可能官方要不维护了</p><h2 id="bug6-爆显存" tabindex="-1"><a class="header-anchor" href="#bug6-爆显存"><span>bug6 爆显存</span></a></h2><p>重启电脑解决</p>`,43))])}const v=i(r,[["render",g],["__file","实战.html.vue"]]),f=JSON.parse('{"path":"/MdNote_Public/01.%20DesignAndDevelop/Develop/04.%20Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/%E5%AE%9E%E6%88%98.html","title":"实战、个人环境备注","lang":"zh-CN","frontmatter":{"description":"实战、个人环境备注 LLM模型替换问题 换用更大参数量的模型 他这里默认是6b，可以给7G的显卡用。但公司电脑是24G显存的3090Ti，可以换用更强的本地模型。 这里大概算一下，b就是billion 十亿。 7G=7∗103∗3=7B，由于其他损耗就大概是7G显卡的样子。 但这也不一定，像 https://zhuanlan.zhihu.com/p/6...","head":[["meta",{"property":"og:url","content":"https://LincZero.github.io/MdNote_Public/01.%20DesignAndDevelop/Develop/04.%20Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/%E5%AE%9E%E6%88%98.html"}],["meta",{"property":"og:site_name","content":"Linc 的小站"}],["meta",{"property":"og:title","content":"实战、个人环境备注"}],["meta",{"property":"og:description","content":"实战、个人环境备注 LLM模型替换问题 换用更大参数量的模型 他这里默认是6b，可以给7G的显卡用。但公司电脑是24G显存的3090Ti，可以换用更强的本地模型。 这里大概算一下，b就是billion 十亿。 7G=7∗103∗3=7B，由于其他损耗就大概是7G显卡的样子。 但这也不一定，像 https://zhuanlan.zhihu.com/p/6..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"LincZero"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"实战、个人环境备注\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LincZero\\",\\"url\\":\\"https://github.com/LincZero/\\"}]}"]]},"headers":[{"level":1,"title":"实战、个人环境备注","slug":"实战、个人环境备注","link":"#实战、个人环境备注","children":[{"level":2,"title":"LLM模型替换问题","slug":"llm模型替换问题","link":"#llm模型替换问题","children":[{"level":3,"title":"换用更大参数量的模型","slug":"换用更大参数量的模型","link":"#换用更大参数量的模型","children":[]},{"level":3,"title":"同参数量的模型、量化or高参数量","slug":"同参数量的模型、量化or高参数量","link":"#同参数量的模型、量化or高参数量","children":[]},{"level":3,"title":"羊驼系模型","slug":"羊驼系模型","link":"#羊驼系模型","children":[]}]},{"level":2,"title":"bug1 无法找到显卡驱动","slug":"bug1-无法找到显卡驱动","link":"#bug1-无法找到显卡驱动","children":[]},{"level":2,"title":"bug2 回答为空","slug":"bug2-回答为空","link":"#bug2-回答为空","children":[]},{"level":2,"title":"bug3 git问题","slug":"bug3-git问题","link":"#bug3-git问题","children":[]},{"level":2,"title":"bug4 找不到存在的路径","slug":"bug4-找不到存在的路径","link":"#bug4-找不到存在的路径","children":[]},{"level":2,"title":"bug5 一直卡在 wait controller running","slug":"bug5-一直卡在-wait-controller-running","link":"#bug5-一直卡在-wait-controller-running","children":[]},{"level":2,"title":"bug6 爆显存","slug":"bug6-爆显存","link":"#bug6-爆显存","children":[]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":5.7,"words":1709},"filePathRelative":"MdNote_Public/01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/实战.md","excerpt":"\\n<h2>LLM模型替换问题</h2>\\n<h3>换用更大参数量的模型</h3>\\n<p>他这里默认是6b，可以给7G的显卡用。但公司电脑是24G显存的3090Ti，可以换用更强的本地模型。\\n这里大概算一下，b就是billion 十亿。 <span v-pre=\\"\\" class=\\"katex\\"><span class=\\"katex-mathml\\"><math xmlns=\\"http://www.w3.org/1998/Math/MathML\\"><semantics><mrow><mn>7</mn><mi>G</mi><mo>=</mo><mn>7</mn><mo>∗</mo><msup><mn>10</mn><mrow><mn>3</mn><mo>∗</mo><mn>3</mn></mrow></msup><mo>=</mo><mn>7</mn><mi>B</mi></mrow><annotation encoding=\\"application/x-tex\\">7G=7*10^{3*3}=7B</annotation></semantics></math></span><span class=\\"katex-html\\" aria-hidden=\\"true\\"><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.6833em;\\"></span><span class=\\"mord\\">7</span><span class=\\"mord mathnormal\\">G</span><span class=\\"mspace\\" style=\\"margin-right:0.2778em;\\"></span><span class=\\"mrel\\">=</span><span class=\\"mspace\\" style=\\"margin-right:0.2778em;\\"></span></span><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.6444em;\\"></span><span class=\\"mord\\">7</span><span class=\\"mspace\\" style=\\"margin-right:0.2222em;\\"></span><span class=\\"mbin\\">∗</span><span class=\\"mspace\\" style=\\"margin-right:0.2222em;\\"></span></span><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.8141em;\\"></span><span class=\\"mord\\">1</span><span class=\\"mord\\"><span class=\\"mord\\">0</span><span class=\\"msupsub\\"><span class=\\"vlist-t\\"><span class=\\"vlist-r\\"><span class=\\"vlist\\" style=\\"height:0.8141em;\\"><span style=\\"top:-3.063em;margin-right:0.05em;\\"><span class=\\"pstrut\\" style=\\"height:2.7em;\\"></span><span class=\\"sizing reset-size6 size3 mtight\\"><span class=\\"mord mtight\\"><span class=\\"mord mtight\\">3</span><span class=\\"mbin mtight\\">∗</span><span class=\\"mord mtight\\">3</span></span></span></span></span></span></span></span></span><span class=\\"mspace\\" style=\\"margin-right:0.2778em;\\"></span><span class=\\"mrel\\">=</span><span class=\\"mspace\\" style=\\"margin-right:0.2778em;\\"></span></span><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.6833em;\\"></span><span class=\\"mord\\">7</span><span class=\\"mord mathnormal\\" style=\\"margin-right:0.05017em;\\">B</span></span></span></span>，由于其他损耗就大概是7G显卡的样子。\\n但这也不一定，像 https://zhuanlan.zhihu.com/p/618690572 这里说这个130B缩放特性可以量化到100B级别，然后可以由 4x24G 来运行</p>","autoDesc":true,"bioChainData":{"outlink":[],"backlink":[],"localMap":{"nodes":[{"id":"MdNote_Public/01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/实战.md","value":{"title":"实战","path":"MdNote_Public/01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/实战.md","outlink":[],"backlink":[]}}],"links":[]}}}');export{v as comp,f as data};
