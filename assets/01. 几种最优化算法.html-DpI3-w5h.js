import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r,o as s,c as o,a as e,d as l,b as n,e as a}from"./app-DtBZtuIa.js";const d={},h=a('<h1 id="吴恩达机器学习" tabindex="-1"><a class="header-anchor" href="#吴恩达机器学习"><span>吴恩达机器学习</span></a></h1><h1 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h1><h1 id="几种最优化算法" tabindex="-1"><a class="header-anchor" href="#几种最优化算法"><span>几种最优化算法</span></a></h1><p>另参考：</p>',4),c={href:"https://blog.csdn.net/qq_40626659/article/details/103074035",target:"_blank",rel:"noopener noreferrer"},p={href:"https://blog.csdn.net/weixin_30947043/article/details/99309191",target:"_blank",rel:"noopener noreferrer"},E={href:"https://www.cnblogs.com/maybe2030/p/4751804.html",target:"_blank",rel:"noopener noreferrer"},u=a('<h2 id="几种最优化算法-1" tabindex="-1"><a class="header-anchor" href="#几种最优化算法-1"><span>几种最优化算法</span></a></h2><p>常见的几种最优化算法：梯度下降法、牛顿法、拟牛顿法、共轭梯度法等</p><h3 id="梯度下降法-gd-gradient-descent" tabindex="-1"><a class="header-anchor" href="#梯度下降法-gd-gradient-descent"><span>梯度下降法（GD，Gradient Descent）</span></a></h3><h4 id="梯度下降法-gd" tabindex="-1"><a class="header-anchor" href="#梯度下降法-gd"><span>梯度下降法（GD）</span></a></h4><h4 id="批量梯度下降-bgd" tabindex="-1"><a class="header-anchor" href="#批量梯度下降-bgd"><span>批量梯度下降（BGD）</span></a></h4><p>在梯度下降的每一步中，我们都用到了所有的训练样本，需要进行求和运算</p><h4 id="随机梯度下降-sgd" tabindex="-1"><a class="header-anchor" href="#随机梯度下降-sgd"><span>随机梯度下降（SGD）</span></a></h4><p>SGD和BGD相反，SGD每次更新参数仅用一个样本进行，而BGD是用所有样本。</p><h4 id="小批量梯度下降-mbgd" tabindex="-1"><a class="header-anchor" href="#小批量梯度下降-mbgd"><span>小批量梯度下降（MBGD）</span></a></h4><h4 id="【比较】各种版本的梯度下降" tabindex="-1"><a class="header-anchor" href="#【比较】各种版本的梯度下降"><span>【比较】各种版本的梯度下降</span></a></h4><h3 id="牛顿法-newton-s-method" tabindex="-1"><a class="header-anchor" href="#牛顿法-newton-s-method"><span>牛顿法（Newton&#39;s Method）</span></a></h3><h4 id="拟牛顿法-quasi-newton-methods" tabindex="-1"><a class="header-anchor" href="#拟牛顿法-quasi-newton-methods"><span>拟牛顿法（Quasi-Newton Methods）</span></a></h4><h2 id="牛顿法和梯度下降法" tabindex="-1"><a class="header-anchor" href="#牛顿法和梯度下降法"><span>牛顿法和梯度下降法</span></a></h2><p>先来比较 牛顿法和梯度下降法</p><ul><li><p>梯度下降（Gradient Descent）</p><ul><li><p>优点：最简单 最常用 最直观、实现简单，也被称为是 “最速下降法”（当然不意味着速度最快）</p></li><li><p>原理</p><blockquote><p>是用于求函数最小值的算法，其步骤为：</p><ul><li>随机选择一个参数组合，计算损失函数</li><li>通过方向和补仓，对参数进行更新，找下一个能够让损失函数值更低的参数组合</li><li>持续迭代直至寻找到一个局部最小值。</li></ul><p>因为没有尝试所有的参数组合，所以不能保证寻找到的局部最小值就是全局最小值</p></blockquote></li><li><p>分类</p><ul><li>批量梯度下降（BGD）：在梯度下降的每一步中，我们都用到了所有的训练样本，需要进行求和运算</li><li>随机梯度下降（SGD）：SGD和BGD相反，SGD每次更新参数仅用一个样本进行，而BGD是用所有样本。</li><li>小批量梯度下降（MBGD）：是BGD和SGD的中和，每次参数迭代用大于一小于所有的样本。其优点为：比SGD精度高，但可能需要的时间比较长。</li></ul></li></ul></li><li><p>牛顿法和拟牛顿法（Newton&#39;s method &amp; Quasi-Newton Methods）</p><ul><li><p>原理</p><blockquote><p>其实质是对 损失函数 进行求导，寻找能使得损失函数导数为0的解，当损失函数导数为0时，也便找到了最优解（极值点）</p><p>其迭代过程是在当前位置x0求该函数的切线，该切线和x轴的交点x1，作为新的x0。</p><p>重复这个过程，直到交点和函数的零点重合。此时的参数值就是使得目标函数取得极值的参数值</p></blockquote></li></ul></li><li><p>比较</p><ul><li><p>收敛速度</p><p>收敛速度上，牛顿法要比梯度下降法更快，因为其参数更新的步长会比较大， 在接近最优解时，梯度下降法也容易因步长比较大而产生来回震荡的效果，从而降低了收敛速度。</p></li><li><p>计算量</p><p>但是在运行过程中，牛顿法的计算量要远大于梯度下降，因为牛顿法要对多个值进行求导运算，而梯度下降仅需要得出方向和步长便能更新参数</p></li></ul></li></ul>',15);function g(m,D){const t=r("ExternalLinkIcon");return s(),o("div",null,[h,e("ul",null,[e("li",null,[e("a",c,[l("【CSDN】梯度下降法和牛顿法"),n(t)])]),e("li",null,[e("a",p,[l("【CSDN】常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等）"),n(t)]),e("ul",null,[e("li",null,[e("a",E,[l("【博客园】【Math】常见的几种最优化方法"),n(t)])])])])]),u])}const B=i(d,[["render",g],["__file","01. 几种最优化算法.html.vue"]]),b=JSON.parse(`{"path":"/MdNote_Public/01.%20%E8%AE%BE%E8%AE%A1%E5%BC%80%E5%8F%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%94%9F%E4%BA%A7/Develop/04.%20Project/Type/Artificial_Intelligence/01.%20%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/10.%20%E8%A1%A5%E5%85%85%E9%A1%B9/01.%20%E5%87%A0%E7%A7%8D%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.html","title":"吴恩达机器学习","lang":"zh-CN","frontmatter":{"description":"吴恩达机器学习 目录 几种最优化算法 另参考： 【CSDN】梯度下降法和牛顿法 【CSDN】常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等） 【博客园】【Math】常见的几种最优化方法 几种最优化算法 常见的几种最优化算法：梯度下降法、牛顿法、拟牛顿法、共轭梯度法等 梯度下降法（GD，Gradient Descent） 梯度下降法（G...","head":[["meta",{"property":"og:url","content":"http://192.168.0.101:8080/MdNote_Public/01.%20%E8%AE%BE%E8%AE%A1%E5%BC%80%E5%8F%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%94%9F%E4%BA%A7/Develop/04.%20Project/Type/Artificial_Intelligence/01.%20%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/10.%20%E8%A1%A5%E5%85%85%E9%A1%B9/01.%20%E5%87%A0%E7%A7%8D%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.html"}],["meta",{"property":"og:site_name","content":"Linc 的小站"}],["meta",{"property":"og:title","content":"吴恩达机器学习"}],["meta",{"property":"og:description","content":"吴恩达机器学习 目录 几种最优化算法 另参考： 【CSDN】梯度下降法和牛顿法 【CSDN】常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等） 【博客园】【Math】常见的几种最优化方法 几种最优化算法 常见的几种最优化算法：梯度下降法、牛顿法、拟牛顿法、共轭梯度法等 梯度下降法（GD，Gradient Descent） 梯度下降法（G..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"LincZero"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"吴恩达机器学习\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LincZero\\",\\"url\\":\\"https://github.com/LincZero/\\"}]}"]]},"headers":[{"level":1,"title":"吴恩达机器学习","slug":"吴恩达机器学习","link":"#吴恩达机器学习","children":[]},{"level":1,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":1,"title":"几种最优化算法","slug":"几种最优化算法","link":"#几种最优化算法","children":[{"level":2,"title":"几种最优化算法","slug":"几种最优化算法-1","link":"#几种最优化算法-1","children":[{"level":3,"title":"梯度下降法（GD，Gradient Descent）","slug":"梯度下降法-gd-gradient-descent","link":"#梯度下降法-gd-gradient-descent","children":[{"level":4,"title":"梯度下降法（GD）","slug":"梯度下降法-gd","link":"#梯度下降法-gd","children":[]},{"level":4,"title":"批量梯度下降（BGD）","slug":"批量梯度下降-bgd","link":"#批量梯度下降-bgd","children":[]},{"level":4,"title":"随机梯度下降（SGD）","slug":"随机梯度下降-sgd","link":"#随机梯度下降-sgd","children":[]},{"level":4,"title":"小批量梯度下降（MBGD）","slug":"小批量梯度下降-mbgd","link":"#小批量梯度下降-mbgd","children":[]},{"level":4,"title":"【比较】各种版本的梯度下降","slug":"【比较】各种版本的梯度下降","link":"#【比较】各种版本的梯度下降","children":[]}]},{"level":3,"title":"牛顿法（Newton's Method）","slug":"牛顿法-newton-s-method","link":"#牛顿法-newton-s-method","children":[{"level":4,"title":"拟牛顿法（Quasi-Newton Methods）","slug":"拟牛顿法-quasi-newton-methods","link":"#拟牛顿法-quasi-newton-methods","children":[]}]}]},{"level":2,"title":"牛顿法和梯度下降法","slug":"牛顿法和梯度下降法","link":"#牛顿法和梯度下降法","children":[]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.73,"words":819},"filePathRelative":"MdNote_Public/01. 设计开发与数据生产/Develop/04. Project/Type/Artificial_Intelligence/01. 吴恩达 机器学习/10. 补充项/01. 几种最优化算法.md","autoDesc":true}`);export{B as comp,b as data};
