import{_ as n,c as r,a as t,b as o,d as a,e as s,o as l,r as m}from"./app-DTo3dt_i.js";const d="/assets/image_to_video-BhSvsZB0.webp",h="/assets/txt_to_image_to_video-CoYt3ZIY.webp",p={},f={href:"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/blob/main/svd.safetensors",target:"_blank",rel:"noopener noreferrer"},g={href:"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/blob/main/svd_xt.safetensors",target:"_blank",rel:"noopener noreferrer"},c={href:"https://github.com/comfyanonymous/ComfyUI",target:"_blank",rel:"noopener noreferrer"};function u(v,e){const i=m("ExternalLinkIcon");return l(),r("div",null,[e[8]||(e[8]=t("h1",{id:"video-examples",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#video-examples"},[t("span",null,"Video Examples")])],-1)),e[9]||(e[9]=t("h2",{id:"image-to-video",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#image-to-video"},[t("span",null,"Image to Video")])],-1)),t("p",null,[e[2]||(e[2]=o("As of writing this there are two image to video checkpoints. Here are the official checkpoints for ")),t("a",f,[e[0]||(e[0]=o("the one tuned to generate 14 frame videos")),a(i)]),e[3]||(e[3]=o(" and ")),t("a",g,[e[1]||(e[1]=o("the one for 25 frame videos")),a(i)]),e[4]||(e[4]=o(". Put them in the ComfyUI/models/checkpoints folder."))]),t("p",null,[e[6]||(e[6]=o("The most basic way of using the image to video model is by giving it an init image like in the following workflow that uses the 14 frame model. You can download this webp animated image and load it or drag it on ")),t("a",c,[e[5]||(e[5]=o("ComfyUI")),a(i)]),e[7]||(e[7]=o(" to get the workflow."))]),e[10]||(e[10]=s('<p><img src="'+d+'" alt="Example" loading="lazy"><a href="workflow_image_to_video.json">Workflow in Json format</a></p><p>If you want the exact input image you can find it on the <a href="../unclip">unCLIP example page</a></p><p>You can also use them like in this workflow that uses SDXL to generate an initial image that is then passed to the 25 frame model:</p><p><img src="'+h+'" alt="Example" loading="lazy"><a href="workflow_txt_to_img_to_video.json">Workflow in Json format</a></p><h4 id="some-explanations-for-the-parameters" tabindex="-1"><a class="header-anchor" href="#some-explanations-for-the-parameters"><span>Some explanations for the parameters:</span></a></h4><p>video_frames: The number of video frames to generate.</p><p>motion_bucket_id: The higher the number the more motion will be in the video.</p><p>fps: The higher the fps the less choppy the video will be.</p><p>augmentation level: The amount of noise added to the init image, the higher it is the less the video will look like the init image. Increase it for more motion.</p><p>VideoLinearCFGGuidance: This node improves sampling for these video models a bit, what it does is linearly scale the cfg across the different frames. In the above example the first frame will be cfg 1.0 (the min_cfg in the node) the middle frame 1.75 and the last frame 2.5. (the cfg set in the sampler). This way frames further away from the init frame get a gradually higher cfg.</p>',10))])}const b=n(p,[["render",u],["__file","index.html.vue"]]),w=JSON.parse('{"path":"/MdNote_Other/ComfyUI_examples/video/","title":"Video Examples","lang":"zh-CN","frontmatter":{"description":"Video Examples Image to Video As of writing this there are two image to video checkpoints. Here are the official checkpoints for the one tuned to generate 14 frame videos and th...","head":[["meta",{"property":"og:url","content":"http://192.168.0.101:8080/MdNote_Other/ComfyUI_examples/video/"}],["meta",{"property":"og:site_name","content":"Linc 的小站"}],["meta",{"property":"og:title","content":"Video Examples"}],["meta",{"property":"og:description","content":"Video Examples Image to Video As of writing this there are two image to video checkpoints. Here are the official checkpoints for the one tuned to generate 14 frame videos and th..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"LincZero"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Video Examples\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LincZero\\",\\"url\\":\\"https://github.com/LincZero/\\"}]}"]]},"headers":[{"level":1,"title":"Video Examples","slug":"video-examples","link":"#video-examples","children":[{"level":2,"title":"Image to Video","slug":"image-to-video","link":"#image-to-video","children":[{"level":4,"title":"Some explanations for the parameters:","slug":"some-explanations-for-the-parameters","link":"#some-explanations-for-the-parameters","children":[]}]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1,"words":299},"filePathRelative":"MdNote_Other/ComfyUI_examples/video/README.md","excerpt":"\\n<h2>Image to Video</h2>\\n<p>As of writing this there are two image to video checkpoints. Here are the official checkpoints for <a href=\\"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/blob/main/svd.safetensors\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">the one tuned to generate 14 frame videos</a> and <a href=\\"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/blob/main/svd_xt.safetensors\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">the one for 25 frame videos</a>. Put them in the ComfyUI/models/checkpoints folder.</p>","autoDesc":true}');export{b as comp,w as data};
