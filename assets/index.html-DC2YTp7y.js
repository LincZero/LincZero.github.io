import{_ as r,c as i,a as n,d as t,b as l,e as s,o,r as p}from"./app-xgHpiPmZ.js";const d="/assets/image-20230906094634937-CRx8xThB.png",g="/assets/image-20230906094624349-BOnO2qlG.png",u="/assets/image-20230906095433786-DFWfrCjo.png",m="/assets/webui_0813_1-CiF0QWGe.png",h="/assets/image-20230906100313064--osWr8cU.png",b="/assets/image-20230906095433786-DFWfrCjo.png",f="/assets/image-20230906101805804-DOVxlawV.png",c={},v={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/INSTALL.md",target:"_blank",rel:"noopener noreferrer"},k={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/Issue-with-Installing-Packages-Using-pip-in-Anaconda.md",target:"_blank",rel:"noopener noreferrer"},E={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/%E5%9C%A8Anaconda%E4%B8%AD%E4%BD%BF%E7%94%A8pip%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%A0%E6%95%88%E9%97%AE%E9%A2%98.md",target:"_blank",rel:"noopener noreferrer"},A={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E4%BB%8B%E7%BB%8D",target:"_blank",rel:"noopener noreferrer"},L={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E5%8F%98%E6%9B%B4%E6%97%A5%E5%BF%97",target:"_blank",rel:"noopener noreferrer"},y={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E6%A8%A1%E5%9E%8B%E6%94%AF%E6%8C%81",target:"_blank",rel:"noopener noreferrer"},x={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#Docker-%E9%83%A8%E7%BD%B2",target:"_blank",rel:"noopener noreferrer"},C={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E8%BD%AF%E4%BB%B6%E9%9C%80%E6%B1%82",target:"_blank",rel:"noopener noreferrer"},I={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#1.-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87",target:"_blank",rel:"noopener noreferrer"},B={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#2.-%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E8%87%B3%E6%9C%AC%E5%9C%B0",target:"_blank",rel:"noopener noreferrer"},D={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#3.-%E8%AE%BE%E7%BD%AE%E9%85%8D%E7%BD%AE%E9%A1%B9",target:"_blank",rel:"noopener noreferrer"},M={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#4.-%E7%9F%A5%E8%AF%86%E5%BA%93%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%BF%81%E7%A7%BB",target:"_blank",rel:"noopener noreferrer"},P={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#6.-%E4%B8%80%E9%94%AE%E5%90%AF%E5%8A%A8",target:"_blank",rel:"noopener noreferrer"},F={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.-%E5%90%AF%E5%8A%A8-API-%E6%9C%8D%E5%8A%A1%E6%88%96-Web-UI",target:"_blank",rel:"noopener noreferrer"},w={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98",target:"_blank",rel:"noopener noreferrer"},T={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E8%B7%AF%E7%BA%BF%E5%9B%BE",target:"_blank",rel:"noopener noreferrer"},q={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#%E9%A1%B9%E7%9B%AE%E4%BA%A4%E6%B5%81%E7%BE%A4",target:"_blank",rel:"noopener noreferrer"},z={href:"https://github.com/hwchase17/langchain",target:"_blank",rel:"noopener noreferrer"},U={href:"https://github.com/GanymedeNil",target:"_blank",rel:"noopener noreferrer"},W={href:"https://github.com/GanymedeNil/document.ai",target:"_blank",rel:"noopener noreferrer"},R={href:"https://github.com/AlexZhangji",target:"_blank",rel:"noopener noreferrer"},N={href:"https://github.com/THUDM/ChatGLM-6B/pull/216",target:"_blank",rel:"noopener noreferrer"},S={href:"https://github.com/lm-sys/FastChat",target:"_blank",rel:"noopener noreferrer"},$={href:"https://github.com/langchain-ai/langchain",target:"_blank",rel:"noopener noreferrer"},H={href:"https://github.com/tiangolo/fastapi",target:"_blank",rel:"noopener noreferrer"},V={href:"https://github.com/streamlit/streamlit",target:"_blank",rel:"noopener noreferrer"},j={href:"https://www.bilibili.com/video/BV13M4y1e7cN/?share_source=copy_web&vd_source=e6c5aafe684f30fbe41925d61ca6d514",target:"_blank",rel:"noopener noreferrer"},G={href:"https://www.codewithgpu.com/i/imClumsyPanda/langchain-ChatGLM/Langchain-Chatchat",target:"_blank",rel:"noopener noreferrer"},O={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0",target:"_blank",rel:"noopener noreferrer"},Q={href:"https://huggingface.co/THUDM/chatglm2-6b",target:"_blank",rel:"noopener noreferrer"},K={href:"https://huggingface.co/moka-ai/m3e-base",target:"_blank",rel:"noopener noreferrer"},Z={href:"https://github.com/lm-sys/FastChat",target:"_blank",rel:"noopener noreferrer"},J={href:"https://huggingface.co/meta-llama/Llama-2-7b-chat-hf",target:"_blank",rel:"noopener noreferrer"},Y={href:"https://huggingface.co/BlinkDL/rwkv-4-raven",target:"_blank",rel:"noopener noreferrer"},X={href:"https://huggingface.co/camel-ai/CAMEL-13B-Combined-Data",target:"_blank",rel:"noopener noreferrer"},_={href:"https://huggingface.co/databricks/dolly-v2-12b",target:"_blank",rel:"noopener noreferrer"},ee={href:"https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b",target:"_blank",rel:"noopener noreferrer"},ne={href:"https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b",target:"_blank",rel:"noopener noreferrer"},te={href:"https://huggingface.co/lcw99/polyglot-ko-12.8b-chang-instruct-chat",target:"_blank",rel:"noopener noreferrer"},ae={href:"https://huggingface.co/lmsys/fastchat-t5",target:"_blank",rel:"noopener noreferrer"},le={href:"https://huggingface.co/mosaicml/mpt-7b-chat",target:"_blank",rel:"noopener noreferrer"},se={href:"https://huggingface.co/Neutralzz/BiLLa-7B-SFT",target:"_blank",rel:"noopener noreferrer"},re={href:"https://huggingface.co/nomic-ai/gpt4all-13b-snoozy",target:"_blank",rel:"noopener noreferrer"},ie={href:"https://huggingface.co/NousResearch/Nous-Hermes-13b",target:"_blank",rel:"noopener noreferrer"},oe={href:"https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg",target:"_blank",rel:"noopener noreferrer"},pe={href:"https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5",target:"_blank",rel:"noopener noreferrer"},de={href:"https://huggingface.co/project-baize/baize-v2-7b",target:"_blank",rel:"noopener noreferrer"},ge={href:"https://huggingface.co/Salesforce/codet5p-6b",target:"_blank",rel:"noopener noreferrer"},ue={href:"https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b",target:"_blank",rel:"noopener noreferrer"},me={href:"https://huggingface.co/THUDM/chatglm-6b",target:"_blank",rel:"noopener noreferrer"},he={href:"https://huggingface.co/THUDM/chatglm2-6b",target:"_blank",rel:"noopener noreferrer"},be={href:"https://huggingface.co/tiiuae/falcon-40b",target:"_blank",rel:"noopener noreferrer"},fe={href:"https://huggingface.co/timdettmers/guanaco-33b-merged",target:"_blank",rel:"noopener noreferrer"},ce={href:"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat",target:"_blank",rel:"noopener noreferrer"},ve={href:"https://huggingface.co/WizardLM/WizardLM-13B-V1.0",target:"_blank",rel:"noopener noreferrer"},ke={href:"https://huggingface.co/WizardLM/WizardCoder-15B-V1.0",target:"_blank",rel:"noopener noreferrer"},Ee={href:"https://huggingface.co/baichuan-inc/baichuan-7B",target:"_blank",rel:"noopener noreferrer"},Ae={href:"https://huggingface.co/internlm/internlm-chat-7b",target:"_blank",rel:"noopener noreferrer"},Le={href:"https://huggingface.co/Qwen/Qwen-7B-Chat",target:"_blank",rel:"noopener noreferrer"},ye={href:"https://huggingface.co/HuggingFaceH4/starchat-beta",target:"_blank",rel:"noopener noreferrer"},xe={href:"https://huggingface.co/EleutherAI",target:"_blank",rel:"noopener noreferrer"},Ce={href:"https://huggingface.co/EleutherAI/pythia-6.9b",target:"_blank",rel:"noopener noreferrer"},Ie={href:"https://github.com/huggingface/peft",target:"_blank",rel:"noopener noreferrer"},Be={href:"https://github.com/lm-sys/FastChat",target:"_blank",rel:"noopener noreferrer"},De={href:"https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md",target:"_blank",rel:"noopener noreferrer"},Me={href:"https://huggingface.co/models?pipeline_tag=sentence-similarity",target:"_blank",rel:"noopener noreferrer"},Pe={href:"https://huggingface.co/moka-ai/m3e-small",target:"_blank",rel:"noopener noreferrer"},Fe={href:"https://huggingface.co/moka-ai/m3e-base",target:"_blank",rel:"noopener noreferrer"},we={href:"https://huggingface.co/moka-ai/m3e-large",target:"_blank",rel:"noopener noreferrer"},Te={href:"https://huggingface.co/BAAI/bge-small-zh",target:"_blank",rel:"noopener noreferrer"},qe={href:"https://huggingface.co/BAAI/bge-base-zh",target:"_blank",rel:"noopener noreferrer"},ze={href:"https://huggingface.co/BAAI/bge-large-zh",target:"_blank",rel:"noopener noreferrer"},Ue={href:"https://huggingface.co/BAAI/bge-large-zh-noinstruct",target:"_blank",rel:"noopener noreferrer"},We={href:"https://huggingface.co/shibing624/text2vec-base-chinese-sentence",target:"_blank",rel:"noopener noreferrer"},Re={href:"https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase",target:"_blank",rel:"noopener noreferrer"},Ne={href:"https://huggingface.co/shibing624/text2vec-base-multilingual",target:"_blank",rel:"noopener noreferrer"},Se={href:"https://huggingface.co/shibing624/text2vec-base-chinese",target:"_blank",rel:"noopener noreferrer"},$e={href:"https://huggingface.co/shibing624/text2vec-bge-large-chinese",target:"_blank",rel:"noopener noreferrer"},He={href:"https://huggingface.co/GanymedeNil/text2vec-large-chinese",target:"_blank",rel:"noopener noreferrer"},Ve={href:"https://huggingface.co/nghuyong/ernie-3.0-nano-zh",target:"_blank",rel:"noopener noreferrer"},je={href:"https://huggingface.co/nghuyong/ernie-3.0-base-zh",target:"_blank",rel:"noopener noreferrer"},Ge={href:"https://platform.openai.com/docs/guides/embeddings",target:"_blank",rel:"noopener noreferrer"},Oe={href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html",target:"_blank",rel:"noopener noreferrer"},Qe={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/INSTALL.md",target:"_blank",rel:"noopener noreferrer"},Ke={href:"https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html",target:"_blank",rel:"noopener noreferrer"},Ze={href:"https://huggingface.co/models",target:"_blank",rel:"noopener noreferrer"},Je={href:"https://huggingface.co/THUDM/chatglm2-6b",target:"_blank",rel:"noopener noreferrer"},Ye={href:"https://huggingface.co/moka-ai/m3e-base",target:"_blank",rel:"noopener noreferrer"},Xe={href:"https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage",target:"_blank",rel:"noopener noreferrer"},_e={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/configs/model_config.py.example",target:"_blank",rel:"noopener noreferrer"},en={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/configs/server_config.py.example",target:"_blank",rel:"noopener noreferrer"},nn={href:"https://github.com/chatchat-space/Langchain-Chatchat/issues/1130#issuecomment-1685291822",target:"_blank",rel:"noopener noreferrer"},tn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_0.png",target:"_blank",rel:"noopener noreferrer"},an={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_1.png",target:"_blank",rel:"noopener noreferrer"},ln={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1.1-%E5%9F%BA%E4%BA%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B%E8%84%9A%E6%9C%AC-llm_api.py-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1",target:"_blank",rel:"noopener noreferrer"},sn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1.2-%E5%9F%BA%E4%BA%8E%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%84%9A%E6%9C%AC-llm_api_stale.py-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1",target:"_blank",rel:"noopener noreferrer"},rn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1.3-PEFT-%E5%8A%A0%E8%BD%BD",target:"_blank",rel:"noopener noreferrer"},on={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/llm_api.py",target:"_blank",rel:"noopener noreferrer"},pn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/llm_api_stale.py",target:"_blank",rel:"noopener noreferrer"},dn={href:"https://github.com/chatchat-space/Langchain-Chatchat/issues/1130#issuecomment-1685291822",target:"_blank",rel:"noopener noreferrer"},gn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.1-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1",target:"_blank",rel:"noopener noreferrer"},un={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/api.py",target:"_blank",rel:"noopener noreferrer"},mn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/api.py",target:"_blank",rel:"noopener noreferrer"},hn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/README.md#5.2-%E5%90%AF%E5%8A%A8-API-%E6%9C%8D%E5%8A%A1",target:"_blank",rel:"noopener noreferrer"},bn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/webui.py",target:"_blank",rel:"noopener noreferrer"},fn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_0.png",target:"_blank",rel:"noopener noreferrer"},cn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/webui_0813_1.png",target:"_blank",rel:"noopener noreferrer"},vn={href:"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/FAQ.md",target:"_blank",rel:"noopener noreferrer"},kn={href:"https://github.com/lm-sys/fastchat",target:"_blank",rel:"noopener noreferrer"};function En(An,e){const a=p("ExternalLinkIcon");return o(),i("div",null,[e[233]||(e[233]=n("h1",{id:"langchain-chatchat",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#langchain-chatchat"},[n("span",null,"Langchain-Chatchat")])],-1)),e[234]||(e[234]=n("p",null,"项目文档目录",-1)),n("ul",null,[e[3]||(e[3]=n("li",null,"FAQ.md",-1)),n("li",null,[n("a",v,[e[0]||(e[0]=t("Install.md")),l(a)])]),n("li",null,[n("a",k,[e[1]||(e[1]=t("在Anaconda中使用pip安装包的问题.md")),l(a)])]),e[4]||(e[4]=n("li",null,"向量库环境 docker.md",-1)),e[5]||(e[5]=n("li",null,"启动API服务.md",-1)),n("li",null,[n("a",E,[e[2]||(e[2]=t("在Anaconda中使用pip安装包无效问题.md")),l(a)])])]),e[235]||(e[235]=n("p",null,"参考：https://github.com/chatchat-space/Langchain-Chatchat",-1)),e[236]||(e[236]=n("p",null,"README 目录",-1)),n("ul",null,[n("li",null,[n("a",A,[e[6]||(e[6]=t("介绍")),l(a)])]),n("li",null,[n("a",L,[e[7]||(e[7]=t("变更日志")),l(a)])]),n("li",null,[n("a",y,[e[8]||(e[8]=t("模型支持")),l(a)])]),n("li",null,[n("a",x,[e[9]||(e[9]=t("Docker 部署")),l(a)])]),n("li",null,[e[17]||(e[17]=t("开发部署 ")),n("ul",null,[n("li",null,[n("a",C,[e[10]||(e[10]=t("软件需求")),l(a)])]),n("li",null,[n("a",I,[e[11]||(e[11]=t("1. 开发环境准备")),l(a)])]),n("li",null,[n("a",B,[e[12]||(e[12]=t("2. 下载模型至本地")),l(a)])]),n("li",null,[n("a",D,[e[13]||(e[13]=t("3. 设置配置项")),l(a)])]),n("li",null,[n("a",M,[e[14]||(e[14]=t("4. 知识库初始化与迁移")),l(a)])]),n("li",null,[n("a",P,[e[15]||(e[15]=t("5. 一键启动API服务或WebUI服务")),l(a)])]),n("li",null,[n("a",F,[e[16]||(e[16]=t("6. 分步启动 API 服务或 Web UI")),l(a)])])])]),n("li",null,[n("a",w,[e[18]||(e[18]=t("常见问题")),l(a)])]),n("li",null,[n("a",T,[e[19]||(e[19]=t("路线图")),l(a)])]),n("li",null,[n("a",q,[e[20]||(e[20]=t("项目交流群")),l(a)])])]),e[237]||(e[237]=n("h2",{id:"介绍",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#介绍"},[n("span",null,"介绍")])],-1)),n("p",null,[e[22]||(e[22]=t("🤖️ 一种利用 ")),n("a",z,[e[21]||(e[21]=t("langchain")),l(a)]),e[23]||(e[23]=t(" 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。"))]),n("p",null,[e[32]||(e[32]=t("💡 受 ")),n("a",U,[e[24]||(e[24]=t("GanymedeNil")),l(a)]),e[33]||(e[33]=t(" 的项目 ")),n("a",W,[e[25]||(e[25]=t("document.ai")),l(a)]),e[34]||(e[34]=t(" 和 ")),n("a",R,[e[26]||(e[26]=t("AlexZhangji")),l(a)]),e[35]||(e[35]=t(" 创建的 ")),n("a",N,[e[27]||(e[27]=t("ChatGLM-6B Pull Request")),l(a)]),e[36]||(e[36]=t(" 启发，建立了全流程可使用开源模型实现的本地知识库问答应用。本项目的最新版本中通过使用 ")),n("a",S,[e[28]||(e[28]=t("FastChat")),l(a)]),e[37]||(e[37]=t(" 接入 Vicuna, Alpaca, LLaMA, Koala, RWKV 等模型，依托于 ")),n("a",$,[e[29]||(e[29]=t("langchain")),l(a)]),e[38]||(e[38]=t(" 框架支持通过基于 ")),n("a",H,[e[30]||(e[30]=t("FastAPI")),l(a)]),e[39]||(e[39]=t(" 提供的 API 调用服务，或使用基于 ")),n("a",V,[e[31]||(e[31]=t("Streamlit")),l(a)]),e[40]||(e[40]=t(" 的 WebUI 进行操作。"))]),e[238]||(e[238]=s("<p>✅ 依托于本项目支持的开源 <strong>LLM</strong> 与 <strong>Embedding</strong> 模型，本项目可实现全部使用<strong>开源</strong>模型<strong>离线私有部署</strong>。与此同时，本项目也支持 OpenAI GPT API 的调用，并将在后续持续扩充对各类模型及模型 API 的接入。 （术语：LLM 是大型语言模型，Embedding 是向量库映射）</p><p>⛓️ 本项目实现原理如下图所示，过程包括加载文件 -&gt; 读取文本 -&gt; 文本分割 -&gt; 文本向量化 -&gt; 问句向量化 -&gt; 在文本向量中匹配出与问句向量最相似的 <code>top k</code>个 -&gt; 匹配出的文本作为上下文和问题一起添加到 <code>prompt</code>中 -&gt; 提交给 <code>LLM</code>生成回答。</p>",2)),n("p",null,[e[42]||(e[42]=t("📺 ")),n("a",j,[e[41]||(e[41]=t("原理介绍视频")),l(a)])]),e[239]||(e[239]=n("p",null,[n("img",{src:d,alt:"image-20230906094634937",loading:"lazy"})],-1)),e[240]||(e[240]=n("p",null,"从文档处理角度来看，实现流程如下：",-1)),e[241]||(e[241]=n("p",null,[n("img",{src:g,alt:"image-20230906094624349",loading:"lazy"})],-1)),e[242]||(e[242]=n("p",null,"🚩 本项目未涉及微调、训练过程，但可利用微调或训练对本项目效果进行优化。",-1)),n("p",null,[e[44]||(e[44]=t("🌐 ")),n("a",G,[e[43]||(e[43]=t("AutoDL 镜像")),l(a)]),e[45]||(e[45]=t(" 中 ")),e[46]||(e[46]=n("code",null,"v7",-1)),e[47]||(e[47]=t(" 版本所使用代码已更新至本项目 ")),e[48]||(e[48]=n("code",null,"v0.2.3",-1)),e[49]||(e[49]=t(" 版本。"))]),n("p",null,[e[51]||(e[51]=t("🐳 ")),n("a",O,[e[50]||(e[50]=t("Docker 镜像")),l(a)])]),e[243]||(e[243]=s(`<p>💻 一行命令运行 Docker：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-p</span> <span class="token number">80</span>:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="模型支持" tabindex="-1"><a class="header-anchor" href="#模型支持"><span>模型支持</span></a></h3>`,3)),n("p",null,[e[54]||(e[54]=t("本项目中默认使用的 LLM 模型为 ")),n("a",Q,[e[52]||(e[52]=t("THUDM/chatglm2-6b")),l(a)]),e[55]||(e[55]=t("，默认使用的 Embedding 模型为 ")),n("a",K,[e[53]||(e[53]=t("moka-ai/m3e-base")),l(a)]),e[56]||(e[56]=t(" 为例。"))]),e[244]||(e[244]=n("h4",{id:"llm-模型支持",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#llm-模型支持"},[n("span",null,"LLM 模型支持")])],-1)),n("p",null,[e[58]||(e[58]=t("本项目最新版本中基于 ")),n("a",Z,[e[57]||(e[57]=t("FastChat")),l(a)]),e[59]||(e[59]=t(" 进行本地 LLM 模型接入，支持模型如下："))]),n("ul",null,[n("li",null,[n("a",J,[e[60]||(e[60]=t("meta-llama/Llama-2-7b-chat-hf")),l(a)])]),e[99]||(e[99]=n("li",null,"Vicuna, Alpaca, LLaMA, Koala",-1)),n("li",null,[n("a",Y,[e[61]||(e[61]=t("BlinkDL/RWKV-4-Raven")),l(a)])]),n("li",null,[n("a",X,[e[62]||(e[62]=t("camel-ai/CAMEL-13B-Combined-Data")),l(a)])]),n("li",null,[n("a",_,[e[63]||(e[63]=t("databricks/dolly-v2-12b")),l(a)])]),n("li",null,[n("a",ee,[e[64]||(e[64]=t("FreedomIntelligence/phoenix-inst-chat-7b")),l(a)])]),n("li",null,[n("a",ne,[e[65]||(e[65]=t("h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b")),l(a)])]),n("li",null,[n("a",te,[e[66]||(e[66]=t("lcw99/polyglot-ko-12.8b-chang-instruct-chat")),l(a)])]),n("li",null,[n("a",ae,[e[67]||(e[67]=t("lmsys/fastchat-t5-3b-v1.0")),l(a)])]),n("li",null,[n("a",le,[e[68]||(e[68]=t("mosaicml/mpt-7b-chat")),l(a)])]),n("li",null,[n("a",se,[e[69]||(e[69]=t("Neutralzz/BiLLa-7B-SFT")),l(a)])]),n("li",null,[n("a",re,[e[70]||(e[70]=t("nomic-ai/gpt4all-13b-snoozy")),l(a)])]),n("li",null,[n("a",ie,[e[71]||(e[71]=t("NousResearch/Nous-Hermes-13b")),l(a)])]),n("li",null,[n("a",oe,[e[72]||(e[72]=t("openaccess-ai-collective/manticore-13b-chat-pyg")),l(a)])]),n("li",null,[n("a",pe,[e[73]||(e[73]=t("OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5")),l(a)])]),n("li",null,[n("a",de,[e[74]||(e[74]=t("project-baize/baize-v2-7b")),l(a)])]),n("li",null,[n("a",ge,[e[75]||(e[75]=t("Salesforce/codet5p-6b")),l(a)])]),n("li",null,[n("a",ue,[e[76]||(e[76]=t("StabilityAI/stablelm-tuned-alpha-7b")),l(a)])]),n("li",null,[n("a",me,[e[77]||(e[77]=t("THUDM/chatglm-6b")),l(a)])]),n("li",null,[n("a",he,[e[78]||(e[78]=t("THUDM/chatglm2-6b")),l(a)])]),n("li",null,[n("a",be,[e[79]||(e[79]=t("tiiuae/falcon-40b")),l(a)])]),n("li",null,[n("a",fe,[e[80]||(e[80]=t("timdettmers/guanaco-33b-merged")),l(a)])]),n("li",null,[n("a",ce,[e[81]||(e[81]=t("togethercomputer/RedPajama-INCITE-7B-Chat")),l(a)])]),n("li",null,[n("a",ve,[e[82]||(e[82]=t("WizardLM/WizardLM-13B-V1.0")),l(a)])]),n("li",null,[n("a",ke,[e[83]||(e[83]=t("WizardLM/WizardCoder-15B-V1.0")),l(a)])]),n("li",null,[n("a",Ee,[e[84]||(e[84]=t("baichuan-inc/baichuan-7B")),l(a)])]),n("li",null,[n("a",Ae,[e[85]||(e[85]=t("internlm/internlm-chat-7b")),l(a)])]),n("li",null,[n("a",Le,[e[86]||(e[86]=t("Qwen/Qwen-7B-Chat")),l(a)])]),n("li",null,[n("a",ye,[e[87]||(e[87]=t("HuggingFaceH4/starchat-beta")),l(a)])]),n("li",null,[e[90]||(e[90]=t("任何 ")),n("a",xe,[e[88]||(e[88]=t("EleutherAI")),l(a)]),e[91]||(e[91]=t(" 的 pythia 模型，如 ")),n("a",Ce,[e[89]||(e[89]=t("pythia-6.9b")),l(a)])]),n("li",null,[e[93]||(e[93]=t("在以上模型基础上训练的任何 ")),n("a",Ie,[e[92]||(e[92]=t("Peft")),l(a)]),e[94]||(e[94]=t(" 适配器。为了激活，模型路径中必须有 ")),e[95]||(e[95]=n("code",null,"peft",-1)),e[96]||(e[96]=t(" 。注意：如果加载多个peft模型，你可以通过在任何模型工作器中设置环境变量 ")),e[97]||(e[97]=n("code",null,"PEFT_SHARE_BASE_WEIGHTS=true",-1)),e[98]||(e[98]=t(" 来使它们共享基础模型的权重。"))])]),n("p",null,[e[102]||(e[102]=t("以上模型支持列表可能随 ")),n("a",Be,[e[100]||(e[100]=t("FastChat")),l(a)]),e[103]||(e[103]=t(" 更新而持续更新，可参考 ")),n("a",De,[e[101]||(e[101]=t("FastChat 已支持模型列表")),l(a)]),e[104]||(e[104]=t("。"))]),e[245]||(e[245]=n("p",null,[t("除本地模型外，本项目也支持直接接入 OpenAI API，具体设置可参考 "),n("code",null,"configs/model_configs.py.example"),t(" 中的 "),n("code",null,"llm_model_dict"),t(" 的 "),n("code",null,"openai-chatgpt-3.5"),t(" 配置信息。")],-1)),e[246]||(e[246]=n("h4",{id:"embedding-模型支持",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#embedding-模型支持"},[n("span",null,"Embedding 模型支持")])],-1)),n("p",null,[e[106]||(e[106]=t("本项目支持调用 ")),n("a",Me,[e[105]||(e[105]=t("HuggingFace")),l(a)]),e[107]||(e[107]=t(" 中的 Embedding 模型，已支持的 Embedding 模型如下："))]),n("ul",null,[n("li",null,[n("a",Pe,[e[108]||(e[108]=t("moka-ai/m3e-small")),l(a)])]),n("li",null,[n("a",Fe,[e[109]||(e[109]=t("moka-ai/m3e-base")),l(a)])]),n("li",null,[n("a",we,[e[110]||(e[110]=t("moka-ai/m3e-large")),l(a)])]),n("li",null,[n("a",Te,[e[111]||(e[111]=t("BAAI/bge-small-zh")),l(a)])]),n("li",null,[n("a",qe,[e[112]||(e[112]=t("BAAI/bge-base-zh")),l(a)])]),n("li",null,[n("a",ze,[e[113]||(e[113]=t("BAAI/bge-large-zh")),l(a)])]),n("li",null,[n("a",Ue,[e[114]||(e[114]=t("BAAI/bge-large-zh-noinstruct")),l(a)])]),n("li",null,[n("a",We,[e[115]||(e[115]=t("shibing624/text2vec-base-chinese-sentence")),l(a)])]),n("li",null,[n("a",Re,[e[116]||(e[116]=t("shibing624/text2vec-base-chinese-paraphrase")),l(a)])]),n("li",null,[n("a",Ne,[e[117]||(e[117]=t("shibing624/text2vec-base-multilingual")),l(a)])]),n("li",null,[n("a",Se,[e[118]||(e[118]=t("shibing624/text2vec-base-chinese")),l(a)])]),n("li",null,[n("a",$e,[e[119]||(e[119]=t("shibing624/text2vec-bge-large-chinese")),l(a)])]),n("li",null,[n("a",He,[e[120]||(e[120]=t("GanymedeNil/text2vec-large-chinese")),l(a)])]),n("li",null,[n("a",Ve,[e[121]||(e[121]=t("nghuyong/ernie-3.0-nano-zh")),l(a)])]),n("li",null,[n("a",je,[e[122]||(e[122]=t("nghuyong/ernie-3.0-base-zh")),l(a)])]),n("li",null,[n("a",Ge,[e[123]||(e[123]=t("OpenAI/text-embedding-ada-002")),l(a)])])]),e[247]||(e[247]=s(`<h2 id="docker-部署" tabindex="-1"><a class="header-anchor" href="#docker-部署"><span>Docker 部署</span></a></h2><p>🐳 Docker 镜像地址: <code>registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0)</code></p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-p</span> <span class="token number">80</span>:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>`,3)),n("ul",null,[e[130]||(e[130]=s("<li>该版本镜像大小 <code>33.9GB</code>，使用 <code>v0.2.0</code>，以 <code>nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04</code> 为基础镜像</li><li>该版本内置一个 <code>embedding</code> 模型：<code>m3e-large</code>，内置 <code>chatglm2-6b-32k</code></li><li>该版本目标为方便一键部署使用，请确保您已经在Linux发行版上安装了NVIDIA驱动程序</li>",3)),n("li",null,[e[125]||(e[125]=t("请注意，您不需要在主机系统上安装CUDA工具包，但需要安装 ")),e[126]||(e[126]=n("code",null,"NVIDIA Driver",-1)),e[127]||(e[127]=t(" 以及 ")),e[128]||(e[128]=n("code",null,"NVIDIA Container Toolkit",-1)),e[129]||(e[129]=t("，请参考")),n("a",Oe,[e[124]||(e[124]=t("安装指南")),l(a)])]),e[131]||(e[131]=n("li",null,[t("首次拉取和启动均需要一定时间，首次启动时请参照下图使用 "),n("code",null,"docker logs -f <container id>"),t(" 查看日志")],-1)),e[132]||(e[132]=n("li",null,[t("如遇到启动过程卡在 "),n("code",null,"Waiting.."),t(" 步骤，建议使用 "),n("code",null,"docker exec -it <container id> bash"),t(" 进入 "),n("code",null,"/logs/"),t(" 目录查看对应阶段日志")],-1))]),e[248]||(e[248]=n("h2",{id:"开发部署",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#开发部署"},[n("span",null,"开发部署")])],-1)),e[249]||(e[249]=n("h3",{id:"环境准备",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#环境准备"},[n("span",null,"环境准备")])],-1)),e[250]||(e[250]=n("p",null,"开发环境准备",-1)),e[251]||(e[251]=n("p",null,"本项目已在 Python 3.8.1 - 3.10，CUDA 11.7 环境下完成测试。已在 Windows、ARM 架构的 macOS、Linux 系统中完成测试。",-1)),n("p",null,[e[134]||(e[134]=t("参见 ")),n("a",Qe,[e[133]||(e[133]=t("开发环境准备")),l(a)]),e[135]||(e[135]=t("。"))]),e[252]||(e[252]=s(`<p><strong>请注意：</strong> <code>0.2.0</code> 及更新版本的依赖包与 <code>0.1.x</code> 版本依赖包可能发生冲突，强烈建议新建环境后重新安装依赖包。</p><h4 id="环境检查" tabindex="-1"><a class="header-anchor" href="#环境检查"><span>环境检查</span></a></h4><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># 首先，确信你的机器安装了 Python 3.8 - 3.10 版本</span>
$ python <span class="token parameter variable">--version</span>
Python <span class="token number">3.8</span>.13

<span class="token comment"># 如果低于这个版本，可使用conda安装环境</span>
$ conda create <span class="token parameter variable">-p</span> /your_path/env_name <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span>

<span class="token comment"># 激活环境</span>
$ <span class="token builtin class-name">source</span> activate /your_path/env_name

<span class="token comment"># 或，conda安装，不指定路径, 注意以下，都将/your_path/env_name替换为env_name</span>
$ conda create <span class="token parameter variable">-n</span> env_name <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span>
$ conda activate env_name <span class="token comment"># Activate the environment</span>

<span class="token comment"># 更新py库</span>
$ pip3 <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pip

<span class="token comment"># 关闭环境</span>
$ <span class="token builtin class-name">source</span> deactivate /your_path/env_name

<span class="token comment"># 删除环境</span>
$ conda <span class="token function">env</span> remove <span class="token parameter variable">-p</span>  /your_path/env_name
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="项目依赖" tabindex="-1"><a class="header-anchor" href="#项目依赖"><span>项目依赖</span></a></h4><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># 拉取仓库</span>
$ <span class="token function">git</span> clone https://github.com/chatchat-space/Langchain-Chatchat.git

<span class="token comment"># 进入目录</span>
$ <span class="token builtin class-name">cd</span> Langchain-Chatchat

<span class="token comment"># 安装依赖 (三选一，为方便用户 API 与 webui 分离运行，可单独根据运行需求安装依赖包)</span>
$ pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt <span class="token comment"># 安装全部依赖</span>
<span class="token comment"># $ pip install -r requirements_api.txt # 如果只需运行 API</span>
<span class="token comment"># $ pip install -r requirements_webui.txt # 如果只需运行 WebUI</span>

<span class="token comment"># 默认依赖包括基本运行环境（FAISS向量库）。如果要使用 milvus/pg_vector 等向量库，请将 requirements.txt 中相应依赖取消注释再安装。</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,5)),n("p",null,[e[137]||(e[137]=t("注：使用 ")),e[138]||(e[138]=n("code",null,"langchain.document_loaders.UnstructuredFileLoader",-1)),e[139]||(e[139]=t(" 进行 ")),e[140]||(e[140]=n("code",null,".docx",-1)),e[141]||(e[141]=t(" 等格式非结构化文件接入时，可能需要依据文档进行其他依赖包的安装，请参考 ")),n("a",Ke,[e[136]||(e[136]=t("langchain 文档")),l(a)]),e[142]||(e[142]=t("。"))]),e[253]||(e[253]=n("h3",{id:"模型下载-llm-embedding",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#模型下载-llm-embedding"},[n("span",null,"模型下载 (LLM & Embedding)")])],-1)),e[254]||(e[254]=n("p",null,"下载模型至本地",-1)),n("p",null,[e[144]||(e[144]=t("如需在本地或离线环境下运行本项目，需要首先将项目所需的模型下载至本地，通常开源 LLM 与 Embedding 模型可以从 ")),n("a",Ze,[e[143]||(e[143]=t("HuggingFace")),l(a)]),e[145]||(e[145]=t(" 下载。"))]),n("p",null,[e[148]||(e[148]=t("以本项目中默认使用的 LLM 模型 ")),n("a",Je,[e[146]||(e[146]=t("THUDM/chatglm2-6b")),l(a)]),e[149]||(e[149]=t(" 与 Embedding 模型 ")),n("a",Ye,[e[147]||(e[147]=t("moka-ai/m3e-base")),l(a)]),e[150]||(e[150]=t(" 为例："))]),n("p",null,[e[152]||(e[152]=t("下载模型需要先")),n("a",Xe,[e[151]||(e[151]=t("安装Git LFS")),l(a)]),e[153]||(e[153]=t("，然后运行"))]),e[255]||(e[255]=s(`<div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ <span class="token function">git</span> clone https://huggingface.co/THUDM/chatglm2-6b
$ <span class="token function">git</span> clone https://huggingface.co/moka-ai/m3e-base
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="配置选项" tabindex="-1"><a class="header-anchor" href="#配置选项"><span>配置选项</span></a></h3><p>设置配置项</p>`,3)),n("p",null,[e[155]||(e[155]=t("复制模型相关参数配置模板文件 ")),n("a",_e,[e[154]||(e[154]=t("configs/model_config.py.example")),l(a)]),e[156]||(e[156]=t(" 存储至项目路径下 ")),e[157]||(e[157]=n("code",null,"./configs",-1)),e[158]||(e[158]=t(" 路径下，并重命名为 ")),e[159]||(e[159]=n("code",null,"model_config.py",-1)),e[160]||(e[160]=t("。"))]),n("p",null,[e[162]||(e[162]=t("复制服务相关参数配置模板文件 ")),n("a",en,[e[161]||(e[161]=t("configs/server_config.py.example")),l(a)]),e[163]||(e[163]=t(" 存储至项目路径下 ")),e[164]||(e[164]=n("code",null,"./configs",-1)),e[165]||(e[165]=t(" 路径下，并重命名为 ")),e[166]||(e[166]=n("code",null,"server_config.py",-1)),e[167]||(e[167]=t("。"))]),e[256]||(e[256]=s(`<p>在开始执行 Web UI 或命令行交互前，请先检查 <code>configs/model_config.py</code> 和 <code>configs/server_config.py</code> 中的各项模型参数设计是否符合需求：</p><ul><li><p>请确认已下载至本地的 LLM 模型本地存储路径写在 <code>llm_model_dict</code> 对应模型的 <code>local_model_path</code> 属性中，如:</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>llm_model_dict<span class="token operator">=</span><span class="token punctuation">{</span>
    <span class="token string">&quot;chatglm2-6b&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;local_model_path&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;/Users/xxx/Downloads/chatglm2-6b&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;api_base_url&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;http://localhost:8888/v1&quot;</span><span class="token punctuation">,</span>  <span class="token comment"># &quot;name&quot;修改为 FastChat 服务中的&quot;api_base_url&quot;</span>
        <span class="token string">&quot;api_key&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;EMPTY&quot;</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>请确认已下载至本地的 Embedding 模型本地存储路径写在 <code>embedding_model_dict</code> 对应模型位置，如：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>embedding_model_dict <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;m3e-base&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;/Users/xxx/Downloads/m3e-base&quot;</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><p>如果你选择使用OpenAI的Embedding模型，请将模型的 <code>key</code>写入 <code>embedding_model_dict</code>中。使用该模型，你需要能够访问OpenAI官的API，或设置代理。</p><h3 id="知识库初始化与迁移" tabindex="-1"><a class="header-anchor" href="#知识库初始化与迁移"><span>知识库初始化与迁移</span></a></h3><p>当前项目的知识库信息存储在数据库中，在正式运行项目之前请先初始化数据库（我们强烈建议您在执行操作前备份您的知识文件）。</p><ul><li><p>如果您是从 <code>0.1.x</code> 版本升级过来的用户，针对已建立的知识库，请确认知识库的向量库类型、Embedding 模型与 <code>configs/model_config.py</code> 中默认设置一致，如无变化只需以下命令将现有知识库信息添加到数据库即可：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python init_database.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li><li><p>如果您是第一次运行本项目，知识库尚未建立，或者配置文件中的知识库类型、嵌入模型发生变化，或者之前的向量库没有开启 <code>normalize_L2</code>，需要以下命令初始化或重建知识库：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python init_database.py --recreate-vs
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li></ul><h3 id="一键启动api-服务或-web-ui" tabindex="-1"><a class="header-anchor" href="#一键启动api-服务或-web-ui"><span>一键启动API 服务或 Web UI</span></a></h3><h4 id="启动命令" tabindex="-1"><a class="header-anchor" href="#启动命令"><span>启动命令</span></a></h4><p>一键启动脚本 startup.py,一键启动所有 Fastchat 服务、API 服务、WebUI 服务，示例代码：</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>$ python startup.py -a
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>并可使用 <code>Ctrl + C</code> 直接关闭所有运行服务。如果一次结束不了，可以多按几次。</p><p>可选参数包括 <code>-a (或--all-webui)</code>, <code>--all-api</code>, <code>--llm-api</code>, <code>-c (或--controller)</code>, <code>--openai-api</code>, <code>-m (或--model-worker)</code>, <code>--api</code>, <code>--webui</code>，其中：</p><ul><li><code>--all-webui</code> 为一键启动 WebUI 所有依赖服务；</li><li><code>--all-api</code> 为一键启动 API 所有依赖服务；</li><li><code>--llm-api</code> 为一键启动 Fastchat 所有依赖的 LLM 服务；</li><li><code>--openai-api</code> 为仅启动 FastChat 的 controller 和 openai-api-server 服务；</li><li>其他为单独服务启动选项。</li></ul><h4 id="启动非默认模型" tabindex="-1"><a class="header-anchor" href="#启动非默认模型"><span>启动非默认模型</span></a></h4><p>若想指定非默认模型，需要用 <code>--model-name</code> 选项，示例：</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>$ python startup.py --all-webui --model-name Qwen-7B-Chat
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>更多信息可通过 <code>python startup.py -h</code>查看。</p><h4 id="多卡加载" tabindex="-1"><a class="header-anchor" href="#多卡加载"><span>多卡加载</span></a></h4><p>项目支持多卡加载，需在 startup.py 中的 create_model_worker_app 函数中，修改如下三个参数:</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>gpus=None, 
num_gpus=1, 
max_gpu_memory=&quot;20GiB&quot;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，<code>gpus</code> 控制使用的显卡的ID，例如 &quot;0,1&quot;;</p><p><code>num_gpus</code> 控制使用的卡数;</p><p><code>max_gpu_memory</code> 控制每个卡使用的显存容量。</p><p>注1：server_config.py的FSCHAT_MODEL_WORKERS字典中也增加了相关配置，如有需要也可通过修改FSCHAT_MODEL_WORKERS字典中对应参数实现多卡加载。</p><p>注2：少数情况下，gpus参数会不生效，此时需要通过设置环境变量CUDA_VISIBLE_DEVICES来指定torch可见的gpu,示例代码：</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>CUDA_VISIBLE_DEVICES=0,1 python startup.py 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h4 id="peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等" tabindex="-1"><a class="header-anchor" href="#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等"><span>PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)</span></a></h4><p>本项目基于 FastChat 加载 LLM 服务，故需以 FastChat 加载 PEFT 路径，即保证路径名称里必须有 peft 这个词，配置文件的名字为 adapter_config.json，peft 路径下包含.bin 格式的 PEFT 权重，peft路径在startup.py中create_model_worker_app函数的args.model_names中指定，并开启环境变量PEFT_SHARE_BASE_WEIGHTS=true参数。</p>`,28)),n("p",null,[e[169]||(e[169]=t("注：如果上述方式启动失败，则需要以标准的fastchat服务启动方式分步启动，分步启动步骤参考第六节，PEFT加载详细步骤参考")),n("a",nn,[e[168]||(e[168]=t("加载lora微调后模型失效")),l(a)]),e[170]||(e[170]=t("，"))]),e[257]||(e[257]=s('<h4 id="注意事项" tabindex="-1"><a class="header-anchor" href="#注意事项"><span><strong>注意事项</strong></span></a></h4><p><strong>1. startup 脚本用多进程方式启动各模块的服务，可能会导致打印顺序问题，请等待全部服务发起后再调用，并根据默认或指定端口调用服务（默认 LLM API 服务端口：<code>127.0.0.1:8888</code>,默认 API 服务端口：<code>127.0.0.1:7861</code>,默认 WebUI 服务端口：<code>本机IP：8501</code>)</strong></p><p><strong>2.服务启动时间示设备不同而不同，约 3-10 分钟，如长时间没有启动请前往 <code>./logs</code>目录下监控日志，定位问题。</strong></p><p><strong>3. 在Linux上使用ctrl+C退出可能会由于linux的多进程机制导致multiprocessing遗留孤儿进程，可通过shutdown_all.sh进行退出</strong></p><h4 id="启动界面示例" tabindex="-1"><a class="header-anchor" href="#启动界面示例"><span>启动界面示例</span></a></h4><ol><li>FastAPI docs 界面</li></ol><p><img src="'+u+'" alt="image-20230906095433786" loading="lazy"></p><ol start="2"><li>webui启动界面示例：</li></ol>',8)),n("ul",null,[n("li",null,[e[172]||(e[172]=t("Web UI 对话界面： ")),n("a",tn,[e[171]||(e[171]=n("img",{src:"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png",alt:"img",loading:"lazy"},null,-1)),l(a)])]),n("li",null,[e[174]||(e[174]=t("Web UI 知识库管理页面： ")),n("a",an,[e[173]||(e[173]=n("img",{src:m,alt:"img",loading:"lazy"},null,-1)),l(a)])])]),e[258]||(e[258]=n("h3",{id:"分步启动-api-服务或-web-ui-一键启动忽略本节",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#分步启动-api-服务或-web-ui-一键启动忽略本节"},[n("span",null,"分步启动 API 服务或 Web UI（一键启动忽略本节）")])],-1)),e[259]||(e[259]=n("p",null,"注意：如使用了一键启动方式，可忽略本节。",-1)),e[260]||(e[260]=n("h4",{id:"启动-llm-服务-三种方式",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#启动-llm-服务-三种方式"},[n("span",null,"启动 LLM 服务 (三种方式)")])],-1)),e[261]||(e[261]=n("p",null,"如需使用开源模型进行本地部署，需首先启动 LLM 服务，启动方式分为三种：",-1)),n("ul",null,[n("li",null,[n("a",ln,[e[175]||(e[175]=t("基于多进程脚本 llm_api.py 启动 LLM 服务")),l(a)])]),n("li",null,[n("a",sn,[e[176]||(e[176]=t("基于命令行脚本 llm_api_stale.py 启动 LLM 服务")),l(a)])]),n("li",null,[n("a",rn,[e[177]||(e[177]=t("PEFT 加载")),l(a)])])]),e[262]||(e[262]=n("p",null,"三种方式只需选择一个即可，具体操作方式详见 5.1.1 - 5.1.3。",-1)),e[263]||(e[263]=n("p",null,"如果启动在线的API服务（如 OPENAI 的 API 接口），则无需启动 LLM 服务，即 5.1 小节的任何命令均无需启动。",-1)),e[264]||(e[264]=n("h5",{id:"基于多进程脚本-llm-api-py-启动-llm-服务",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#基于多进程脚本-llm-api-py-启动-llm-服务"},[n("span",null,"基于多进程脚本 llm_api.py 启动 LLM 服务")])],-1)),n("p",null,[e[179]||(e[179]=t("在项目根目录下，执行 ")),n("a",on,[e[178]||(e[178]=t("server/llm_api.py")),l(a)]),e[180]||(e[180]=t(" 脚本启动 ")),e[181]||(e[181]=n("strong",null,"LLM 模型",-1)),e[182]||(e[182]=t("服务："))]),e[265]||(e[265]=s(`<div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>$ python server/llm_api.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>项目支持多卡加载，需在 llm_api.py 中的 create_model_worker_app 函数中，修改如下三个参数:</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>gpus=None, 
num_gpus=1, 
max_gpu_memory=&quot;20GiB&quot;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，<code>gpus</code> 控制使用的显卡的ID，如果 &quot;0,1&quot;;</p><p><code>num_gpus</code> 控制使用的卡数;</p><p><code>max_gpu_memory</code> 控制每个卡使用的显存容量。</p><h5 id="基于命令行脚本-llm-api-stale-py-启动-llm-服务" tabindex="-1"><a class="header-anchor" href="#基于命令行脚本-llm-api-stale-py-启动-llm-服务"><span>基于命令行脚本 llm_api_stale.py 启动 LLM 服务</span></a></h5><p>⚠️ <strong>注意:</strong></p><p><strong>1.llm_api_stale.py脚本原生仅适用于linux。mac设备需要安装对应的linux命令，win平台请使用wsl;</strong></p><p><strong>2.加载非默认模型需要用命令行参数--model-path-address指定模型，不会读取model_config.py配置;</strong></p>`,10)),n("p",null,[e[184]||(e[184]=t("在项目根目录下，执行 ")),n("a",pn,[e[183]||(e[183]=t("server/llm_api_stale.py")),l(a)]),e[185]||(e[185]=t(" 脚本启动 ")),e[186]||(e[186]=n("strong",null,"LLM 模型",-1)),e[187]||(e[187]=t("服务："))]),e[266]||(e[266]=s(`<div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_stale.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>该方式支持启动多个worker，示例启动方式：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_stale.py --model-path-address model1@host1@port1 model2@host2@port2
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>如果出现server端口占用情况，需手动指定server端口,并同步修改model_config.py下对应模型的base_api_url为指定端口:</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_stale.py --server-port <span class="token number">8887</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>如果要启动多卡加载，示例命令如下：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_stale.py <span class="token parameter variable">--gpus</span> <span class="token number">0,1</span> --num-gpus <span class="token number">2</span> --max-gpu-memory 10GiB
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>注：以如上方式启动LLM服务会以nohup命令在后台运行 FastChat 服务，如需停止服务，可以运行如下命令：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/llm_api_shutdown.py <span class="token parameter variable">--serve</span> all 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>亦可单独停止一个 FastChat 服务模块，可选 [<code>all</code>, <code>controller</code>, <code>model_worker</code>, <code>openai_api_server</code>]</p><h5 id="peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1" tabindex="-1"><a class="header-anchor" href="#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1"><span>PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)</span></a></h5>`,11)),n("p",null,[e[189]||(e[189]=t("本项目基于 FastChat 加载 LLM 服务，故需以 FastChat 加载 PEFT 路径，即保证路径名称里必须有 peft 这个词，配置文件的名字为 adapter_config.json，peft 路径下包含 model.bin 格式的 PEFT 权重。 详细步骤参考")),n("a",dn,[e[188]||(e[188]=t("加载lora微调后模型失效")),l(a)])]),e[267]||(e[267]=n("p",null,[n("img",{src:h,alt:"image-20230906100313064",loading:"lazy"})],-1)),e[268]||(e[268]=n("h4",{id:"启动-api-服务",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#启动-api-服务"},[n("span",null,"启动 API 服务")])],-1)),n("p",null,[e[192]||(e[192]=t("本地部署情况下，按照 ")),n("a",gn,[e[190]||(e[190]=t("5.1 节")),l(a)]),e[193]||(e[193]=n("strong",null,"启动 LLM 服务后",-1)),e[194]||(e[194]=t("，再执行 ")),n("a",un,[e[191]||(e[191]=t("server/api.py")),l(a)]),e[195]||(e[195]=t(" 脚本启动 ")),e[196]||(e[196]=n("strong",null,"API",-1)),e[197]||(e[197]=t(" 服务；"))]),n("p",null,[e[199]||(e[199]=t("在线调用API服务的情况下，直接执执行 ")),n("a",mn,[e[198]||(e[198]=t("server/api.py")),l(a)]),e[200]||(e[200]=t(" 脚本启动 ")),e[201]||(e[201]=n("strong",null,"API",-1)),e[202]||(e[202]=t(" 服务；"))]),e[269]||(e[269]=s(`<p>调用命令示例：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python server/api.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>启动 API 服务后，可访问 <code>localhost:7861</code> 或 <code>{API 所在服务器 IP}:7861</code> FastAPI 自动生成的 docs 进行接口查看与测试。</p><ul><li>FastAPI docs 界面 <img src="`+b+'" alt="image-20230906100403529" loading="lazy"></li></ul><h4 id="启动-web-ui-服务" tabindex="-1"><a class="header-anchor" href="#启动-web-ui-服务"><span>启动 Web UI 服务</span></a></h4>',5)),n("p",null,[e[205]||(e[205]=t("按照 ")),n("a",hn,[e[203]||(e[203]=t("5.2 节")),l(a)]),e[206]||(e[206]=n("strong",null,"启动 API 服务后",-1)),e[207]||(e[207]=t("，执行 ")),n("a",bn,[e[204]||(e[204]=t("webui.py")),l(a)]),e[208]||(e[208]=t(" 启动 ")),e[209]||(e[209]=n("strong",null,"Web UI",-1)),e[210]||(e[210]=t(" 服务（默认使用端口 ")),e[211]||(e[211]=n("code",null,"8501",-1)),e[212]||(e[212]=t("）"))]),e[270]||(e[270]=s(`<div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ streamlit run webui.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>使用 Langchain-Chatchat 主题色启动 <strong>Web UI</strong> 服务（默认使用端口 <code>8501</code>）</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ streamlit run webui.py <span class="token parameter variable">--theme.base</span> <span class="token string">&quot;light&quot;</span> <span class="token parameter variable">--theme.primaryColor</span> <span class="token string">&quot;#165dff&quot;</span> <span class="token parameter variable">--theme.secondaryBackgroundColor</span> <span class="token string">&quot;#f5f5f5&quot;</span> <span class="token parameter variable">--theme.textColor</span> <span class="token string">&quot;#000000&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>或使用以下命令指定启动 <strong>Web UI</strong> 服务并指定端口号</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ streamlit run webui.py <span class="token parameter variable">--server.port</span> <span class="token number">666</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>`,5)),n("ul",null,[n("li",null,[e[214]||(e[214]=n("p",null,"Web UI 对话界面：",-1)),n("p",null,[n("a",fn,[e[213]||(e[213]=n("img",{src:"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png",alt:"img",loading:"lazy"},null,-1)),l(a)])])]),n("li",null,[e[216]||(e[216]=n("p",null,"Web UI 知识库管理页面：",-1)),n("p",null,[n("a",cn,[e[215]||(e[215]=n("img",{src:"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_1.png",alt:"img",loading:"lazy"},null,-1)),l(a)])])])]),e[271]||(e[271]=n("h2",{id:"常见问题",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#常见问题"},[n("span",null,"常见问题")])],-1)),n("p",null,[e[218]||(e[218]=t("参见 ")),n("a",vn,[e[217]||(e[217]=t("常见问题")),l(a)]),e[219]||(e[219]=t("。"))]),e[272]||(e[272]=n("h2",{id:"路线图",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#路线图"},[n("span",null,"路线图")])],-1)),e[273]||(e[273]=n("ul",null,[n("li",null,"Langchain 应用"),n("li",null,[t("本地数据接入 - 接入非结构化文档 "),n("ul",null,[n("li",null,".md"),n("li",null,".txt"),n("li",null,".docx")])])],-1)),e[274]||(e[274]=n("pre",null,[n("code",null,`-  结构化数据接入
  -  .csv
  -  .xlsx
-  分词及召回
  -  接入不同类型 TextSplitter
  -  优化依据中文标点符号设计的 ChineseTextSplitter
  -  重新实现上下文拼接召回
-  本地网页接入
-  SQL 接入
-  知识图谱/图数据库接入
`)],-1)),n("ul",null,[e[223]||(e[223]=n("li",null,"搜索引擎接入 - Bing 搜索 - DuckDuckGo 搜索",-1)),e[224]||(e[224]=n("li",null,"Agent 实现",-1)),e[225]||(e[225]=n("li",null,"LLM 模型接入",-1)),n("li",null,[e[221]||(e[221]=t("支持通过调用 ")),n("a",kn,[e[220]||(e[220]=t("FastChat")),l(a)]),e[222]||(e[222]=t(" api 调用 llm"))]),e[226]||(e[226]=n("li",null,"支持 ChatGLM API 等 LLM API 的接入",-1)),e[227]||(e[227]=n("li",null,"Embedding 模型接入",-1)),e[228]||(e[228]=n("li",null,"支持调用 HuggingFace 中各开源 Emebdding 模型",-1)),e[229]||(e[229]=n("li",null,"支持 OpenAI Embedding API 等 Embedding API 的接入",-1)),e[230]||(e[230]=n("li",null,"基于 FastAPI 的 API 方式调用",-1)),e[231]||(e[231]=n("li",null,"Web UI",-1)),e[232]||(e[232]=n("li",null,"基于 Streamlit 的 Web UI",-1))]),e[275]||(e[275]=n("h2",{id:"项目交流群",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#项目交流群"},[n("span",null,"项目交流群")])],-1)),e[276]||(e[276]=n("img",{src:f,alt:"image-20230906101805804",style:{zoom:"25%"}},null,-1))])}const yn=r(c,[["render",En],["__file","index.html.vue"]]),xn=JSON.parse('{"path":"/MdNote_Public/01.%20DesignAndDevelop/Develop/04.%20Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/","title":"Langchain-Chatchat","lang":"zh-CN","frontmatter":{"description":"Langchain-Chatchat 项目文档目录 FAQ.md Install.md 在Anaconda中使用pip安装包的问题.md 向量库环境 docker.md 启动API服务.md 在Anaconda中使用pip安装包无效问题.md 参考：https://github.com/chatchat-space/Langchain-Chatchat...","head":[["meta",{"property":"og:url","content":"https://LincZero.github.io/MdNote_Public/01.%20DesignAndDevelop/Develop/04.%20Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/"}],["meta",{"property":"og:site_name","content":"Linc 的小站"}],["meta",{"property":"og:title","content":"Langchain-Chatchat"}],["meta",{"property":"og:description","content":"Langchain-Chatchat 项目文档目录 FAQ.md Install.md 在Anaconda中使用pip安装包的问题.md 向量库环境 docker.md 启动API服务.md 在Anaconda中使用pip安装包无效问题.md 参考：https://github.com/chatchat-space/Langchain-Chatchat..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"LincZero"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Langchain-Chatchat\\",\\"image\\":[\\"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png\\",\\"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_0.png\\",\\"https://github.com/chatchat-space/Langchain-Chatchat/raw/master/img/webui_0813_1.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LincZero\\",\\"url\\":\\"https://github.com/LincZero/\\"}]}"]]},"headers":[{"level":1,"title":"Langchain-Chatchat","slug":"langchain-chatchat","link":"#langchain-chatchat","children":[{"level":2,"title":"介绍","slug":"介绍","link":"#介绍","children":[{"level":3,"title":"模型支持","slug":"模型支持","link":"#模型支持","children":[{"level":4,"title":"LLM 模型支持","slug":"llm-模型支持","link":"#llm-模型支持","children":[]},{"level":4,"title":"Embedding 模型支持","slug":"embedding-模型支持","link":"#embedding-模型支持","children":[]}]}]},{"level":2,"title":"Docker 部署","slug":"docker-部署","link":"#docker-部署","children":[]},{"level":2,"title":"开发部署","slug":"开发部署","link":"#开发部署","children":[{"level":3,"title":"环境准备","slug":"环境准备","link":"#环境准备","children":[{"level":4,"title":"环境检查","slug":"环境检查","link":"#环境检查","children":[]},{"level":4,"title":"项目依赖","slug":"项目依赖","link":"#项目依赖","children":[]}]},{"level":3,"title":"模型下载 (LLM & Embedding)","slug":"模型下载-llm-embedding","link":"#模型下载-llm-embedding","children":[]},{"level":3,"title":"配置选项","slug":"配置选项","link":"#配置选项","children":[]},{"level":3,"title":"知识库初始化与迁移","slug":"知识库初始化与迁移","link":"#知识库初始化与迁移","children":[]},{"level":3,"title":"一键启动API 服务或 Web UI","slug":"一键启动api-服务或-web-ui","link":"#一键启动api-服务或-web-ui","children":[{"level":4,"title":"启动命令","slug":"启动命令","link":"#启动命令","children":[]},{"level":4,"title":"启动非默认模型","slug":"启动非默认模型","link":"#启动非默认模型","children":[]},{"level":4,"title":"多卡加载","slug":"多卡加载","link":"#多卡加载","children":[]},{"level":4,"title":"PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)","slug":"peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等","link":"#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等","children":[]},{"level":4,"title":"注意事项","slug":"注意事项","link":"#注意事项","children":[]},{"level":4,"title":"启动界面示例","slug":"启动界面示例","link":"#启动界面示例","children":[]}]},{"level":3,"title":"分步启动 API 服务或 Web UI（一键启动忽略本节）","slug":"分步启动-api-服务或-web-ui-一键启动忽略本节","link":"#分步启动-api-服务或-web-ui-一键启动忽略本节","children":[{"level":4,"title":"启动 LLM 服务 (三种方式)","slug":"启动-llm-服务-三种方式","link":"#启动-llm-服务-三种方式","children":[{"level":5,"title":"基于多进程脚本 llm_api.py 启动 LLM 服务","slug":"基于多进程脚本-llm-api-py-启动-llm-服务","link":"#基于多进程脚本-llm-api-py-启动-llm-服务","children":[]},{"level":5,"title":"基于命令行脚本 llm_api_stale.py 启动 LLM 服务","slug":"基于命令行脚本-llm-api-stale-py-启动-llm-服务","link":"#基于命令行脚本-llm-api-stale-py-启动-llm-服务","children":[]},{"level":5,"title":"PEFT 加载(包括lora,p-tuning,prefix tuning, prompt tuning,ia3等)","slug":"peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1","link":"#peft-加载-包括lora-p-tuning-prefix-tuning-prompt-tuning-ia3等-1","children":[]}]},{"level":4,"title":"启动 API 服务","slug":"启动-api-服务","link":"#启动-api-服务","children":[]},{"level":4,"title":"启动 Web UI 服务","slug":"启动-web-ui-服务","link":"#启动-web-ui-服务","children":[]}]}]},{"level":2,"title":"常见问题","slug":"常见问题","link":"#常见问题","children":[]},{"level":2,"title":"路线图","slug":"路线图","link":"#路线图","children":[]},{"level":2,"title":"项目交流群","slug":"项目交流群","link":"#项目交流群","children":[]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":15.14,"words":4543},"filePathRelative":"MdNote_Public/01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/Project/Chat/Langchain-Chatchat/README.md","excerpt":"\\n<p>项目文档目录</p>\\n<ul>\\n<li>FAQ.md</li>\\n<li><a href=\\"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/INSTALL.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Install.md</a></li>\\n<li><a href=\\"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/Issue-with-Installing-Packages-Using-pip-in-Anaconda.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">在Anaconda中使用pip安装包的问题.md</a></li>\\n<li>向量库环境 docker.md</li>\\n<li>启动API服务.md</li>\\n<li><a href=\\"https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/%E5%9C%A8Anaconda%E4%B8%AD%E4%BD%BF%E7%94%A8pip%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%A0%E6%95%88%E9%97%AE%E9%A2%98.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">在Anaconda中使用pip安装包无效问题.md</a></li>\\n</ul>","autoDesc":true}');export{yn as comp,xn as data};
